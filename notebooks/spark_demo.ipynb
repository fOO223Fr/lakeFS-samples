{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c053f0c-88da-4972-bdbe-686a37af7325",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.lakefs.io/assets/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Integration of lakeFS with Spark and Python\n",
    "\n",
    "Use Case: Isolated Testing Environment\n",
    "\n",
    "Access lakeFS using the S3A gateway"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fea8ed1f-74f0-40fe-aa8f-f4548a108c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd4b5229-ed90-4ff0-893b-dcfdddec161f",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials\n",
    "\n",
    "Change these if using lakeFS other than provided in the samples repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b1b96f-a734-4cf4-953e-97c2e4315e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7c151e3-c469-4258-a7e3-9d25c00a9cc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Storage Information\n",
    "\n",
    "If you're not using sample repo lakeFS, then change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c64b30-4e57-40bd-aa76-fdaf6000f7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageRootPath = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6213e05b-03d4-4065-b92d-b189eec16206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"spark-demo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9e61a4-052c-4992-92c4-103fd68552ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e00d40ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuring lakeFSClient and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b4453c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07041af8-fae2-4064-94c5-afc758695903",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e56a66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository spark-demo does not exist, so going to try and create it now.\n",
      "Created new repo spark-demo using storage namespace s3://example/spark-demo\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageRootPath}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8125e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3ff784eec764:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff64356b30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed2c663f-8c94-4cd9-a49f-6ab5a8cb80db",
   "metadata": {},
   "source": [
    "## Versioning Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fdcc13-19bc-4aee-a833-9d645d2d7b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sourceBranch = \"main\"\n",
    "newBranch = \"experiment01\"\n",
    "newPath = \"partitioned_data\"\n",
    "fileName = \"lakefs_test.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "051af1c6-be68-416b-888d-b88766a5d966",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712fab39-07a9-4050-b91e-1d6434b1e86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checksum': 'afe805e5a3c3ec7fa05645a6a2a6e607',\n",
       " 'content_type': 'text/csv',\n",
       " 'mtime': 1685029026,\n",
       " 'path': 'lakefs_test.csv',\n",
       " 'path_type': 'object',\n",
       " 'physical_address': 's3://example/spark-demo/data/gnfo5ab6tohs77jkmhn0/chno18j6tohs77jkmi8g',\n",
       " 'size_bytes': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "contentToUpload = open(f\"/data/{fileName}\", 'rb') # Only a single file per upload which must be named \\\\\\\"content\\\\\\\"\n",
    "lakefs.objects.upload_object(\n",
    "    repository=repo.id,\n",
    "    branch=sourceBranch,\n",
    "    path=fileName, content=contentToUpload)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f11730f",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17efe410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'everything-bagel',\n",
       " 'creation_date': 1685029026,\n",
       " 'id': '5e12af83c01591d0a44a0904bd98c907bd59603ca17d820835c64f0b40272b1d',\n",
       " 'message': 'Added my first file!',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'using': 'python_api'},\n",
       " 'parents': ['8d964e97f746f0743aa87e86bdca412ddf89ea7be1c73d4b5b5778f71a90090a']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=sourceBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Added my first file!',\n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e234b50",
   "metadata": {},
   "source": [
    "## Reading data by using S3A GatewaydataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1f7c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV from s3a://spark-demo/main/lakefs_test.csv\n",
      "+---+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|_c4|\n",
      "+---+---+---+---+---+\n",
      "|  1|  2|  3|  4|  5|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataPath = f\"s3a://{repo.id}/{sourceBranch}/{fileName}\"\n",
    "print(f\"Reading CSV from {dataPath}\")\n",
    "df = spark.read.csv(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f29fc32",
   "metadata": {},
   "source": [
    "# Experimentation Starts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6485c9b",
   "metadata": {},
   "source": [
    "## List the repository branches by using lakeFS Python client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaffedfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id    commit_id\n",
      "----  ----------------------------------------------------------------\n",
      "main  5e12af83c01591d0a44a0904bd98c907bd59603ca17d820835c64f0b40272b1d\n"
     ]
    }
   ],
   "source": [
    "results = map(\n",
    "    lambda n:[n.id,n.commit_id],\n",
    "    lakefs.branches.list_branches(\n",
    "        repository=repo.id).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['id','commit_id']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86d72d92",
   "metadata": {},
   "source": [
    "## Create a new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d035ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5e12af83c01591d0a44a0904bd98c907bd59603ca17d820835c64f0b40272b1d'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.branches.create_branch(\n",
    "    repository=repo.id,\n",
    "    branch_creation=BranchCreation(\n",
    "        name=newBranch,\n",
    "        source=sourceBranch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90b8c7c0",
   "metadata": {},
   "source": [
    "## Partition the data and write to new branch by using S3A Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8117b502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newDataPath = f\"s3a://{repo.id}/{newBranch}/{newPath}\"\n",
    "\n",
    "df.write.partitionBy(\"_c0\").csv(newDataPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "075eeabc",
   "metadata": {},
   "source": [
    "## Diffing a single branch will show all the uncommitted changes on that branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b760418e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path                                                                             Path Type      Size(Bytes)  Type\n",
      "-------------------------------------------------------------------------------  -----------  -------------  ------\n",
      "partitioned_data/_SUCCESS                                                        object                   8  added\n",
      "partitioned_data/_c0=1/part-00000-525e3d01-093b-4223-93c2-272ba43801fd.c000.csv  object                   8  added\n"
     ]
    }
   ],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    lakefs.branches.diff_branch(\n",
    "        repository=repo.id,\n",
    "        branch=newBranch).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2f0d636",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43d95193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'everything-bagel',\n",
       " 'creation_date': 1685029033,\n",
       " 'id': '77bd9e5348e619bd261b798b33795df0a2158b1cfd8bc17f68889a5bcfd909c9',\n",
       " 'message': 'Partitioned CSV file!',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'using': 'python_api'},\n",
       " 'parents': ['5e12af83c01591d0a44a0904bd98c907bd59603ca17d820835c64f0b40272b1d']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=newBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Partitioned CSV file!',\n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9f5e995",
   "metadata": {},
   "source": [
    "## Diff between the new branch and the source branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b19dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path                                                                             Path Type      Size(Bytes)  Type\n",
      "-------------------------------------------------------------------------------  -----------  -------------  ------\n",
      "partitioned_data/_SUCCESS                                                        object                   8  added\n",
      "partitioned_data/_c0=1/part-00000-525e3d01-093b-4223-93c2-272ba43801fd.c000.csv  object                   8  added\n"
     ]
    }
   ],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    lakefs.refs.diff_refs(\n",
    "        repository=repo.id,\n",
    "        left_ref=sourceBranch,\n",
    "        right_ref=newBranch).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4749992",
   "metadata": {},
   "source": [
    "# Experimentation Completes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a39c242-ac7d-4e56-8099-51e667e2c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Option A: Experimentation fails, so just delete the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c6f3460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.delete_branch(\n",
    "    repository=repo.id,\n",
    "    branch=newBranch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63704bc3",
   "metadata": {},
   "source": [
    "## Option B: Experimentation succeeds, so merge new branch to the main branch (atomic promotion to production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30cb92fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '42dc1632c6c27057004827abbf67d93dbb4d0dcf981809cbbf40f11c0d9a051f',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id,\n",
    "    source_ref=newBranch, \n",
    "    destination_branch=sourceBranch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ca2c741",
   "metadata": {},
   "source": [
    "## Diff between the new branch and the source branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da98c099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path    Path Type    Size(Bytes)    Type\n",
      "------  -----------  -------------  ------\n"
     ]
    }
   ],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    lakefs.refs.diff_refs(\n",
    "        repository=repo.id,\n",
    "        left_ref=sourceBranch,\n",
    "        right_ref=newBranch).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7aa1778",
   "metadata": {},
   "source": [
    "## If you merged new branch to the main branch then you can atomically rollback all changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f678a71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.revert_branch(\n",
    "    repository=repo.id,\n",
    "    branch=sourceBranch, \n",
    "    revert_creation=RevertCreation(\n",
    "        ref=sourceBranch, parent_number=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "873b7142",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
