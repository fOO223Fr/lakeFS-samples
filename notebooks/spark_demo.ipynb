{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c053f0c-88da-4972-bdbe-686a37af7325",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Integration of lakeFS with Spark and Python\n",
    "\n",
    "Use Case: Isolated Testing Environment\n",
    "\n",
    "Access lakeFS using the S3A gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8ed1f-74f0-40fe-aa8f-f4548a108c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b5229-ed90-4ff0-893b-dcfdddec161f",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials\n",
    "\n",
    "Change these if using lakeFS other than provided in the samples repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1b96f-a734-4cf4-953e-97c2e4315e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c151e3-c469-4258-a7e3-9d25c00a9cc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Storage Information\n",
    "\n",
    "If you're not using sample repo lakeFS, then change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c64b30-4e57-40bd-aa76-fdaf6000f7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213e05b-03d4-4065-b92d-b189eec16206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"spark-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e61a4-052c-4992-92c4-103fd68552ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d40ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4453c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07041af8-fae2-4064-94c5-afc758695903",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e56a66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcd743-1cad-4a01-9c28-6adfda24bbb4",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8125e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c663f-8c94-4cd9-a49f-6ab5a8cb80db",
   "metadata": {},
   "source": [
    "## Versioning Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdcc13-19bc-4aee-a833-9d645d2d7b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sourceBranch = \"main\"\n",
    "newBranch = \"experiment01\"\n",
    "newPath = \"partitioned_data\"\n",
    "fileName = \"lakefs_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051af1c6-be68-416b-888d-b88766a5d966",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fab39-07a9-4050-b91e-1d6434b1e86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "contentToUpload = open(f\"/data/{fileName}\", 'rb') # Only a single file per upload which must be named \\\\\\\"content\\\\\\\"\n",
    "lakefs.objects.upload_object(\n",
    "    repository=repo.id,\n",
    "    branch=sourceBranch,\n",
    "    path=fileName, content=contentToUpload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11730f",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efe410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=sourceBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Added my first file!',\n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e234b50",
   "metadata": {},
   "source": [
    "## Reading data by using S3A GatewaydataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f7c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataPath = f\"s3a://{repo.id}/{sourceBranch}/{fileName}\"\n",
    "print(f\"Reading CSV from {dataPath}\")\n",
    "df = spark.read.csv(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29fc32",
   "metadata": {},
   "source": [
    "# Experimentation Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6485c9b",
   "metadata": {},
   "source": [
    "## List the repository branches by using lakeFS Python client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaffedfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = map(\n",
    "    lambda n:[n.id,n.commit_id],\n",
    "    lakefs.branches.list_branches(\n",
    "        repository=repo.id).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['id','commit_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d72d92",
   "metadata": {},
   "source": [
    "## Create a new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d035ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(\n",
    "    repository=repo.id,\n",
    "    branch_creation=BranchCreation(\n",
    "        name=newBranch,\n",
    "        source=sourceBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8c7c0",
   "metadata": {},
   "source": [
    "## Partition the data and write to new branch by using S3A Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117b502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newDataPath = f\"s3a://{repo.id}/{newBranch}/{newPath}\"\n",
    "\n",
    "df.write.partitionBy(\"_c0\").csv(newDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075eeabc",
   "metadata": {},
   "source": [
    "## Diffing a single branch will show all the uncommitted changes on that branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b760418e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    lakefs.branches.diff_branch(\n",
    "        repository=repo.id,\n",
    "        branch=newBranch).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0d636",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d95193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=newBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Partitioned CSV file!',\n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5e995",
   "metadata": {},
   "source": [
    "## Diff between the new branch and the source branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b19dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    lakefs.refs.diff_refs(\n",
    "        repository=repo.id,\n",
    "        left_ref=sourceBranch,\n",
    "        right_ref=newBranch).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4749992",
   "metadata": {},
   "source": [
    "# Experimentation Completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39c242-ac7d-4e56-8099-51e667e2c9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Option A: Experimentation fails, so just delete the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f3460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.delete_branch(\n",
    "    repository=repo.id,\n",
    "    branch=newBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63704bc3",
   "metadata": {},
   "source": [
    "## Option B: Experimentation succeeds, so merge new branch to the main branch (atomic promotion to production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb92fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id,\n",
    "    source_ref=newBranch, \n",
    "    destination_branch=sourceBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2c741",
   "metadata": {},
   "source": [
    "## Diff between the new branch and the source branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98c099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    lakefs.refs.diff_refs(\n",
    "        repository=repo.id,\n",
    "        left_ref=sourceBranch,\n",
    "        right_ref=newBranch).results)\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa1778",
   "metadata": {},
   "source": [
    "## If you merged new branch to the main branch then you can atomically rollback all changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678a71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.revert_branch(\n",
    "    repository=repo.id,\n",
    "    branch=sourceBranch, \n",
    "    revert_creation=RevertCreation(\n",
    "        ref=sourceBranch, parent_number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b7142",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
