{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d8a1d2b",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# ML Experimentation 01 (Dogs)\n",
    "\n",
    "In this tutorial, you will learn how to version your ML training data, model artifacts, metrics and  your training code together with lakeFS.\n",
    "\n",
    "We will be using [Stanford-Dogs-Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) (aka ImageNetDogs) for the image classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7543161f",
   "metadata": {},
   "source": [
    "## Before you start!\n",
    "\n",
    "Download the [Stanford-Dogs-Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) to your MinIO bucket. We will be importing this data into a lakeFS repository and use it for ML model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8633c2c9",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3424646",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0f0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8663ef6a",
   "metadata": {},
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a835b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77635c43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9805f763",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3e6aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"ml-experimentation-dogs\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fa5e22d",
   "metadata": {},
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a553232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_lakefs.import *\n",
    "from lakefs_lakefs.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3949b16",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2270980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying lakeFS credentials‚Ä¶\n",
      "‚Ä¶‚úÖlakeFS credentials verified\n",
      "\n",
      "‚ÑπÔ∏èlakeFS version 0.101.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version {v.version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "828d4dc9",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e97359c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository ml-demo does not exist, so going to try and create it now.\n",
      "Created new repo ml-demo using storage namespace s3://example/ml-demo\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919654c2",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c376fac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d1b69b813d41:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff38e4d990>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbffb17f",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python tensorflow nbimporter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9285a279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fad5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import s3fs\n",
    "import joblib\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import nbimporter\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c266d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_reproducibility.ml_utils import *\n",
    "from ml_reproducibility.file_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9996d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, time, datetime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7b080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55e0ad76",
   "metadata": {},
   "source": [
    "### Configure boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f3aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3',\n",
    "    endpoint_url='http://lakefs:8000',\n",
    "    aws_access_key_id=lakefsAccessKey,\n",
    "    aws_secret_access_key=lakefsSecretKey)\n",
    "\n",
    "s3_resource = boto3.resource('s3',\n",
    "    endpoint_url='http://lakefs:8000',\n",
    "    aws_access_key_id=lakefsAccessKey,\n",
    "    aws_secret_access_key=lakefsSecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "549d8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=False,\n",
    "                      key=lakefsAccessKey,\n",
    "                      secret=lakefsSecretKey,\n",
    "                      client_kwargs={'endpoint_url': lakefsEndPoint})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37562fc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c212abdd",
   "metadata": {},
   "source": [
    "# Main Tutorial starts here üö¶ üëáüèª"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04cca1d5",
   "metadata": {},
   "source": [
    "1. Show the dataset in MinIO\n",
    "2. Create a new MinIO bucket for lakeFS repository\n",
    "3. Create a lakeFS repository (ml-demo)\n",
    "4. Import dataset into the lakeFS repo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe2521f2",
   "metadata": {},
   "source": [
    "## Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "997200c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_branch = \"_main_imported\"\n",
    "exp1_branch = \"experiment-1\"\n",
    "exp2_branch = \"experiment-2\"\n",
    "\n",
    "prod_branch = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82b6c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{repo_name}\"\n",
    "\n",
    "images_path = \"dogs_dataset_/images/Images\"\n",
    "annotations = \"dogs_dataset_/annotations/Annotations\"\n",
    "\n",
    "raw_path = \"raw\"\n",
    "processed_path = \"processed\"\n",
    "config_path = \"config\"\n",
    "artifact_path = \"artifacts\"\n",
    "metrics_path = \"metrics\"\n",
    "training_code_path = \"src\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4f96762",
   "metadata": {},
   "source": [
    "# Experimentation Begins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811c8da2",
   "metadata": {},
   "source": [
    "## Experiment #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84933890",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp1 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp1_branch,\n",
    "    'image_path': f\"{exp1_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp1_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp1_branch}/{metrics_path}\",\n",
    "    'config_path': f\"{exp1_branch}/{config_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 100,\n",
    "    'is_shuffle':True,\n",
    "    'is_normalize': False,\n",
    "    'epochs': 200,\n",
    "    'train_test_split_ratio': 0.2,\n",
    "    'optimizer': \"adam\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "041f28aa",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #1\n",
    "\n",
    "#### Create a new branch: `experiment-1` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edd15e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pagination': {'has_more': False,\n",
       "                'max_per_page': 1000,\n",
       "                'next_offset': '',\n",
       "                'results': 1},\n",
       " 'results': [{'commit_id': '29fb4ad5ac670f81dd3faad9303877feb75edb384bacd891b584fd4995a7f76b',\n",
       "              'id': 'main'}]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e357713",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundException",
     "evalue": "(404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Request-Id': '7dd65011-2ce4-460d-9c7e-c0735b435919', 'Date': 'Fri, 02 Jun 2023 13:42:05 GMT', 'Content-Length': '59'})\nHTTP response body: {\"message\":\"source reference '_main_imported': not found\"}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lakefs\u001b[39m.\u001b[39;49mbranches\u001b[39m.\u001b[39;49mcreate_branch(repository\u001b[39m=\u001b[39;49mrepo_name, \n\u001b[1;32m      2\u001b[0m                               branch_creation\u001b[39m=\u001b[39;49mBranchCreation(name\u001b[39m=\u001b[39;49mexp1_branch, \n\u001b[1;32m      3\u001b[0m                                                                     source\u001b[39m=\u001b[39;49mingest_branch)\n\u001b[1;32m      4\u001b[0m                              )\n\u001b[1;32m      5\u001b[0m lakefs\u001b[39m.\u001b[39mbranches\u001b[39m.\u001b[39mlist_branches(repo_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/api/branches_api.py:740\u001b[0m, in \u001b[0;36mBranchesApi.create_branch\u001b[0;34m(self, repository, branch_creation, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mrepository\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    737\u001b[0m     repository\n\u001b[1;32m    738\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mbranch_creation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    739\u001b[0m     branch_creation\n\u001b[0;32m--> 740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_branch_endpoint\u001b[39m.\u001b[39;49mcall_with_http_info(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/api_client.py:835\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m         header_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mselect_header_content_type(\n\u001b[1;32m    832\u001b[0m             content_type_headers_list)\n\u001b[1;32m    833\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m header_list\n\u001b[0;32m--> 835\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[1;32m    836\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mendpoint_path\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mhttp_method\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    837\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    838\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    839\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    840\u001b[0m     body\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    841\u001b[0m     post_params\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mform\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    842\u001b[0m     files\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    843\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mresponse_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    844\u001b[0m     auth_settings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mauth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    845\u001b[0m     async_req\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    846\u001b[0m     _check_type\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_check_return_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    847\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    848\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    849\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    850\u001b[0m     _host\u001b[39m=\u001b[39;49m_host,\n\u001b[1;32m    851\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mcollection_format\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/api_client.py:409\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[39mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 409\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[1;32m    410\u001b[0m                            path_params, query_params, header_params,\n\u001b[1;32m    411\u001b[0m                            body, post_params, files,\n\u001b[1;32m    412\u001b[0m                            response_type, auth_settings,\n\u001b[1;32m    413\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[1;32m    414\u001b[0m                            _preload_content, _request_timeout, _host,\n\u001b[1;32m    415\u001b[0m                            _check_type)\n\u001b[1;32m    417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    418\u001b[0m                                                method, path_params,\n\u001b[1;32m    419\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    427\u001b[0m                                                _request_timeout,\n\u001b[1;32m    428\u001b[0m                                                _host, _check_type))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/api_client.py:203\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_response \u001b[39m=\u001b[39m response_data\n\u001b[1;32m    207\u001b[0m return_data \u001b[39m=\u001b[39m response_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/api_client.py:196\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    192\u001b[0m     url \u001b[39m=\u001b[39m _host \u001b[39m+\u001b[39m resource_path\n\u001b[1;32m    194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    197\u001b[0m         method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[1;32m    198\u001b[0m         post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    199\u001b[0m         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    200\u001b[0m         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/api_client.py:455\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mOPTIONS(url,\n\u001b[1;32m    448\u001b[0m                                     query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m    449\u001b[0m                                     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m                                     _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[1;32m    453\u001b[0m                                     body\u001b[39m=\u001b[39mbody)\n\u001b[1;32m    454\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mPOST(url,\n\u001b[1;32m    456\u001b[0m                                  query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    457\u001b[0m                                  headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    458\u001b[0m                                  post_params\u001b[39m=\u001b[39;49mpost_params,\n\u001b[1;32m    459\u001b[0m                                  _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    460\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    461\u001b[0m                                  body\u001b[39m=\u001b[39;49mbody)\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPUT\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mPUT(url,\n\u001b[1;32m    464\u001b[0m                                 query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m    465\u001b[0m                                 headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[1;32m    469\u001b[0m                                 body\u001b[39m=\u001b[39mbody)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/rest.py:267\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPOST\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, post_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m          body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[1;32m    268\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    269\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    270\u001b[0m                         post_params\u001b[39m=\u001b[39;49mpost_params,\n\u001b[1;32m    271\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    272\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    273\u001b[0m                         body\u001b[39m=\u001b[39;49mbody)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lakefs_client/rest.py:221\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39mraise\u001b[39;00m ForbiddenException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m NotFoundException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m500\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m599\u001b[39m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mraise\u001b[39;00m ServiceException(http_resp\u001b[39m=\u001b[39mr)\n",
      "\u001b[0;31mNotFoundException\u001b[0m: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Request-Id': '7dd65011-2ce4-460d-9c7e-c0735b435919', 'Date': 'Fri, 02 Jun 2023 13:42:05 GMT', 'Content-Length': '59'})\nHTTP response body: {\"message\":\"source reference '_main_imported': not found\"}\n\n"
     ]
    }
   ],
   "source": [
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=exp1_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "with open(f'./config.json', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=exp1_branch, \n",
    "                                 path=f\"{config_path}/config.json\", \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1aaf934",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)\n",
    "print(\"Loading training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Commit the training data after preprocessing under /processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af08282d",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, metrics1 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb92fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(metrics1, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e715f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5881d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "502a9d74",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model1, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "print(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4301dc20",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd72477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model1_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c7d91b6",
   "metadata": {},
   "source": [
    "## Experiment #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6211d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp2 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp2_branch,\n",
    "    'image_path': f\"{exp2_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp2_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp2_branch}/{metrics_path}\",\n",
    "    'config_path': f\"{exp2_branch}/{config_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3989e46",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #2\n",
    "\n",
    "1. Create a new branch: `experiment-2` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6202158",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)\n",
    "\n",
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=exp2_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "\n",
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "with open(f'./config.json', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=exp2_branch, \n",
    "                                 path=f\"{config_path}/config.json\", \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81022681",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764664de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Commit training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654d5b5d",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e18b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2, metrics2 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_metrics(metrics2, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a62439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c743e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8e983b4",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model2, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f4d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e262d9e6",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855af9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db225f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model2_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2146e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47198a92",
   "metadata": {},
   "source": [
    "### Compare in both branches. Merge the winning model to Prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch = exp2_branch\n",
    "if metrics1['accuracy']> metrics2['accuracy']:\n",
    "    win_branch = exp1_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c824b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=win_branch, \n",
    "                              destination_branch=prod_branch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c72748a4",
   "metadata": {},
   "source": [
    "## Reproducing ML experiments with lakeFS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_branch = exp1_branch\n",
    "tag = f'{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}_{tag_branch}'\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.tags.create_tag(\n",
    "    repository=repo_name,\n",
    "    tag_creation=TagCreation(\n",
    "        id=tag, \n",
    "        ref=tag_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acf96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tag ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': tag,\n",
    "    'image_path': f\"{tag}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{tag}/{artifact_path}\",\n",
    "    'metrics_path': f\"{tag}/{metrics_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "pprint.pprint(params_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = preprocess(images, labels, params['is_shuffle'], params['is_normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227a8ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_model_reloaded = model_load(params_tag['model_name'], \n",
    "           params_tag['repo_name'], \n",
    "           params_tag['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tag_model_reloaded.predict(images)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb0a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d968288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77f5674e",
   "metadata": {},
   "source": [
    "## DONE!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
