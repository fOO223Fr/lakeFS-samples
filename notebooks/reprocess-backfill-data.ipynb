{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Reprocess and Backfill Data with new ETL logic\n",
    "\n",
    "_Note that whilst this works, it's a bit of a hack!_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67662314-7dcd-4e6c-869f-24a1a76d1a0e",
   "metadata": {},
   "source": [
    "You will run following steps in this notebook (refer to the image below):\n",
    "\n",
    "1. Create repository with the Main branch\n",
    "2. Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "3. Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "4. Repetition of step # 2\n",
    "5. Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes.\n",
    "6. Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d68a0676-60ff-47e8-af5b-fa44206dcb3d",
   "metadata": {},
   "source": [
    "![Reprocess](./images/reprocess-data/Reprocess.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ae6f67a-d18c-43f1-b4ee-8ce2e3038857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1a1744d-4095-4497-a443-c2cb76d0ecee",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410b2f7-bbd1-48f2-88cd-43440e4cd8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc6b274-8d00-4fbf-830b-339fd29e0ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1c20a-e248-4596-a99b-a7a3784cf9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "037e6a97-c4c3-42b4-8881-568878f54d04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae22a8f6-ae2d-449c-b5ae-6b0abc720210",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fbab3-5f98-46ad-8e12-9021ed2bf81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"reprocess-backfill-data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0296a658-9fa0-470c-b829-45d0e7dd1164",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795b1e7-761e-4f7a-840b-982f99ff3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7c9217c-af15-48a1-ba77-c497d93b9cda",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbe675-3c5e-41c7-8471-7a8a90253827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version{v.version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1441793-7174-4f6f-b400-7eca020ad561",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7196c9-370a-441a-8ea3-02ee03484991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ea20cc8-9a7b-4285-a675-7373a0fb65cc",
   "metadata": {},
   "source": [
    "### Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b839850-5954-4631-8e7e-d0dee6d17dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestBranch = \"ingest\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "processedFileName = \"lakefs_test_processed.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db425f40-db8d-4298-8cdb-c532fc98fc97",
   "metadata": {},
   "source": [
    "### Define data file schema + python libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42f399-19b7-4def-9988-916a7f8ba067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "dataFileSchema = StructType([\n",
    "  StructField(\"Apparel_Sales\", DoubleType(), False),\n",
    "  StructField(\"Books_Sales\", DoubleType(), False),\n",
    "  StructField(\"Electronics_Sales\", DoubleType(), False),\n",
    "  StructField(\"Furniture_Sales\", DoubleType(), False),\n",
    "  StructField(\"Toys_Sales\", DoubleType(), False)\n",
    "])\n",
    "processedDataFileSchema = StructType([\n",
    "  StructField(\"Apparel_Sales\", DoubleType(), False),\n",
    "  StructField(\"Books_Sales\", DoubleType(), False),\n",
    "  StructField(\"Electronics_Sales\", DoubleType(), False),\n",
    "  StructField(\"Furniture_Sales\", DoubleType(), False),\n",
    "  StructField(\"Toys_Sales\", DoubleType(), False),\n",
    "  StructField(\"Total_Sales\", DoubleType(), False),\n",
    "  StructField(\"Average_Sales_per_Product_Category\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bf16b13-7300-4840-a3a9-91765e945b3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8431be3f-d741-4cb6-98cf-8b21b8f2a489",
   "metadata": {},
   "source": [
    "## Step 1: Create repository with the Main branch\n",
    "\n",
    "### (if above mentioned repo already exists on your lakeFS server then you can skip this operation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e2218d-3af0-415d-a35a-ab4ba06ac040",
   "metadata": {},
   "source": [
    "![Step 1](./images/reprocess-data/Step1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384aa4d3-038a-4a56-b30e-c536e3911478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e360b5f6-76db-438b-9aa0-e40070b86bb9",
   "metadata": {},
   "source": [
    "## Step 2: Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "\n",
    "### ([ETL](./ReprocessData/ETL.ipynb) job normally run as a batch job but run ETL job manually here for the demo. It will take around a minute to run this step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81e2b2de-617e-4485-8b41-e471feae8a93",
   "metadata": {},
   "source": [
    "![Step 2](./images/reprocess-data/Step2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7db274-07b5-49f9-b7de-5b912818317c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./reprocess-data/etl.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2ca865d-0aa0-46bb-b6f2-54d7205dda64",
   "metadata": {},
   "source": [
    "# Reprocessing Starts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ace14847-e55c-4b53-8648-0ae88eaebbad",
   "metadata": {},
   "source": [
    "## Step 3: Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "### (you can change the name for reprocessing branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job here)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8029d9f-2f74-4cb1-950f-616ef69c063d",
   "metadata": {},
   "source": [
    "![Step 3](./images/reprocess-data/Step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732966a0-f4e1-4a81-98ae-89b9c0799b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reprocessBranch = \"new-logic\"\n",
    "%run ./reprocess-data/reprocessing.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57bc3b3d-3e5e-4be7-82b4-10124d85613e",
   "metadata": {},
   "source": [
    "## While ETL logic is getting fixed, old ETL job is still running in parallel.\n",
    "\n",
    "## Received new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25504879-37e8-46b2-8c3f-88596fe0240b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_new.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cedbbaaa-51cf-4986-be84-d6a22997e706",
   "metadata": {},
   "source": [
    "## Step 4: Repetition of step # 2\n",
    "\n",
    "### (run [ETL](./ReprocessData/ETL.ipynb) job again)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82d26f30-afc7-4a10-a3b9-6486a48dff7a",
   "metadata": {},
   "source": [
    "![Step 4](./images/reprocess-data/Step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25463f0e-e09e-4c46-9baf-97c2a2e4a4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./reprocess-data/etl.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "002fcd72-417c-4079-a051-6ee8fe6662ac",
   "metadata": {},
   "source": [
    "## Now Reprocessing branch is behind Main branch in terms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e289db8-a33d-4c70-933f-e24282fd0788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + reprocessBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{reprocessBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0462b6-c8a1-4212-bb1b-54ffbfda4c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "870d894e-d6bb-4a2d-8132-c405b8defabb",
   "metadata": {},
   "source": [
    "## Once ETL logic is fixed, pause the old ETL job to deploy new ETL logic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "677e5cbd-0a0a-48ff-827e-68dc0dd4b712",
   "metadata": {},
   "source": [
    "## Step 5: Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes\n",
    "### (you can change the name for the \"Backfill and Deploy\" branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job again on \"Backfill and Deploy\" branch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74206d1e-9ec8-4209-a940-49f7b2d333f6",
   "metadata": {},
   "source": [
    "![Step 5](./images/reprocess-data/Step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216328cf-29b7-4aa1-bfce-0bddbed58590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backfillAndDeployBranch = \"backfill-and-deploy\"\n",
    "reprocessBranch = backfillAndDeployBranch\n",
    "%run ./reprocess-data/reprocessing.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19852e24-d198-4d0f-98fe-d756baf586e2",
   "metadata": {},
   "source": [
    "## Now \"Backfill and Deploy\" branch has same data as Main branch and correct ETL logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573cfb2-049c-4617-ba99-b169c84cfb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + backfillAndDeployBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{backfillAndDeployBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2fc43-9056-414c-9516-d1eb6da77631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d113388d-94b3-4bf9-807f-0ea08360089e",
   "metadata": {},
   "source": [
    "## Step 6: Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04df9f1d-c21a-4ab3-b7f0-f2aa5bb7fd4b",
   "metadata": {},
   "source": [
    "![Step 6](./images/reprocess-data/Step6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40762c9-0344-4cbc-b28a-12673f30a670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id, source_ref=backfillAndDeployBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5762d1a0-276f-4a20-8ca0-18f4a463a622",
   "metadata": {},
   "source": [
    "# Reprocessing and Backfill completes\n",
    "\n",
    "## Verify data on Main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f67d-98ad-45f8-b52d-a4a8a9860443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e583e2de-b113-4e31-9bab-1de5feb16f9d",
   "metadata": {},
   "source": [
    "## Now you can schedule the new ETL job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7b24a6f-56e6-4bd3-b898-0ff0c188e223",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
