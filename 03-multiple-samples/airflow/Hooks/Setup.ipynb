{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "# Integration of lakeFS with Airflow via Hooks\n",
    "\n",
    "## Use Case: Isolated Ingestion & ETL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca884fa-ced3-416a-9e82-e66e544a0bb8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "###### This Notebook requires connecting to a lakeFS Server. \n",
    "###### To spin up lakeFS quickly - use the Playground (https://demo.lakefs.io) which provides lakeFS server on-demand with a single click; \n",
    "###### Or, alternatively, refer to lakeFS Quickstart doc (https://docs.lakefs.io/quickstart/run.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eca3eb-94ca-4176-8297-4d40b18f5834",
   "metadata": {},
   "source": [
    "## Change your lakeFS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b903ff7-123a-4a65-80f1-f84705c7105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://host.docker.internal:8000'\n",
    "lakefsUIEndPoint = 'http://127.0.0.1:8000'\n",
    "lakefsAccessKey = 'AKIAJ64AEHEF4JPRG24Q'\n",
    "lakefsSecretKey = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec4b59-b71c-4def-b4a2-9a4ea63f0738",
   "metadata": {},
   "source": [
    "## Storage Information\n",
    "#### Change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578851b6-4be3-4c37-99f4-8ed7449b2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 'local://ml-data-repo' # e.g. \"s3://username-lakefs-cloud/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916f597-b2d3-4ba8-ab97-35e6cb3673c7",
   "metadata": {},
   "source": [
    "## Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28136c1e-e8ad-4843-a4be-03300073fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceBranch = \"main\"\n",
    "newBranch = \"ingest\"\n",
    "airflowBranch = \"etl_airflow\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "newPath = \"partitioned_data\"\n",
    "successFileName = \"success.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eaec38-02cc-44df-8d66-f68b4e6071d6",
   "metadata": {},
   "source": [
    "## You can change lakeFS repo name (it can be an existing repo or provide another repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559c6aa-49d6-4071-a624-d6d7d693527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"ml-data-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03a5b0-2557-4ee1-bd0a-d46fd2452a7f",
   "metadata": {},
   "source": [
    "## Working with the lakeFS Python client API\n",
    "\n",
    "###### Note: To learn more about lakeFS Python integration visit https://docs.lakefs.io/integrations/python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf5659-2a56-48cb-bc2d-aa469551dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "if not 'client' in locals():\n",
    "    import lakefs_client\n",
    "    from lakefs_client import models\n",
    "    from lakefs_client.client import LakeFSClient\n",
    "\n",
    "    # lakeFS credentials and endpoint\n",
    "    configuration = lakefs_client.Configuration()\n",
    "    configuration.username = lakefsAccessKey\n",
    "    configuration.password = lakefsSecretKey\n",
    "    configuration.host = lakefsEndPoint\n",
    "\n",
    "    client = LakeFSClient(configuration)\n",
    "    print(\"Created lakeFS client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d893f-2e21-4835-a085-bb0bafbafe16",
   "metadata": {},
   "source": [
    "## Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f3626-0e16-4211-9156-3dd4e88fbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentials\")\n",
    "client.config.get_lake_fs_version()\n",
    "print(\"lakeFS credentials verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378caff2-6f10-4ec4-b3b2-61311cebe3da",
   "metadata": {},
   "source": [
    "## S3A Gateway configuration\n",
    "\n",
    "##### Note: lakeFS can be configured to work with Spark in two ways:\n",
    "###### * Access lakeFS using the S3A gateway https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-s3a-gateway.\n",
    "###### * Access lakeFS using the lakeFS-specific Hadoop FileSystem https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec927dba-e131-4524-92ae-ec8389132e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc949c1-82f4-47ee-97fd-f55605b496e9",
   "metadata": {},
   "source": [
    "## Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8338423-e478-4ca9-8b80-6ec04769da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Airflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ee38e-49b1-423b-8577-d3e802518edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pkill airflow\n",
    "! pkill airflow\n",
    "! pkill airflow\n",
    "! pkill airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74000dcb-539e-4814-ae06-3f0c83b0e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae18802-b39d-4ddc-8010-2223f96051ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "airflow standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af218ac-f25c-475f-ade9-3efc2b353263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Airflow to start\n",
    "! sleep 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4d90f-e236-40eb-9d14-6a919f104565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script bash --bg --out script_out --err script_error\n",
    "#airflow scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f56fa-c0bd-4be7-b186-06991035fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Airflow Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4729f-5f2e-4327-b79b-fbd78a2c0ad5",
   "metadata": {},
   "source": [
    "## Create Airflow connections for lakeFS and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d346dbb-8259-4bf0-996b-04c334e46a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow connections delete conn_lakefs\n",
    "lakeFSConnectionCommand = 'airflow connections add conn_lakefs --conn-type=http --conn-host=' + lakefsEndPoint + ' --conn-extra=\\'{\"access_key_id\":\"' + lakefsAccessKey + '\",\"secret_access_key\":\"' + lakefsSecretKey + '\"}\\''\n",
    "! $lakeFSConnectionCommand\n",
    "\n",
    "! airflow connections delete conn_spark\n",
    "sparkConnectionCommand = 'airflow connections add conn_spark --conn-type=spark --conn-host=local[*]'\n",
    "! $sparkConnectionCommand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c36a-4a47-41c5-a000-b5708d53ee33",
   "metadata": {},
   "source": [
    "## Set Airflow variables which are used by the demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012d111-e45b-4030-96da-6f270437fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow variables set lakefsAccessKey $lakefsAccessKey\n",
    "! airflow variables set lakefsSecretKey $lakefsSecretKey\n",
    "! airflow variables set lakefsEndPoint $lakefsEndPoint\n",
    "! airflow variables set lakefsUIEndPoint $lakefsUIEndPoint\n",
    "! airflow variables set repo $repo\n",
    "! airflow variables set sourceBranch $newBranch\n",
    "! airflow variables set newBranch $airflowBranch\n",
    "! airflow variables set fileName $fileName\n",
    "! airflow variables set newPath $newPath\n",
    "! airflow variables set successFileName $successFileName\n",
    "! airflow variables set conn_lakefs 'conn_lakefs'\n",
    "\n",
    "import os\n",
    "spark_home = os.getenv('SPARK_HOME')\n",
    "! airflow variables set spark_home $spark_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6792e-b1be-4c26-90cd-e9d54924354f",
   "metadata": {},
   "source": [
    "## Copy DAG programs to Airflow DAGs directory and sync to Airflow database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f31409a-f75c-4b2d-97ad-f82eb18dd776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat './airflow/Hooks/lakefs_hooks_post_commit_dag.py': No such file or directory\n",
      "cp: cannot stat './airflow/Hooks/lakefs_hooks_pre_merge_dag.py': No such file or directory\n",
      "[\u001b[34m2022-12-07 23:16:51,676\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m508} INFO\u001b[0m - Filling up the DagBag from /home/jovyan/airflow/dags\u001b[0m\n",
      "[\u001b[34m2022-12-07 23:16:51,976\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m321} ERROR\u001b[0m - Failed to import: /home/jovyan/airflow/dags/.ipynb_checkpoints/lakefs_hooks_post_commit_dag-checkpoint.py\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/airflow/models/dagbag.py\", line 318, in parse\n",
      "    loader.exec_module(new_module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/airflow/dags/.ipynb_checkpoints/lakefs_hooks_post_commit_dag-checkpoint.py\", line 100, in <module>\n",
      "    post_commit_dag = lakefs_hooks_post_commit_dag()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/airflow/models/dag.py\", line 3020, in factory\n",
      "    f(**f_kwargs)\n",
      "  File \"/home/jovyan/airflow/dags/.ipynb_checkpoints/lakefs_hooks_post_commit_dag-checkpoint.py\", line 67, in lakefs_hooks_post_commit_dag\n",
      "    task_create_success_file = LakeFSUploadOperator(\n",
      "NameError: name 'LakeFSUploadOperator' is not defined\n",
      "[\u001b[34m2022-12-07 23:16:52,009\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m475} ERROR\u001b[0m - Exception bagging dag: lakefs_hooks_pre_merge_dag\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/airflow/models/dagbag.py\", line 466, in _bag_dag\n",
      "    raise AirflowDagDuplicatedIdException(\n",
      "airflow.exceptions.AirflowDagDuplicatedIdException: Ignoring DAG lakefs_hooks_pre_merge_dag from /home/jovyan/airflow/dags/.ipynb_checkpoints/lakefs_hooks_pre_merge_dag-checkpoint.py - also found in /home/jovyan/airflow/dags/lakefs_hooks_pre_merge_dag.py\n",
      "[\u001b[34m2022-12-07 23:16:52,010\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m419} ERROR\u001b[0m - Failed to bag_dag: /home/jovyan/airflow/dags/.ipynb_checkpoints/lakefs_hooks_pre_merge_dag-checkpoint.py\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/airflow/models/dagbag.py\", line 407, in _process_modules\n",
      "    self.bag_dag(dag=dag, root_dag=dag)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/airflow/models/dagbag.py\", line 434, in bag_dag\n",
      "    self._bag_dag(dag=dag, root_dag=root_dag, recursive=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/airflow/models/dagbag.py\", line 466, in _bag_dag\n",
      "    raise AirflowDagDuplicatedIdException(\n",
      "airflow.exceptions.AirflowDagDuplicatedIdException: Ignoring DAG lakefs_hooks_pre_merge_dag from /home/jovyan/airflow/dags/.ipynb_checkpoints/lakefs_hooks_pre_merge_dag-checkpoint.py - also found in /home/jovyan/airflow/dags/lakefs_hooks_pre_merge_dag.py\n",
      "[\u001b[34m2022-12-07 23:16:52,056\u001b[0m] {\u001b[34mdag.py:\u001b[0m2420} INFO\u001b[0m - Sync 2 DAGs\u001b[0m\n",
      "[\u001b[34m2022-12-07 23:16:52,077\u001b[0m] {\u001b[34mdag.py:\u001b[0m2968} INFO\u001b[0m - Setting next_dagrun for lakefs_hooks_post_commit_dag to None, run_after=None\u001b[0m\n",
      "[\u001b[34m2022-12-07 23:16:52,078\u001b[0m] {\u001b[34mdag.py:\u001b[0m2968} INFO\u001b[0m - Setting next_dagrun for lakefs_hooks_pre_merge_dag to None, run_after=None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! cp ./airflow/Hooks/lakefs_hooks_post_commit_dag.py ./airflow/dags\n",
    "! cp ./airflow/Hooks/lakefs_hooks_pre_merge_dag.py ./airflow/dags\n",
    "\n",
    "from airflow.models import DagBag\n",
    "dagbag = DagBag(include_examples=False)\n",
    "dagbag.sync_to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43fcb5-96c1-4b91-8ab4-0f943bd5343c",
   "metadata": {},
   "source": [
    "## Unpause Airflow DAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f083dbcb-0a5c-42dd-8408-82e9baa41774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dag: lakefs_hooks_post_commit_dag, paused: False\n",
      "Dag: lakefs_hooks_pre_merge_dag, paused: False\n"
     ]
    }
   ],
   "source": [
    "! airflow dags unpause lakefs_hooks_post_commit_dag\n",
    "! airflow dags unpause lakefs_hooks_pre_merge_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8baac0b-ee59-4611-a227-49cad78d0797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
