{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "# Integration of lakeFS with Airflow via Hooks\n",
    "\n",
    "## Use Case: Versioning DAGs and running pipeline from hooks using a configurable version of DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45932d7-2efe-4017-9fc3-822d92897884",
   "metadata": {},
   "source": [
    "## Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfaa39-dbac-4d1c-91ec-25428013b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceBranch = \"main\"\n",
    "newBranch = \"version1\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "local_path = 'airflow/DAG_Versioning'\n",
    "dags_folder_on_lakefs = 'dags'\n",
    "data_folder_on_lakefs = 'data'\n",
    "actions_folder_on_lakefs = '_lakefs_actions'\n",
    "newPath = data_folder_on_lakefs + '/' + \"partitioned_data\"\n",
    "dag_name = 'lakefs_versioning_dag'\n",
    "dag_template_filename = 'lakefs_versioning_dag_template.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24fa50-8557-4a9e-8463-f093ce8d2bf9",
   "metadata": {},
   "source": [
    "## Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfe84c-fce2-4be0-8314-073c6b9aa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "import lakefs_demo\n",
    "import os\n",
    "from airflow.models import DagBag\n",
    "\n",
    "# importing sys\n",
    "import sys\n",
    " \n",
    "# adding folder to the system path\n",
    "sys.path.insert(0, './'+local_path)\n",
    " \n",
    "from lakefs_create_dag import lakefs_create_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ede096-1b84-4b59-bdd1-2608ced6be51",
   "metadata": {},
   "source": [
    "## Working with the lakeFS Python client API\n",
    "\n",
    "###### Note: To learn more about lakeFS Python integration visit https://docs.lakefs.io/integrations/python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e98e4-7e06-4373-b36b-f1763cc7755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144747fc-cade-4929-8ae2-d488091ff273",
   "metadata": {},
   "source": [
    "## Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20525e09-3b2a-4ac1-827f-c0d87a6b0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentials\")\n",
    "client.config.get_lake_fs_version()\n",
    "print(\"lakeFS credentials verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3db5c-4745-4c90-9ea8-5b22165bbfbe",
   "metadata": {},
   "source": [
    "## S3A Gateway configuration\n",
    "\n",
    "##### Note: lakeFS can be configured to work with Spark in two ways:\n",
    "###### * Access lakeFS using the S3A gateway https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-s3a-gateway.\n",
    "###### * Access lakeFS using the lakeFS-specific Hadoop FileSystem https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af90a80-7944-4ec1-8e41-7130e7548250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f0f3a-da95-4c09-b8b5-4c00380ce913",
   "metadata": {},
   "source": [
    "## Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8338423-e478-4ca9-8b80-6ec04769da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Airflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ee38e-49b1-423b-8577-d3e802518edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pkill airflow\n",
    "! pkill airflow\n",
    "! pkill airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e9ecb-1e7b-406b-a3f1-e7fa4b67117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae18802-b39d-4ddc-8010-2223f96051ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "airflow standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af218ac-f25c-475f-ade9-3efc2b353263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Airflow to start\n",
    "! sleep 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f56fa-c0bd-4be7-b186-06991035fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Airflow Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4729f-5f2e-4327-b79b-fbd78a2c0ad5",
   "metadata": {},
   "source": [
    "## Create Airflow connections for lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d346dbb-8259-4bf0-996b-04c334e46a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow connections delete conn_lakefs\n",
    "lakeFSConnectionCommand = 'airflow connections add conn_lakefs --conn-type=http --conn-host=' + lakefsEndPoint + ' --conn-extra=\\'{\"access_key_id\":\"' + lakefsAccessKey + '\",\"secret_access_key\":\"' + lakefsSecretKey + '\"}\\''\n",
    "! $lakeFSConnectionCommand\n",
    "\n",
    "! airflow connections delete conn_spark\n",
    "sparkConnectionCommand = 'airflow connections add conn_spark --conn-type=spark --conn-host=local[*]'\n",
    "! $sparkConnectionCommand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c36a-4a47-41c5-a000-b5708d53ee33",
   "metadata": {},
   "source": [
    "## Set Airflow variables which are used by the demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced25d00-c2c5-4c72-8ab6-c6f6b8e62c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow variables set lakefsAccessKey $lakefsAccessKey\n",
    "! airflow variables set lakefsSecretKey $lakefsSecretKey\n",
    "! airflow variables set lakefsEndPoint $lakefsEndPoint\n",
    "! airflow variables set lakefsUIEndPoint $lakefsUIEndPoint\n",
    "! airflow variables set repo $repo\n",
    "! airflow variables set sourceBranch $sourceBranch\n",
    "! airflow variables set newBranch $newBranch\n",
    "fileName_on_lakefs = data_folder_on_lakefs + '/' + fileName\n",
    "! airflow variables set fileName $fileName_on_lakefs\n",
    "! airflow variables set newPath $newPath\n",
    "! airflow variables set conn_lakefs 'conn_lakefs'\n",
    "! airflow variables set dags_folder_on_lakefs $dags_folder_on_lakefs\n",
    "! airflow variables set dag_name $dag_name\n",
    "! airflow variables set dag_template_filename $dag_template_filename\n",
    "\n",
    "spark_home = os.getenv('SPARK_HOME')\n",
    "! airflow variables set spark_home $spark_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7120e1-83ca-4c58-b1e1-cb83128c8b09",
   "metadata": {},
   "source": [
    "## Copy DAG programs to Airflow DAGs directory and sync to Airflow database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa830c-e7c7-4d7b-b6be-2716c894d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ./airflow/DAG_Versioning/lakefs_create_dag_auto.py ./airflow/dags\n",
    "! cp ./airflow/DAG_Versioning/lakefs_delete_dag_auto.py ./airflow/dags\n",
    "! cp ./airflow/DAG_Versioning/lakefs_trigger_dag_auto.py ./airflow/dags\n",
    "\n",
    "dagbag = DagBag(include_examples=False)\n",
    "dagbag.sync_to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32738a9-d669-4845-a5c3-b4c5f8af78fb",
   "metadata": {},
   "source": [
    "## Unpause Airflow DAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6625b71-1665-4780-ae4f-1076fd2a8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow dags unpause lakefs_create_dag\n",
    "! airflow dags unpause lakefs_delete_dag\n",
    "! airflow dags unpause lakefs_trigger_dag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
