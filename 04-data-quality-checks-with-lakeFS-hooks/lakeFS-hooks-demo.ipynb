{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0d51de",
   "metadata": {},
   "source": [
    "### Installing lakeFS python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f557468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lakefs_client in /opt/conda/lib/python3.9/site-packages (0.83.4)\r\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (2.8.2)\r\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (1.26.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil->lakefs_client) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lakefs_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a56afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cb2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d40ca",
   "metadata": {},
   "source": [
    "### Configuring lakeFSClient and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65cfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AccessKey and SecretKey are present in the docker-compose.yaml file we used to spin up the everything bagel\n",
    "lakefsAccessKey = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "lakefsSecretKey = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "lakefsEndPoint = \"http://lakefs:8000\"\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8125e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b4453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LakeFSClient(configuration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07acc4",
   "metadata": {},
   "source": [
    "## Creating Ingest and Staging branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997200c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"example\"\n",
    "\n",
    "ingest_branch = \"ingest-landing-area\"\n",
    "staging_branch = \"staging-area\"\n",
    "prod_branch = \"main\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651c7e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pagination': {'has_more': False,\n",
       "                'max_per_page': 1000,\n",
       "                'next_offset': '',\n",
       "                'results': 1},\n",
       " 'results': [{'commit_id': '9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af',\n",
       "              'id': 'main'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.branches.list_branches(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac2681e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=ingest_branch, \n",
    "                                                                    source=prod_branch)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ee13a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=staging_branch, \n",
    "                                                                    source=prod_branch)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eda20a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pagination': {'has_more': False,\n",
       "                'max_per_page': 1000,\n",
       "                'next_offset': '',\n",
       "                'results': 3},\n",
       " 'results': [{'commit_id': '9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af',\n",
       "              'id': 'ingest-landing-area'},\n",
       "             {'commit_id': '9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af',\n",
       "              'id': 'main'},\n",
       "             {'commit_id': '9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af',\n",
       "              'id': 'staging-area'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.branches.list_branches(repo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22215b",
   "metadata": {},
   "source": [
    "## Uploading movies data to ingest branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897649d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dt=2022-11-02/movies.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_data = \"movies.csv\"\n",
    "\n",
    "ingest_path = f'dt={str(date.today())}/{ingest_data}'\n",
    "ingest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b87b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{ingest_data}', 'rb') as f:\n",
    "    client.objects.upload_object(repository=repo_name, \n",
    "                                 branch=ingest_branch, \n",
    "                                 path=ingest_path, \n",
    "                                 content=f\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd829e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'dt=2022-11-02/movies.csv',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 1071619,\n",
       "  'type': 'added'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=ingest_branch).results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b96c0e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1667365963,\n",
       " 'id': '28229cfc8f387d8207dcf83df4d0c9aa12d83bbe2af0fe074ee88386b8581e0e',\n",
       " 'message': \"netflix movie data arrived at landing area (today's partition)\",\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=ingest_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=\"netflix movie data arrived at landing area (today's partition)\")\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a6c11",
   "metadata": {},
   "source": [
    "## Uploading actions.yaml config file to staging branch\n",
    "\n",
    "* We want to run data quality tests on staging branch before merging the data into production. Hooks config file `actions.yaml` needs to be in the branch on which the tests are run.\n",
    "\n",
    "* So add `_lakefs_actions/actions.yaml` to staging branch\n",
    "* `actions.yaml` contains a pre-merge hook configured to check for file format validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "106d2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks_config_yaml = \"actions.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f22bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{hooks_config_yaml}', 'rb') as f:\n",
    "    client.objects.upload_object(repository=repo_name, \n",
    "                                 branch=staging_branch, \n",
    "                                 path=f'{hooks_prefix}/{hooks_config_yaml}', \n",
    "                                 content=f\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09b677a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '_lakefs_actions/actions.yaml',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 420,\n",
       "  'type': 'added'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=staging_branch).results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06aed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1667365963,\n",
       " 'id': 'e9be752c525cb5ea37e25dd50106fbbf001f08d6692a970c47734be727b84189',\n",
       " 'message': 'Added hooks config file - actions.yaml to staging area',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['9c133ddbfc5d23978b8fba7345b8f19a6e2ac7d7a7ec9511ef164018ae4624af']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message='Added hooks config file - actions.yaml to staging area')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d4979",
   "metadata": {},
   "source": [
    "## Extracting data from ingest branch for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41981c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://example/ingest-landing-area/dt=2022-11-02/movies.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_long_path = f\"s3a://{repo_name}/{ingest_branch}/{ingest_path}\"\n",
    "ingest_long_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dad4a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8791\n",
      "root\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "movies_df = spark.read.option(\"header\",\"true\").csv(ingest_long_path)\n",
    "print(movies_df.count())\n",
    "print(movies_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65193a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+-------------------+--------------+----------+------------+------+---------+--------------------+\n",
      "|show_id|   type|               title|           director|       country|date_added|release_year|rating| duration|           listed_in|\n",
      "+-------+-------+--------------------+-------------------+--------------+----------+------------+------+---------+--------------------+\n",
      "|     s1|  Movie|Dick Johnson Is Dead|    Kirsten Johnson| United States| 9/25/2021|        2020| PG-13|   90 min|       Documentaries|\n",
      "|     s3|TV Show|           Ganglands|    Julien Leclercq|        France| 9/24/2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|\n",
      "|     s6|TV Show|       Midnight Mass|      Mike Flanagan| United States| 9/24/2021|        2021| TV-MA| 1 Season|TV Dramas, TV Hor...|\n",
      "|    s14|  Movie|Confessions of an...|      Bruno Garotti|        Brazil| 9/22/2021|        2021| TV-PG|   91 min|Children & Family...|\n",
      "|     s8|  Movie|             Sankofa|       Haile Gerima| United States| 9/24/2021|        1993| TV-MA|  125 min|Dramas, Independe...|\n",
      "|     s9|TV Show|The Great British...|    Andy Devonshire|United Kingdom| 9/24/2021|        2021| TV-14|9 Seasons|British TV Shows,...|\n",
      "|    s10|  Movie|        The Starling|     Theodore Melfi| United States| 9/24/2021|        2021| PG-13|  104 min|    Comedies, Dramas|\n",
      "|   s939|  Movie|Motu Patlu in the...|        Suhas Kadav|         India|  5/1/2021|        2019| TV-Y7|   87 min|Children & Family...|\n",
      "|    s13|  Movie|        Je Suis Karl|Christian Schwochow|       Germany| 9/23/2021|        2021| TV-MA|  127 min|Dramas, Internati...|\n",
      "|   s940|  Movie|Motu Patlu in Won...|        Suhas Kadav|         India|  5/1/2021|        2013| TV-Y7|   76 min|Children & Family...|\n",
      "+-------+-------+--------------------+-------------------+--------------+----------+------------+------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "554e1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.sample(False,0.1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c8c5f",
   "metadata": {},
   "source": [
    "## Loading transformed data into Staging Area/Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cee0af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://example/staging-area'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_long_path = f\"s3a://{repo_name}/{staging_branch}\"\n",
    "staging_long_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dde9d",
   "metadata": {},
   "source": [
    "## Scenario #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9676d4",
   "metadata": {},
   "source": [
    "### Writing parquet files to staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce5577b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.write.option(\"header\",True)\\\n",
    "        .partitionBy(\"type\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .parquet(f\"{staging_long_path}/analytics/movies-by-type-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "babad0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1667365976,\n",
       " 'id': '0d4de5b8a83357928c63e9e9e34b77ae86050125a67e2767b83e6dc90a48a2b8',\n",
       " 'message': 'loaded paritioned movies parquet to staging area',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['e9be752c525cb5ea37e25dd50106fbbf001f08d6692a970c47734be727b84189']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message='loaded paritioned movies parquet to staging area'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec9501",
   "metadata": {},
   "source": [
    "### Pushing parquet files to Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "831a051f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '900bb7b4c9800e511c8187189620a20a56998cc533e5f8eaed3d070b6e3eb553',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=staging_branch, \n",
    "                              destination_branch=prod_branch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7867f",
   "metadata": {},
   "source": [
    "## Scenario #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172a4f0",
   "metadata": {},
   "source": [
    "### Writing csv files to staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79b8cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.write.option(\"header\",True)\\\n",
    "        .partitionBy(\"type\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .csv(f\"{staging_long_path}/analytics/movies-by-type-csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfbc736c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1667365978,\n",
       " 'id': '6e76a6d1173908888beaf0fb6fa97f3908d82104095f7695a16fe576b375df6c',\n",
       " 'message': 'loaded paritioned movies csv to staging area',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['0d4de5b8a83357928c63e9e9e34b77ae86050125a67e2767b83e6dc90a48a2b8']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message='loaded paritioned movies csv to staging area'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e839f",
   "metadata": {},
   "source": [
    "### Pushing csv files to Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a70f5c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(412)\nReason: Precondition Failed\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Request-Id': 'e134c599-c241-4396-b09f-5224a0e9a403', 'Date': 'Wed, 02 Nov 2022 05:12:58 GMT', 'Content-Length': '261'})\nHTTP response body: {\"message\":\"update branch main: pre-merge hook aborted, run id '5n99v9n42i5s773bapug': 1 error occurred:\\n\\t* hook run id '0000_0000' failed on action 'ParquetOnlyInProduction' hook 'production_format_validator': webhook request failed (status code: 400)\\n\\n\"}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_627/25130392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m client.refs.merge_into_branch(repository=repo_name, \n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0msource_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstaging_branch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               destination_branch=prod_branch)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/api/refs_api.py\u001b[0m in \u001b[0;36mmerge_into_branch\u001b[0;34m(self, repository, source_ref, destination_branch, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'destination_branch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0mdestination_branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_into_branch_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     def restore_refs(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endpoint_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'http_method'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \"\"\"\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             return self.__call_api(resource_path, method,\n\u001b[0m\u001b[1;32m    410\u001b[0m                                    \u001b[0mpath_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                                    \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    453\u001b[0m                                             body=body)\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             return self.rest_client.POST(url,\n\u001b[0m\u001b[1;32m    456\u001b[0m                                          \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                          \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    265\u001b[0m     def POST(self, url, headers=None, query_params=None, post_params=None,\n\u001b[1;32m    266\u001b[0m              body=None, _preload_content=True, _request_timeout=None):\n\u001b[0;32m--> 267\u001b[0;31m         return self.request(\"POST\", url,\n\u001b[0m\u001b[1;32m    268\u001b[0m                             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                             \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/lakefs_client/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mServiceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (412)\nReason: Precondition Failed\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Request-Id': 'e134c599-c241-4396-b09f-5224a0e9a403', 'Date': 'Wed, 02 Nov 2022 05:12:58 GMT', 'Content-Length': '261'})\nHTTP response body: {\"message\":\"update branch main: pre-merge hook aborted, run id '5n99v9n42i5s773bapug': 1 error occurred:\\n\\t* hook run id '0000_0000' failed on action 'ParquetOnlyInProduction' hook 'production_format_validator': webhook request failed (status code: 400)\\n\\n\"}\n\n"
     ]
    }
   ],
   "source": [
    "client.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=staging_branch, \n",
    "                              destination_branch=prod_branch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831cb9b",
   "metadata": {},
   "source": [
    "### Why did the merge operation fail?\n",
    "If you look deeper into the error log, you'll see that the merge request failed with status code '412' (precondition failed). The actions file was executed and blocked a commit with a csv file to merge into main.\n",
    "* Hint: You can see previous actions run [here](http://localhost:8000/repositories/example/actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
