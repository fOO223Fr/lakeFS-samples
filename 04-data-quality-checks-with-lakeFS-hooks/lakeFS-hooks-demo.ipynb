{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0d51de",
   "metadata": {},
   "source": [
    "### Installing lakeFS python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f557468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lakefs_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a56afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d40ca",
   "metadata": {},
   "source": [
    "### Configuring lakeFSClient and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AccessKey and SecretKey are present in the docker-compose.yaml file we used to spin up the everything bagel\n",
    "lakefsAccessKey = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "lakefsSecretKey = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "lakefsEndPoint = \"http://lakefs:8000\"\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8125e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LakeFSClient(configuration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07acc4",
   "metadata": {},
   "source": [
    "## Creating Ingest and Staging branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997200c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"example\"\n",
    "\n",
    "ingest_branch = \"ingest-landing-area\"\n",
    "staging_branch = \"staging-area\"\n",
    "prod_branch = \"main\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.list_branches(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2681e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=ingest_branch, \n",
    "                                                                    source=prod_branch)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=staging_branch, \n",
    "                                                                    source=prod_branch)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda20a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.list_branches(repo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22215b",
   "metadata": {},
   "source": [
    "## Uploading movies data to ingest branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897649d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data = \"movies.csv\"\n",
    "\n",
    "ingest_path = f'dt={str(date.today())}/{ingest_data}'\n",
    "ingest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{ingest_data}', 'rb') as f:\n",
    "    client.objects.upload_object(repository=repo_name, \n",
    "                                 branch=ingest_branch, \n",
    "                                 path=ingest_path, \n",
    "                                 content=f\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd829e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=ingest_branch).results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c0e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=ingest_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=\"netflix movie data arrived at landing area (today's partition)\")\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a6c11",
   "metadata": {},
   "source": [
    "## Uploading actions.yaml config file to staging branch\n",
    "\n",
    "* Hooks config file `actions.yaml` needs to be uploaded to the branch on which the tests are run. i.e., we want to run data quality tests on staging branch before merging the data into production.\n",
    "\n",
    "* So add `_lakefs_actions/actions.yaml` to staging branch\n",
    "* `actions.yaml` contains a pre-merge hook configured to check for file format validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks_config_yaml = \"actions.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f22bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{hooks_config_yaml}', 'rb') as f:\n",
    "    client.objects.upload_object(repository=repo_name, \n",
    "                                 branch=staging_branch, \n",
    "                                 path=f'{hooks_prefix}/{hooks_config_yaml}', \n",
    "                                 content=f\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b677a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=staging_branch).results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06aed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message='Added hooks config file - actions.yaml to staging area')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d4979",
   "metadata": {},
   "source": [
    "## Extracting data from ingest branch for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41981c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_long_path = f\"s3a://{repo_name}/{ingest_branch}/{ingest_path}\"\n",
    "ingest_long_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.option(\"header\",\"true\").csv(ingest_long_path)\n",
    "print(movies_df.count())\n",
    "print(movies_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65193a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.sample(False,0.1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c8c5f",
   "metadata": {},
   "source": [
    "## Loading transformed data into Staging Area/Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_long_path = f\"s3a://{repo_name}/{staging_branch}\"\n",
    "staging_long_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172a4f0",
   "metadata": {},
   "source": [
    "### Writing csv files to staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.write.option(\"header\",True)\\\n",
    "        .partitionBy(\"type\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .csv(f\"{staging_long_path}/analytics/movies-by-type-csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message='loaded paritioned movies csv to staging area'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e839f",
   "metadata": {},
   "source": [
    "### Pushing csv files to Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=staging_branch, \n",
    "                              destination_branch=prod_branch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babad0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
