{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e13cd9",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.lakefs.io/assets/logo.svg\" alt=\"lakeFS logo\" width=300/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78797e22",
   "metadata": {},
   "source": [
    "## Write-Audit-Publish (WAP) pattern with multiple data formats and cross-collection consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89a51c",
   "metadata": {},
   "source": [
    "**New to Write-Audit-Publish? This [talk](https://www.youtube.com/watch?v=fXHdeBnpXrg&t=1001s) explains it well.**\n",
    "\n",
    "[@rmoff](https://twitter.com/rmoff/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89718426",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6185a4",
   "metadata": {},
   "source": [
    "## Set up Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b2b666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eb609d7ee688:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff6c9b22f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://lakefs:8000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b929",
   "metadata": {},
   "source": [
    "## Set up the connection to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e44584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "lakefs_config = lakefs_client.Configuration()\n",
    "lakefs_config.username = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefs_config.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "lakefs_config.host = 'http://lakefs:8000'\n",
    "\n",
    "lakefs = LakeFSClient(lakefs_config)\n",
    "lakefs_api_client = lakefs_client.ApiClient(lakefs_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047139f",
   "metadata": {},
   "source": [
    "### Get the first repository present in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb017136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lakeFS repository 'example' with storage namespace s3://example\n"
     ]
    }
   ],
   "source": [
    "repo=lakefs.repositories.list_repositories().results[0]\n",
    "print(f\"Using lakeFS repository '{repo.id}' with storage namespace {repo.storage_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517f0ac",
   "metadata": {},
   "source": [
    "### Define the data storage directory based on the provided namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b647611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3a://example for data storage\n"
     ]
    }
   ],
   "source": [
    "data_dir=repo.storage_namespace.replace('s3','s3a')\n",
    "print(f\"Using {data_dir} for data storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7028fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main data path:\ts3a://example/main\n"
     ]
    }
   ],
   "source": [
    "branch=\"main\"\n",
    "main_datapath=(f\"{data_dir}/{branch}\")\n",
    "print(f\"Main data path:\\t{main_datapath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b35915",
   "metadata": {},
   "source": [
    "## Upload sample data files to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda58a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded /data/DOI-10-13012-b2idb-7865141_v1/dataset_info.txt to example/main/src/dataset_info.txt\n",
      "Uploaded /data/DOI-10-13012-b2idb-7865141_v1/morrow-plots_README.txt to example/main/src/morrow-plots_README.txt\n",
      "Uploaded /data/DOI-10-13012-b2idb-7865141_v1/morrow-plots_v01_1888-2019_soil.csv to example/main/src/morrow-plots_v01_1888-2019_soil.csv\n",
      "Uploaded /data/DOI-10-13012-b2idb-7865141_v1/morrow-plots_v01_2020-2021_soil.csv to example/main/src/morrow-plots_v01_2020-2021_soil.csv\n",
      "Uploaded /data/DOI-10-13012-b2idb-7865141_v1/morrow-plots_v01_codebook.pdf to example/main/src/morrow-plots_v01_codebook.pdf\n",
      "Uploaded /data/DOI-10-13012-b2idb-7865141_v1/morrow-plots_v01x_2020-2021_soil.csv to example/main/src/morrow-plots_v01x_2020-2021_soil.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url='http://lakefs:8000/',\n",
    "                  aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n",
    "                  aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')\n",
    "\n",
    "\n",
    "# Set the path to the folder you want to upload\n",
    "folder_path = '/data/DOI-10-13012-b2idb-7865141_v1'\n",
    "\n",
    "# Set the S3 bucket name and key prefix\n",
    "bucket_name = repo.id\n",
    "branch=\"main\"\n",
    "key_prefix = f\"{branch}/src/\"\n",
    "\n",
    "# Iterate over the files in the folder and upload each file to S3\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        s3_key = os.path.join(key_prefix, os.path.relpath(local_path, folder_path))\n",
    "        s3.upload_file(local_path, bucket_name, s3_key)\n",
    "        print(f\"Uploaded {local_path} to {bucket_name}/{s3_key}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b740f39",
   "metadata": {},
   "source": [
    "### Commit the sample data to the `main` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3cce87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1684407937,\n",
       " 'id': '5bea70de7a40f2ae5ab7528c62ef9f8035feb5b68c3e6122f16ee0d7629fe716',\n",
       " 'message': 'Import the source data',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'citation': 'Morrow Plots Data Curation Working Group (2022): '\n",
       "                          'Morrow Plots Treatment and Yield Data. University '\n",
       "                          'of Illinois at Urbana-Champaign. '\n",
       "                          'https://doi.org/10.13012/B2IDB-7865141_V1',\n",
       "              'url': 'https://databank.illinois.edu/datasets/IDB-7865141'},\n",
       " 'parents': ['2393b3f1e52d51968bac08cd25cd8020e05a1534e8ac0312429c4fab0f0bcbb6']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(lakefs_api_client)\n",
    "\n",
    "api_instance.commit(repo.id, \n",
    "                    'main', \n",
    "                    CommitCreation(\n",
    "                        message=\"Import the source data\", \n",
    "                        metadata={'url': 'https://databank.illinois.edu/datasets/IDB-7865141', \n",
    "                                  'citation': 'Morrow Plots Data Curation Working Group (2022): Morrow Plots Treatment and Yield Data. University of Illinois at Urbana-Champaign. https://doi.org/10.13012/B2IDB-7865141_V1'\n",
    "                                 }) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89179da3",
   "metadata": {},
   "source": [
    "### 👉🏻 [View of the files in lakeFS](http://localhost:8000/repositories/example/objects?ref=main&path=src%2F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47f59f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa158f2",
   "metadata": {},
   "source": [
    "# Import the first CSV file and store it as a Delta Lake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f704e1",
   "metadata": {},
   "source": [
    "## Create a branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac5e469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5bea70de7a40f2ae5ab7528c62ef9f8035feb5b68c3e6122f16ee0d7629fe716'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(lakefs_api_client)\n",
    "\n",
    "branch=\"initial_load\"\n",
    "api_instance.create_branch(repo.id, \n",
    "                           BranchCreation(\n",
    "                                name=branch,\n",
    "                                source=\"main\"\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6878129b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch:\t\tinitial_load\n",
      "Base data path:\ts3a://example/initial_load\n"
     ]
    }
   ],
   "source": [
    "base_datapath=(f\"{data_dir}/{branch}\")\n",
    "print(f\"Branch:\\t\\t{branch}\\nBase data path:\\t{base_datapath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceeaa9b",
   "metadata": {},
   "source": [
    "## Load the data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad496308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_csv=(f\"{base_datapath}/src/morrow-plots_v01_1888-2019_soil.csv\")\n",
    "df = spark.read.csv(src_csv,inferSchema=True,header=True)\n",
    "df.createOrReplaceTempView(\"soil_src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e74717d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      1888 |      2019 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM soil_src "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fb202",
   "metadata": {},
   "source": [
    "## Write the data to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2160dcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode('overwrite').save(f\"{base_datapath}/raw/soil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b497a9e",
   "metadata": {},
   "source": [
    "## Table structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8cab05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>col_name</th>\n",
       "        <th>data_type</th>\n",
       "        <th>comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>phase</td>\n",
       "        <td>int</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>year</td>\n",
       "        <td>int</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>plot</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>plot_num</td>\n",
       "        <td>int</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>plot_dir</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>rotation</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>corn</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>crop</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>variety</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>all_corn</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>yield_bush</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>yield_ton</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>treated</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>treatment</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>manure</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>lime</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>nit</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>p205</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>k20</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>stover</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>population</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>plant_date</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>plant_day</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>soil_sample</td>\n",
       "        <td>boolean</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>damage</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>notes</td>\n",
       "        <td>string</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td># Partitioning</td>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Not partitioned</td>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td># Detailed Table Information</td>\n",
       "        <td></td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Name</td>\n",
       "        <td>delta.`s3a://example/initial_load/raw/soil`</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Location</td>\n",
       "        <td>s3a://example/initial_load/raw/soil</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Provider</td>\n",
       "        <td>delta</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Table Properties</td>\n",
       "        <td>[delta.minReaderVersion=1,delta.minWriterVersion=2]</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------------------------+-----------------------------------------------------+---------+\n",
       "|                     col_name |                                           data_type | comment |\n",
       "+------------------------------+-----------------------------------------------------+---------+\n",
       "|                        phase |                                                 int |         |\n",
       "|                         year |                                                 int |         |\n",
       "|                         plot |                                              string |         |\n",
       "|                     plot_num |                                                 int |         |\n",
       "|                     plot_dir |                                              string |         |\n",
       "|                     rotation |                                              string |         |\n",
       "|                         corn |                                              string |         |\n",
       "|                         crop |                                              string |         |\n",
       "|                      variety |                                              string |         |\n",
       "|                     all_corn |                                              string |         |\n",
       "|                   yield_bush |                                              string |         |\n",
       "|                    yield_ton |                                              string |         |\n",
       "|                      treated |                                              string |         |\n",
       "|                    treatment |                                              string |         |\n",
       "|                       manure |                                              string |         |\n",
       "|                         lime |                                              string |         |\n",
       "|                          nit |                                              string |         |\n",
       "|                         p205 |                                              string |         |\n",
       "|                          k20 |                                              string |         |\n",
       "|                       stover |                                              string |         |\n",
       "|                   population |                                              string |         |\n",
       "|                   plant_date |                                              string |         |\n",
       "|                    plant_day |                                              string |         |\n",
       "|                  soil_sample |                                             boolean |         |\n",
       "|                       damage |                                              string |         |\n",
       "|                        notes |                                              string |         |\n",
       "|                              |                                                     |         |\n",
       "|               # Partitioning |                                                     |         |\n",
       "|              Not partitioned |                                                     |         |\n",
       "|                              |                                                     |         |\n",
       "| # Detailed Table Information |                                                     |         |\n",
       "|                         Name |         delta.`s3a://example/initial_load/raw/soil` |         |\n",
       "|                     Location |                 s3a://example/initial_load/raw/soil |         |\n",
       "|                     Provider |                                               delta |         |\n",
       "|             Table Properties | [delta.minReaderVersion=1,delta.minWriterVersion=2] |         |\n",
       "+------------------------------+-----------------------------------------------------+---------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DESCRIBE EXTENDED delta.`{base_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f17847",
   "metadata": {},
   "source": [
    "## Query the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c036e6db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count(1)</th>\n",
       "        <th>min(year)</th>\n",
       "        <th>num_rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3168</td>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+-----------+----------+\n",
       "| count(1) | min(year) | num_rows |\n",
       "+----------+-----------+----------+\n",
       "|     3168 |      1888 |     2019 |\n",
       "+----------+-----------+----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(*),min(year),max(year) as num_rows FROM delta.`{base_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdcb3c",
   "metadata": {},
   "source": [
    "## Commit the Delta Table in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0078cc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1684407964,\n",
       " 'id': 'b53446a273fb1fd91fed59b199ecde494518ad9fbf844fb7f70f2cf20614a740',\n",
       " 'message': 'Convert CSV to Delta Lake table',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'src_file': 'morrow-plots_v01_1888-2019_soil.csv'},\n",
       " 'parents': ['5bea70de7a40f2ae5ab7528c62ef9f8035feb5b68c3e6122f16ee0d7629fe716']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_client = lakefs_client.ApiClient(lakefs_config)\n",
    "\n",
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(lakefs_api_client)\n",
    "\n",
    "api_instance.commit(repo.id, branch, CommitCreation(\n",
    "    message=\"Convert CSV to Delta Lake table\",\n",
    "    metadata={'src_file': 'morrow-plots_v01_1888-2019_soil.csv'}\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f2216",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458abde1",
   "metadata": {},
   "source": [
    "# Build a couple of aggregate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4342a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dims = {\n",
    "    \"variety\",\n",
    "    \"plot\"\n",
    "}\n",
    "\n",
    "for dim in dims:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TABLE agg_{dim} USING DELTA \n",
    "                    LOCATION '{base_datapath}/aggs/agg_{dim}'\n",
    "                    AS SELECT {dim}, \n",
    "                              COUNT(*) AS record_ct, \n",
    "                              PERCENTILE_APPROX(yield_bush,0.5) AS median_yield,\n",
    "                              min(year) AS min_year,\n",
    "                              max(year) AS max_year\n",
    "                        FROM delta.`{base_datapath}/raw/soil`\n",
    "                        GROUP BY {dim}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e1cafd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|  default|   agg_plot|      false|\n",
      "|  default|agg_variety|      false|\n",
      "|         |   soil_src|      false|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c2e9e",
   "metadata": {},
   "source": [
    "## Commit the change "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd66a9",
   "metadata": {},
   "source": [
    "### 👉🏻 [Uncommitted changes in lakeFS](http://localhost:8000/repositories/example/changes?ref=initial_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05680a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1684407991,\n",
       " 'id': '8815c6f65e2f019d718d4ce63e07b7a3cc6e2e01b8e00943c2cff784d65ec28b',\n",
       " 'message': 'Build two aggregate tables',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['b53446a273fb1fd91fed59b199ecde494518ad9fbf844fb7f70f2cf20614a740']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_client = lakefs_client.ApiClient(lakefs_config)\n",
    "\n",
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(lakefs_api_client)\n",
    "# commit_creation = CommitCreation(\n",
    "#     message=f\"Build two aggregate tables\"\n",
    "# ) \n",
    "\n",
    "\n",
    "api_instance.commit(repo.id, branch, CommitCreation(f\"Build two aggregate tables\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39158ef",
   "metadata": {},
   "source": [
    "## We're happy with the change, so merge the branch back into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24d87320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '996096b3ccaedbfa5c05ec2980beb0786049c11c8b74f400ad9fbf338b949cc4',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo.id, source_ref=branch, destination_branch='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4d485",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028cc303",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d416841",
   "metadata": {},
   "source": [
    "# 👀 WAP stuff happens now 👇🏻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70837833",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2d8dc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4ff7",
   "metadata": {},
   "source": [
    "## Load new data for year 2020 and 2021\n",
    "\n",
    "_and use the Write-Audit-Publish pattern as part of this process_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad8990",
   "metadata": {},
   "source": [
    "## Start a new Spark session\n",
    "\n",
    "This would be a new Spark session in real life anyway, and is needed here so that table locations don't get confused when reused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "023ec1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcaf3569",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eb609d7ee688:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff677b5510>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://lakefs:8000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac9084",
   "metadata": {},
   "source": [
    "## Create a branch for our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd778708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branch=\"load_new_data_20-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d71a5876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch:\t\tload_new_data_20-21\n",
      "Base data path:\ts3a://example/load_new_data_20-21\n"
     ]
    }
   ],
   "source": [
    "base_datapath=(f\"{data_dir}/{branch}\")\n",
    "print(f\"Branch:\\t\\t{branch}\\nBase data path:\\t{base_datapath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "729d0292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'996096b3ccaedbfa5c05ec2980beb0786049c11c8b74f400ad9fbf338b949cc4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(lakefs_api_client)\n",
    "\n",
    "api_instance.create_branch(repo.id, BranchCreation(branch,\"main\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5319c05",
   "metadata": {},
   "source": [
    "## Load the new data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f0e36f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_csv=(f\"{data_dir}/{branch}/src/morrow-plots_v01x_2020-2021_soil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dca8566b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(src_csv,inferSchema=True,header=True)\n",
    "df.createOrReplaceTempView(\"soil_src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1f308ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     2020|     2021|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(year),max(year) FROM soil_src\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535c9a1",
   "metadata": {},
   "source": [
    "## Align schemas\n",
    "\n",
    "CSV import just takes a best guess as data types, and the new set of data whilst matching on fields may not on derived data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "393627e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema of the two tables is different.\n",
      "Field rotation has type StringType() in existing table, but type IntegerType() in import data. Fixing.\n",
      "Field corn has type StringType() in existing table, but type BooleanType() in import data. Fixing.\n",
      "Field all_corn has type StringType() in existing table, but type BooleanType() in import data. Fixing.\n",
      "Field yield_bush has type StringType() in existing table, but type IntegerType() in import data. Fixing.\n",
      "Field treated has type StringType() in existing table, but type BooleanType() in import data. Fixing.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "existing_table = spark.read.format(\"delta\").load(f\"{base_datapath}/raw/soil\")\n",
    "\n",
    "# Get the schema of each table\n",
    "existing_schema = existing_table.schema\n",
    "new_schema = df.schema\n",
    "\n",
    "# Compare the schema of the tables\n",
    "if existing_schema == new_schema:\n",
    "    print(\"The schema of the two tables is the same.\")\n",
    "else:\n",
    "    print(\"The schema of the two tables is different.\")\n",
    "    # Find any mismatched field types\n",
    "    for field1, field2 in zip(existing_schema, new_schema):\n",
    "        if field1 != field2:\n",
    "            print(f\"Field {field1.name} has type {field1.dataType} in existing table, but type {field2.dataType} in import data. Fixing.\")\n",
    "            # Update the newly-imported data to have the same schema as the existing data\n",
    "            df = df.withColumnRenamed(field2.name, field1.name).withColumn(field1.name, col(field2.name).cast(field1.dataType))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71569f9a",
   "metadata": {},
   "source": [
    "## Append the new data to existing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f40daed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode('append').save(f\"{base_datapath}/raw/soil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c3a04",
   "metadata": {},
   "source": [
    "## Inspecting the staged/unpublished data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09de33",
   "metadata": {},
   "source": [
    "### Staged/unpublished data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e5ba8",
   "metadata": {},
   "source": [
    "#### The changes are reflected in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1329060f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      1888 |      2021 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM delta.`{base_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e26a26",
   "metadata": {},
   "source": [
    "### Published data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a4eb7",
   "metadata": {},
   "source": [
    "The data on the `main` branch remains unchanged. We can validate this by running a query against the data, specifying `main` as the branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "114abc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      1888 |      2019 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM delta.`{main_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bedcfe",
   "metadata": {},
   "source": [
    "## Rebuild aggregates with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7e57d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dims = {\n",
    "    \"variety\",\n",
    "    \"plot\"\n",
    "}\n",
    "\n",
    "for dim in dims:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TABLE agg_{dim} USING DELTA \n",
    "                    LOCATION '{base_datapath}/aggs/agg_{dim}'\n",
    "                    AS SELECT {dim}, \n",
    "                              COUNT(*) AS record_ct, \n",
    "                              PERCENTILE_APPROX(yield_bush,0.5) AS median_yield,\n",
    "                              min(year) AS min_year,\n",
    "                              max(year) AS max_year\n",
    "                        FROM delta.`{base_datapath}/raw/soil`\n",
    "                        GROUP BY {dim}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab6a93",
   "metadata": {},
   "source": [
    "## Reminder: the changes are not published yet! The `main` copy of the data remains as it was before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cdd8f",
   "metadata": {},
   "source": [
    "### Staged/unpublished data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "801cd184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table</th>\n",
       "        <th>row_ct</th>\n",
       "        <th>min(min_year)</th>\n",
       "        <th>max(max_year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_variety</td>\n",
       "        <td>46</td>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_plot</td>\n",
       "        <td>25</td>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------+--------+---------------+---------------+\n",
       "|       table | row_ct | min(min_year) | max(max_year) |\n",
       "+-------------+--------+---------------+---------------+\n",
       "| agg_variety |     46 |          1888 |          2021 |\n",
       "|    agg_plot |     25 |          1888 |          2021 |\n",
       "+-------------+--------+---------------+---------------+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT \"agg_variety\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{base_datapath}/aggs/agg_variety` UNION ALL SELECT \"agg_plot\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{base_datapath}/aggs/agg_plot` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6aa32a",
   "metadata": {},
   "source": [
    "### Published data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b30887",
   "metadata": {},
   "source": [
    "The data on the `main` branch remains unchanged. We can validate this by running a query against the data, specifying `main` as the branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dedfc3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table</th>\n",
       "        <th>row_ct</th>\n",
       "        <th>min(min_year)</th>\n",
       "        <th>max(max_year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_variety</td>\n",
       "        <td>46</td>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_plot</td>\n",
       "        <td>24</td>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------+--------+---------------+---------------+\n",
       "|       table | row_ct | min(min_year) | max(max_year) |\n",
       "+-------------+--------+---------------+---------------+\n",
       "| agg_variety |     46 |          1888 |          2019 |\n",
       "|    agg_plot |     24 |          1888 |          2019 |\n",
       "+-------------+--------+---------------+---------------+"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT \"agg_variety\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{main_datapath}/aggs/agg_variety` UNION ALL SELECT \"agg_plot\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{main_datapath}/aggs/agg_plot` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58244fc",
   "metadata": {},
   "source": [
    "# Audit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d8ce7",
   "metadata": {},
   "source": [
    "At the moment the data is written to the audit branch (`data_load`), but not published to `main`. \n",
    "\n",
    "How you audit the data is up to you. The nice thing about the data being staged is that you can do it within the same ETL job, or have another tool do it.\n",
    "\n",
    "Here's a very simple example of doing in Python. We're going to programatically check that: \n",
    "\n",
    "1. The year on each table we've loaded matches the most recent year in the source CSV file\n",
    "2. From a data quality point of view, there should be no NULLs in the dimension field for each aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad0510",
   "metadata": {},
   "source": [
    "## Has the latest data been added to each table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62d6b827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest year in source file is 2021\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "src_csv=(f\"{data_dir}/{branch}/src/morrow-plots_v01x_2020-2021_soil.csv\")\n",
    "latest_year_src = spark.read.csv(src_csv,inferSchema=True,header=True).orderBy(desc(\"year\")).first()[\"year\"]\n",
    "print(f\"Latest year in source file is {latest_year_src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05b9a5",
   "metadata": {},
   "source": [
    "Then we get the latest year in each table in turn and compare it to the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82582ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙌 Audit has passed: the latest year on raw/soil (2021) matches the source (2021)\n",
      "🙌 Audit has passed: the latest year on aggs/agg_variety (2021) matches the source (2021)\n",
      "🙌 Audit has passed: the latest year on aggs/agg_plot (2021) matches the source (2021)\n"
     ]
    }
   ],
   "source": [
    "tables = {\n",
    "    \"raw/soil\": \"year\",\n",
    "    \"aggs/agg_variety\": \"max_year\",\n",
    "    \"aggs/agg_plot\": \"max_year\"\n",
    "}\n",
    "\n",
    "for table, year_col in tables.items():\n",
    "    df = spark.read.format(\"delta\").load(f\"{base_datapath}/{table}\")\n",
    "    latest_year = df.selectExpr(f\"max({year_col})\").collect()[0][0]\n",
    "\n",
    "    if ( latest_year_src!=latest_year ):\n",
    "        raise ValueError(f\"Audit failed: latest year on {table} ({latest_year}) does not match source ({latest_year_src})\")\n",
    "    else:\n",
    "        print(f\"🙌 Audit has passed: the latest year on {table} ({latest_year}) matches the source ({latest_year_src})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aedd60",
   "metadata": {},
   "source": [
    "## Are there any NULLs in the dimension of each aggregate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b28eb9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Audit failed: Aggregate agg_plot has 1 non-null rows",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m ct\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT COUNT(*) AS ct FROM agg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m WHERE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m IS NULL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ( ct\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m ):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudit failed: Aggregate agg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m non-null rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🙌 Audit has passed: Aggregate agg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has 0 non-null rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Audit failed: Aggregate agg_plot has 1 non-null rows"
     ]
    }
   ],
   "source": [
    "dims = {\n",
    "    \"variety\",\n",
    "    \"plot\"\n",
    "}\n",
    "\n",
    "for dim in dims:\n",
    "    ct=spark.sql(f\"SELECT COUNT(*) AS ct FROM agg_{dim} WHERE {dim} IS NULL\").first()['ct']\n",
    "    \n",
    "    if ( ct!=0 ):\n",
    "        raise ValueError(f\"Audit failed: Aggregate agg_{dim} has {ct} non-null rows\")\n",
    "    else:\n",
    "        print(f\"🙌 Audit has passed: Aggregate agg_{dim} has 0 non-null rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f9c2b",
   "metadata": {},
   "source": [
    "## The audit failed! Oh noes!\n",
    "\n",
    "`ValueError: Audit failed: Aggregate agg_plot has 1 non-null rows`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93137277",
   "metadata": {},
   "source": [
    "Why would `plot` be null? Let's look at the underlying data. First, is it just the new data (for 2020 and 2021)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39112710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>phase</th>\n",
       "        <th>year</th>\n",
       "        <th>plot</th>\n",
       "        <th>plot_num</th>\n",
       "        <th>plot_dir</th>\n",
       "        <th>rotation</th>\n",
       "        <th>corn</th>\n",
       "        <th>crop</th>\n",
       "        <th>variety</th>\n",
       "        <th>all_corn</th>\n",
       "        <th>yield_bush</th>\n",
       "        <th>yield_ton</th>\n",
       "        <th>treated</th>\n",
       "        <th>treatment</th>\n",
       "        <th>manure</th>\n",
       "        <th>lime</th>\n",
       "        <th>nit</th>\n",
       "        <th>p205</th>\n",
       "        <th>k20</th>\n",
       "        <th>stover</th>\n",
       "        <th>population</th>\n",
       "        <th>plant_date</th>\n",
       "        <th>plant_day</th>\n",
       "        <th>soil_sample</th>\n",
       "        <th>damage</th>\n",
       "        <th>notes</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------+------+------+----------+----------+----------+------+------+---------+----------+------------+-----------+---------+-----------+--------+------+-----+------+-----+--------+------------+------------+-----------+-------------+--------+-------+\n",
       "| phase | year | plot | plot_num | plot_dir | rotation | corn | crop | variety | all_corn | yield_bush | yield_ton | treated | treatment | manure | lime | nit | p205 | k20 | stover | population | plant_date | plant_day | soil_sample | damage | notes |\n",
       "+-------+------+------+----------+----------+----------+------+------+---------+----------+------------+-----------+---------+-----------+--------+------+-----+------+-----+--------+------------+------------+-----------+-------------+--------+-------+\n",
       "+-------+------+------+----------+----------+----------+------+------+---------+----------+------------+-----------+---------+-----------+--------+------+-----+------+-----+--------+------------+------------+-----------+-------------+--------+-------+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM delta.`{base_datapath}/raw/soil` WHERE plot IS NULL AND year NOT IN (2020,2021);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80cd30",
   "metadata": {},
   "source": [
    "It's only in the new data. Let's look at the source CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec66f992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|phase,year,plot,p...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|5,2020,,3,SE,1,tr...|\n",
      "|5,2020,,4,SE,2,fa...|\n",
      "|5,2020,,5,NW,3,fa...|\n",
      "|5,2021,,3,NW,1,tr...|\n",
      "|5,2021,,4,NE,2,tr...|\n",
      "|5,2021,,4,SW,2,tr...|\n",
      "|5,2021,,5,SE,3,tr...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src=spark.read.text(src_csv)\n",
    "\n",
    "src.show(1)\n",
    "src.sample(fraction=0.2, seed=42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69d3e6",
   "metadata": {},
   "source": [
    "We can see the third field is `plot` (based on the header), and based on a random sample of the file seems to be always empty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ded8f3",
   "metadata": {},
   "source": [
    "🤦🏻 turns out we're using the wrong source file! Let's fix that and check it looks better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d716f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_csv=(f\"{data_dir}/{branch}/src/morrow-plots_v01_2020-2021_soil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcac493d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|phase,year,plot,p...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|5,2020,3SC,3,SE,1...|\n",
      "|5,2020,4SD,4,SE,2...|\n",
      "|5,2020,5NB,5,NW,3...|\n",
      "|5,2021,3NB,3,NW,1...|\n",
      "|5,2021,4ND,4,NE,2...|\n",
      "|5,2021,4SB,4,SW,2...|\n",
      "|5,2021,5SD,5,SE,3...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src=spark.read.text(src_csv)\n",
    "\n",
    "src.show(1)\n",
    "src.sample(fraction=0.2, seed=42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58982134",
   "metadata": {},
   "source": [
    "**That** does look better - we can see the plot value clearly in the source. \n",
    "\n",
    "Before we can re-run the data load we need to reset the branch to undo the existing unpublished work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd07a6",
   "metadata": {},
   "source": [
    "### Show list of uncommitted changes in lakeFS\n",
    "\n",
    "👉🏻 [Web UI showing uncommitted changes in lakeFS](http://localhost:8000/repositories/example/changes?ref=data_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef096cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'aggs/agg_plot/_delta_log/00000000000000000001.json',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 8885,\n",
       "  'type': 'added'},\n",
       " {'path': 'aggs/agg_plot/part-00000-5367224f-97f0-4661-897b-4be013fd503a-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 8885,\n",
       "  'type': 'added'},\n",
       " {'path': 'aggs/agg_variety/_delta_log/00000000000000000001.json',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 8885,\n",
       "  'type': 'added'},\n",
       " {'path': 'aggs/agg_variety/part-00000-88223df5-3048-42e3-ab4e-b64672cae9b8-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 8885,\n",
       "  'type': 'added'},\n",
       " {'path': 'raw/soil/_delta_log/00000000000000000001.json',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 8885,\n",
       "  'type': 'added'},\n",
       " {'path': 'raw/soil/part-00000-a9a3b430-621d-4486-84a6-d264eaf165e7-c000.snappy.parquet',\n",
       "  'path_type': 'object',\n",
       "  'size_bytes': 8885,\n",
       "  'type': 'added'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api_instance = branches_api.BranchesApi(api_client)\n",
    "\n",
    "api_response = api_instance.diff_branch(repo.id, branch)\n",
    "if api_response.pagination.results==0:\n",
    "    display(\"Nothing to commit\")\n",
    "else:\n",
    "    display(api_response.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842b8ec",
   "metadata": {},
   "source": [
    "### Reset the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cedf023a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseting branch load_new_data_20-21 on repo example\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.model.reset_creation import ResetCreation\n",
    "\n",
    "print(f\"Reseting branch {branch} on repo {repo.id}\")\n",
    "lakefs.branches.reset_branch(repo.id, branch,ResetCreation(\"reset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c439a9",
   "metadata": {},
   "source": [
    "### Now re-run the data load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ef470",
   "metadata": {},
   "source": [
    "# ⚠️⚠️⚠️⚠️⚠️⚠️ RESTART THE JUPYTER KERNEL BEFORE CONTINUING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f08ef",
   "metadata": {},
   "source": [
    "This kills the kernel and forces it to restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c102055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8271a1f",
   "metadata": {},
   "source": [
    "# ⚠️⚠️⚠️⚠️⚠️⚠️ RESTART THE JUPYTER KERNEL BEFORE CONTINUING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05291273",
   "metadata": {},
   "source": [
    "### Set stuff back up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06200237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "lakefs_config = lakefs_client.Configuration()\n",
    "lakefs_config.username = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefs_config.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "lakefs_config.host = 'http://lakefs:8000'\n",
    "\n",
    "lakefs = LakeFSClient(lakefs_config)\n",
    "lakefs_api_client = lakefs_client.ApiClient(lakefs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e6931d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lakeFS repository 'example' with storage namespace s3://example\n"
     ]
    }
   ],
   "source": [
    "repo=lakefs.repositories.list_repositories().results[0]\n",
    "print(f\"Using lakeFS repository '{repo.id}' with storage namespace {repo.storage_namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8c889c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3a://example for data storage\n"
     ]
    }
   ],
   "source": [
    "data_dir=repo.storage_namespace.replace('s3','s3a')\n",
    "print(f\"Using {data_dir} for data storage\")\n",
    "main_datapath=(f\"{data_dir}/main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d642d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branch=\"load_new_data_20-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef06c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch:\t\tload_new_data_20-21\n",
      "Base data path:\ts3a://example/load_new_data_20-21\n"
     ]
    }
   ],
   "source": [
    "base_datapath=(f\"{data_dir}/{branch}\")\n",
    "print(f\"Branch:\\t\\t{branch}\\nBase data path:\\t{base_datapath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79a0e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eb609d7ee688:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter load_new_data_20-21</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff62e7f940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(f\"lakeFS / Jupyter {branch}\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://lakefs:8000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816990eb",
   "metadata": {},
   "source": [
    "## Load the new data from the corrected source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e10a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_csv=(f\"{data_dir}/{branch}/src/morrow-plots_v01_2020-2021_soil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08eb024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file: s3a://example/load_new_data_20-21/src/morrow-plots_v01_2020-2021_soil.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source file: {src_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe81acc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(src_csv,inferSchema=True,header=True)\n",
    "df.createOrReplaceTempView(\"soil_src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fec398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2020</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      2020 |      2021 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM soil_src "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6081e",
   "metadata": {},
   "source": [
    "## Align schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedf7780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema of the two tables is different.\n",
      "Field rotation has type StringType() in existing table, but type IntegerType() in import data. Fixing.\n",
      "Field corn has type StringType() in existing table, but type BooleanType() in import data. Fixing.\n",
      "Field all_corn has type StringType() in existing table, but type BooleanType() in import data. Fixing.\n",
      "Field yield_bush has type StringType() in existing table, but type IntegerType() in import data. Fixing.\n",
      "Field treated has type StringType() in existing table, but type BooleanType() in import data. Fixing.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "existing_table = spark.read.format(\"delta\").load(f\"{base_datapath}/raw/soil\")\n",
    "\n",
    "# Get the schema of each table\n",
    "existing_schema = existing_table.schema\n",
    "new_schema = df.schema\n",
    "\n",
    "# Compare the schema of the tables\n",
    "if existing_schema == new_schema:\n",
    "    print(\"The schema of the two tables is the same.\")\n",
    "else:\n",
    "    print(\"The schema of the two tables is different.\")\n",
    "    # Find any mismatched field types\n",
    "    for field1, field2 in zip(existing_schema, new_schema):\n",
    "        if field1 != field2:\n",
    "            print(f\"Field {field1.name} has type {field1.dataType} in existing table, but type {field2.dataType} in import data. Fixing.\")\n",
    "            # Update the newly-imported data to have the same schema as the existing data\n",
    "            df = df.withColumnRenamed(field2.name, field1.name).withColumn(field1.name, col(field2.name).cast(field1.dataType))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3698f",
   "metadata": {},
   "source": [
    "## Append the new data to existing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bcac08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode('append').save(f\"{base_datapath}/raw/soil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f9ad3",
   "metadata": {},
   "source": [
    "## Inspecting the staged/unpublished data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60617a1e",
   "metadata": {},
   "source": [
    "### Staged/unpublished data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ea696",
   "metadata": {},
   "source": [
    "#### The changes are reflected in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e460462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      1888 |      2021 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM delta.`{base_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7527b6",
   "metadata": {},
   "source": [
    "### Published data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a524e5",
   "metadata": {},
   "source": [
    "The data on the `main` branch remains unchanged. We can validate this by running a query against the data, specifying `main` as the branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69490018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      1888 |      2019 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM delta.`{main_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe549a",
   "metadata": {},
   "source": [
    "## Rebuild aggregates with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b254d390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dims = {\n",
    "    \"variety\",\n",
    "    \"plot\"\n",
    "}\n",
    "\n",
    "for dim in dims:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TABLE agg_{dim} USING DELTA \n",
    "                    LOCATION '{base_datapath}/aggs/agg_{dim}'\n",
    "                    AS SELECT {dim}, \n",
    "                              COUNT(*) AS record_ct, \n",
    "                              PERCENTILE_APPROX(yield_bush,0.5) AS median_yield,\n",
    "                              min(year) AS min_year,\n",
    "                              max(year) AS max_year\n",
    "                        FROM delta.`{base_datapath}/raw/soil`\n",
    "                        GROUP BY {dim}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221ee70",
   "metadata": {},
   "source": [
    "## Reminder: the changes are not published yet! The `main` copy of the data remains as it was before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f899b",
   "metadata": {},
   "source": [
    "### Staged/unpublished data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807395d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table</th>\n",
       "        <th>row_ct</th>\n",
       "        <th>min(min_year)</th>\n",
       "        <th>max(max_year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_variety</td>\n",
       "        <td>46</td>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_plot</td>\n",
       "        <td>24</td>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------+--------+---------------+---------------+\n",
       "|       table | row_ct | min(min_year) | max(max_year) |\n",
       "+-------------+--------+---------------+---------------+\n",
       "| agg_variety |     46 |          1888 |          2021 |\n",
       "|    agg_plot |     24 |          1888 |          2021 |\n",
       "+-------------+--------+---------------+---------------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT \"agg_variety\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{base_datapath}/aggs/agg_variety` UNION ALL SELECT \"agg_plot\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{base_datapath}/aggs/agg_plot` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334fde1",
   "metadata": {},
   "source": [
    "### Published data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f6bdd",
   "metadata": {},
   "source": [
    "The data on the `main` branch remains unchanged. We can validate this by running a query against the data, specifying `main` as the branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7acac019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table</th>\n",
       "        <th>row_ct</th>\n",
       "        <th>min(min_year)</th>\n",
       "        <th>max(max_year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_variety</td>\n",
       "        <td>46</td>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_plot</td>\n",
       "        <td>24</td>\n",
       "        <td>1888</td>\n",
       "        <td>2019</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------+--------+---------------+---------------+\n",
       "|       table | row_ct | min(min_year) | max(max_year) |\n",
       "+-------------+--------+---------------+---------------+\n",
       "| agg_variety |     46 |          1888 |          2019 |\n",
       "|    agg_plot |     24 |          1888 |          2019 |\n",
       "+-------------+--------+---------------+---------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT \"agg_variety\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{main_datapath}/aggs/agg_variety` UNION ALL SELECT \"agg_plot\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{main_datapath}/aggs/agg_plot` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0179548",
   "metadata": {},
   "source": [
    "# Audit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c8884",
   "metadata": {},
   "source": [
    "At the moment the data is written to the audit branch (`data_load`), but not published to `main`. \n",
    "\n",
    "How you audit the data is up to you. The nice thing about the data being staged is that you can do it within the same ETL job, or have another tool do it.\n",
    "\n",
    "Here's a very simple example of doing in Python. We're going to programatically check that: \n",
    "\n",
    "1. The year on each table we've loaded matches the most recent year in the source CSV file\n",
    "2. From a data quality point of view, there should be no NULLs in the dimension field for each aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013674eb",
   "metadata": {},
   "source": [
    "## Has the latest data been added to each table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "384a2a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest year in source file is 2021\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "src_csv=(f\"{data_dir}/{branch}/src/morrow-plots_v01x_2020-2021_soil.csv\")\n",
    "latest_year_src = spark.read.csv(src_csv,inferSchema=True,header=True).orderBy(desc(\"year\")).first()[\"year\"]\n",
    "print(f\"Latest year in source file is {latest_year_src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fffae",
   "metadata": {},
   "source": [
    "Then we get the latest year in each table in turn and compare it to the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d8728f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙌 Audit has passed: the latest year on raw/soil (2021) matches the source (2021)\n",
      "🙌 Audit has passed: the latest year on aggs/agg_variety (2021) matches the source (2021)\n",
      "🙌 Audit has passed: the latest year on aggs/agg_plot (2021) matches the source (2021)\n"
     ]
    }
   ],
   "source": [
    "tables = {\n",
    "    \"raw/soil\": \"year\",\n",
    "    \"aggs/agg_variety\": \"max_year\",\n",
    "    \"aggs/agg_plot\": \"max_year\"\n",
    "}\n",
    "\n",
    "for table, year_col in tables.items():\n",
    "    df = spark.read.format(\"delta\").load(f\"{base_datapath}/{table}\")\n",
    "    latest_year = df.selectExpr(f\"max({year_col})\").collect()[0][0]\n",
    "\n",
    "    if ( latest_year_src!=latest_year ):\n",
    "        raise ValueError(f\"Audit failed: latest year on {table} ({latest_year}) does not match source ({latest_year_src})\")\n",
    "    else:\n",
    "        print(f\"🙌 Audit has passed: the latest year on {table} ({latest_year}) matches the source ({latest_year_src})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d928f",
   "metadata": {},
   "source": [
    "## Are there any NULLs in the dimension of each aggregate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d9c7a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙌 Audit has passed: Aggregate agg_variety has 0 non-null rows\n",
      "🙌 Audit has passed: Aggregate agg_plot has 0 non-null rows\n"
     ]
    }
   ],
   "source": [
    "dims = {\n",
    "    \"variety\",\n",
    "    \"plot\"\n",
    "}\n",
    "\n",
    "for dim in dims:\n",
    "    ct=spark.sql(f\"SELECT COUNT(*) AS ct FROM agg_{dim} WHERE {dim} IS NULL\").first()['ct']\n",
    "    \n",
    "    if ( ct!=0 ):\n",
    "        raise ValueError(f\"Audit failed: Aggregate agg_{dim} has {ct} non-null rows\")\n",
    "    else:\n",
    "        print(f\"🙌 Audit has passed: Aggregate agg_{dim} has 0 non-null rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f2ac8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154893d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09efa5",
   "metadata": {},
   "source": [
    "# 🎈🎈🎈🎉🎉🎉🎉 The Audit passed! We are now ready to Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62cb13",
   "metadata": {},
   "source": [
    "# 🎈🎈🎈🎉🎉🎉🎉 The Audit passed! We are now ready to Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb1a0f",
   "metadata": {},
   "source": [
    "# 🎈🎈🎈🎉🎉🎉🎉 The Audit passed! We are now ready to Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76da6ee",
   "metadata": {},
   "source": [
    "# 🎈🎈🎈🎉🎉🎉🎉 The Audit passed! We are now ready to Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dceda1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3bcff",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19526bb",
   "metadata": {},
   "source": [
    "# Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc07949",
   "metadata": {},
   "source": [
    "Publishing data in lakeFS means merging the audit branch back into `main`, making it available to anyone working with the data in that branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a637a54",
   "metadata": {},
   "source": [
    "## Commit the data to the audit [working] branch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b466db",
   "metadata": {},
   "source": [
    "We can add a commit message, as well as optional metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30ff1ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1684408072,\n",
       " 'id': 'c1170427f2e59a291e7a344a10efc057a4b83e3d37aab5427170f3e9405f2c1f',\n",
       " 'message': 'Add data for years 2020 and 2021',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'src_file': 'morrow-plots_v01_2020-2021_soil.csv'},\n",
       " 'parents': ['996096b3ccaedbfa5c05ec2980beb0786049c11c8b74f400ad9fbf338b949cc4']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(lakefs_api_client)\n",
    "\n",
    "api_instance.commit(repo.id, branch, CommitCreation(\n",
    "    message=\"Add data for years 2020 and 2021\",\n",
    "    metadata={'src_file': 'morrow-plots_v01_2020-2021_soil.csv'}\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdce995",
   "metadata": {},
   "source": [
    "## Merge the branch back into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd0375e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '00b07af2b550fdee6a19437999806b67bd2aae894220881293b790207205a6c1',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo.id, source_ref=branch, destination_branch='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38524dd",
   "metadata": {},
   "source": [
    "## Inspect the published data\n",
    "\n",
    "The data on `main` now includes the additional data for 2020 and 2021, and can be read by anyone using this trunk branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35cdfe70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table</th>\n",
       "        <th>row_ct</th>\n",
       "        <th>min(min_year)</th>\n",
       "        <th>max(max_year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_variety</td>\n",
       "        <td>46</td>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>agg_plot</td>\n",
       "        <td>24</td>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------+--------+---------------+---------------+\n",
       "|       table | row_ct | min(min_year) | max(max_year) |\n",
       "+-------------+--------+---------------+---------------+\n",
       "| agg_variety |     46 |          1888 |          2021 |\n",
       "|    agg_plot |     24 |          1888 |          2021 |\n",
       "+-------------+--------+---------------+---------------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT \"agg_variety\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{main_datapath}/aggs/agg_variety` UNION ALL SELECT \"agg_plot\" AS table,COUNT(*) AS row_ct,min(min_year),max(max_year) FROM delta.`{main_datapath}/aggs/agg_plot` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ddbe6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>min(year)</th>\n",
       "        <th>max(year)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1888</td>\n",
       "        <td>2021</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-----------+\n",
       "| min(year) | max(year) |\n",
       "+-----------+-----------+\n",
       "|      1888 |      2021 |\n",
       "+-----------+-----------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT min(year),max(year) FROM delta.`{main_datapath}/raw/soil`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482b2db",
   "metadata": {},
   "source": [
    "# Where Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d43a5",
   "metadata": {},
   "source": [
    "* For more information about write-audit-publish see [this talk from Michelle Winters](https://www.youtube.com/watch?v=fXHdeBnpXrg&t=1001s) and [this talk from Sam Redai](https://www.dremio.com/wp-content/uploads/2022/05/Sam-Redai-The-Write-Audit-Publish-Pattern-via-Apache-Iceberg.pdf).\n",
    "* To try out lakeFS check out the [hands-on Quickstart](https://docs.lakefs.io/quickstart/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
