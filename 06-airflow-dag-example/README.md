## lakeFS + Github + Airflow - example

This sample will give you an idea of how you can use the [lakeFS Airflow provider](https://github.com/treeverse/airflow-provider-lakeFS) to:
* Version control your raw, intermediate and processed data.
* Link between code versions and the data generated by running them.

![](assets/git-lakefs-airflow.png)

### Run Instructions

1. Clone the sample.

1. Spin up the environment:
   `docker-compose up`

1. Browse to Airflow in [http://localhost:8080/](http://localhost:8080/). 

      * User: `airflow`
      * Password: `airflow`

1. Run the _etl_ DAG in Airflow

      ![](assets/airflow-ui-01.png)

1. Observe the results in lakeFS. Login to the lakeFS UI in [http://localhost:8080/](http://localhost:8080/)

      * User: `AKIAIOSFODNN7EXAMPLE`
      * Password: `wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY`

1. In the `airflow-example` repository's [`main` branch](http://localhost:8000/repositories/airflow-example/objects) you'll see the raw data alongside the transformed results

      ![](assets/lakefs-ui-01.png)

1. Drill down on any path to view the CSV file and get an understanding of the transform process

      ![](assets/lakefs-ui-02.png)

1. Click on the **Commits** tab to see the commit history for the branch. Change the branch from the dropdown menu to see the history for each branch. 

1. From the **Branches** tab note that each transform (by event type / month / user) was isolated on its own branch and only merged back into `main` once it was completed sucessfully. 

      ![](assets/lakefs-ui-03.png)

Here's the [DAG](./dags/dag.py) that's used: 

![](assets/airflow-ui-02.png)
