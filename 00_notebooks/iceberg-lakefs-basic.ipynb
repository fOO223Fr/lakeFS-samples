{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1041ae6f",
   "metadata": {},
   "source": [
    "<img src=\"https://www.apache.org/logos/res/iceberg/iceberg.png\" alt=\"Apache Iceberg logo\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6633c1",
   "metadata": {},
   "source": [
    "## lakeFS ‚ù§Ô∏è Apache Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d5e48-44f1-4da4-b8c1-b354b1b61e17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01815b-cc6e-420a-bca9-970b23fb895b",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad336dd-cb3d-478a-bc90-cd1df7ca7e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0cc4bb-ebd4-4731-9ada-d924ae7a08da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f452ecfd-a7be-4a3a-9e5c-5a59b1d6c9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcfc3fe-0617-447c-8c1e-6d0db05717ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2595447-3d6c-425f-b2cd-b1e0e38827e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada2113d-b298-4606-88ef-42e0a3b528ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"lakefs-iceberg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b7467-4440-4e41-a8d6-bdd658a1820e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03d274e-88e3-4567-9dd3-bc9af3397a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851be970-1ba6-4514-b9fc-d489342db8d0",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d332303-8ceb-40a7-9c4c-1956480bdcad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying lakeFS credentials‚Ä¶\n",
      "‚Ä¶‚úÖlakeFS credentials verified\n",
      "\n",
      "‚ÑπÔ∏èlakeFS version 0.103.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version {v.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36e1b9-607f-467e-8ef2-98cbb2c60e18",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2db150-3df3-43b9-97e0-e6fe69aea3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository lakefs-iceberg does not exist, so going to try and create it now.\n",
      "Created new repo lakefs-iceberg using storage namespace s3://example/lakefs-iceberg\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c5b07-f5b7-4d72-879c-e18bf6e4f0b1",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a98f60-3fcf-44b7-b31d-d9d56f4b25a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dc58cc99c0e5:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Iceberg / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff6527ac80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Iceberg / Jupyter\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0,io.lakefs:lakefs-iceberg:0.0.1\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs.warehouse\", f\"lakefs://{repo_name}\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs.uri\", lakefsEndPoint) \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb2cfd-9393-46ee-b1a9-f409e0a23b74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58bd823-5b30-43cb-bf7c-cc2aef57db39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464902f-39ce-4d01-813e-fda188188b43",
   "metadata": {},
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1fc9b-ca63-4e24-a2a4-83c2b9495408",
   "metadata": {},
   "source": [
    "## Create an Iceberg table in the lakeFS catalog `main` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4016b8b0-1bf9-46a1-be2d-31faeeb8e793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE lakefs.main.rmoff.table4 (id int, data string);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d2423-8d52-498d-b62d-c8cde01932e0",
   "metadata": {},
   "source": [
    "## Write two rows of data to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3a4f5d-d27c-41f2-a472-99364055a8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = spark.range(0, 2) \\\n",
    "     .withColumn(\"data\", when(col(\"id\") % 2 == 0, \"foo\") \\\n",
    "                 .otherwise(\"bar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d1b0d5-023d-4f18-aecc-d9d796831171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o67.append.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:767)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:244)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:244)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:195)\n\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:149)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2) (dc58cc99c0e5 executor driver): java.io.UncheckedIOException: Failed to close current writer\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:124)\n\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:716)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:698)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:453)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: can not write FileMetaData(version:1, schema:[SchemaElement(name:table, num_children:2), SchemaElement(type:INT32, repetition_type:REQUIRED, name:id, field_id:1), SchemaElement(type:BYTE_ARRAY, repetition_type:REQUIRED, name:data, converted_type:UTF8, field_id:2, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:INT32, encodings:[BIT_PACKED, PLAIN], path_in_schema:[id], codec:GZIP, num_values:1, total_uncompressed_size:27, total_compressed_size:47, data_page_offset:4, statistics:Statistics(max:00 00 00 00, min:00 00 00 00, null_count:0, max_value:00 00 00 00, min_value:00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:145, offset_index_length:10, column_index_offset:101, column_index_length:23), ColumnChunk(file_offset:51, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[BIT_PACKED, PLAIN], path_in_schema:[data], codec:GZIP, num_values:1, total_uncompressed_size:30, total_compressed_size:50, data_page_offset:51, statistics:Statistics(max:66 6F 6F, min:66 6F 6F, null_count:0, max_value:66 6F 6F, min_value:66 6F 6F), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:155, offset_index_length:10, column_index_offset:124, column_index_length:21)], total_byte_size:57, num_rows:1, file_offset:4, total_compressed_size:97, ordinal:0)], key_value_metadata:[KeyValue(key:iceberg.schema, value:{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"id\",\"required\":true,\"type\":\"int\"},{\"id\":2,\"name\":\"data\",\"required\":true,\"type\":\"string\"}]})], created_by:parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:376)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:143)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:138)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:1338)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:1203)\n\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:255)\n\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n\t... 16 more\nCaused by: org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TTransportException: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:199)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:482)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:489)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:263)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:245)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:71)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1390)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1240)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData.write(FileMetaData.java:1118)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:373)\n\t... 23 more\nCaused by: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.checkOpen(S3ABlockOutputStream.java:243)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.write(S3ABlockOutputStream.java:294)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:197)\n\t... 32 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n\t... 43 more\nCaused by: java.io.UncheckedIOException: Failed to close current writer\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:124)\n\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:716)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:698)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:453)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.io.IOException: can not write FileMetaData(version:1, schema:[SchemaElement(name:table, num_children:2), SchemaElement(type:INT32, repetition_type:REQUIRED, name:id, field_id:1), SchemaElement(type:BYTE_ARRAY, repetition_type:REQUIRED, name:data, converted_type:UTF8, field_id:2, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:INT32, encodings:[BIT_PACKED, PLAIN], path_in_schema:[id], codec:GZIP, num_values:1, total_uncompressed_size:27, total_compressed_size:47, data_page_offset:4, statistics:Statistics(max:00 00 00 00, min:00 00 00 00, null_count:0, max_value:00 00 00 00, min_value:00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:145, offset_index_length:10, column_index_offset:101, column_index_length:23), ColumnChunk(file_offset:51, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[BIT_PACKED, PLAIN], path_in_schema:[data], codec:GZIP, num_values:1, total_uncompressed_size:30, total_compressed_size:50, data_page_offset:51, statistics:Statistics(max:66 6F 6F, min:66 6F 6F, null_count:0, max_value:66 6F 6F, min_value:66 6F 6F), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:155, offset_index_length:10, column_index_offset:124, column_index_length:21)], total_byte_size:57, num_rows:1, file_offset:4, total_compressed_size:97, ordinal:0)], key_value_metadata:[KeyValue(key:iceberg.schema, value:{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"id\",\"required\":true,\"type\":\"int\"},{\"id\":2,\"name\":\"data\",\"required\":true,\"type\":\"string\"}]})], created_by:parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:376)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:143)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:138)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:1338)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:1203)\n\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:255)\n\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n\t... 16 more\nCaused by: org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TTransportException: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:199)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:482)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:489)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:263)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:245)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:71)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1390)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1240)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData.write(FileMetaData.java:1118)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:373)\n\t... 23 more\nCaused by: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.checkOpen(S3ABlockOutputStream.java:243)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.write(S3ABlockOutputStream.java:294)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:197)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlakefs.main.rmoff.table4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1460\u001b[0m, in \u001b[0;36mDataFrameWriterV2.append\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m    Append the contents of the data frame to the output table.\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.append.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:767)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:244)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:244)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:195)\n\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:149)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2) (dc58cc99c0e5 executor driver): java.io.UncheckedIOException: Failed to close current writer\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:124)\n\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:716)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:698)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:453)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: can not write FileMetaData(version:1, schema:[SchemaElement(name:table, num_children:2), SchemaElement(type:INT32, repetition_type:REQUIRED, name:id, field_id:1), SchemaElement(type:BYTE_ARRAY, repetition_type:REQUIRED, name:data, converted_type:UTF8, field_id:2, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:INT32, encodings:[BIT_PACKED, PLAIN], path_in_schema:[id], codec:GZIP, num_values:1, total_uncompressed_size:27, total_compressed_size:47, data_page_offset:4, statistics:Statistics(max:00 00 00 00, min:00 00 00 00, null_count:0, max_value:00 00 00 00, min_value:00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:145, offset_index_length:10, column_index_offset:101, column_index_length:23), ColumnChunk(file_offset:51, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[BIT_PACKED, PLAIN], path_in_schema:[data], codec:GZIP, num_values:1, total_uncompressed_size:30, total_compressed_size:50, data_page_offset:51, statistics:Statistics(max:66 6F 6F, min:66 6F 6F, null_count:0, max_value:66 6F 6F, min_value:66 6F 6F), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:155, offset_index_length:10, column_index_offset:124, column_index_length:21)], total_byte_size:57, num_rows:1, file_offset:4, total_compressed_size:97, ordinal:0)], key_value_metadata:[KeyValue(key:iceberg.schema, value:{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"id\",\"required\":true,\"type\":\"int\"},{\"id\":2,\"name\":\"data\",\"required\":true,\"type\":\"string\"}]})], created_by:parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:376)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:143)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:138)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:1338)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:1203)\n\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:255)\n\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n\t... 16 more\nCaused by: org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TTransportException: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:199)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:482)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:489)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:263)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:245)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:71)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1390)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1240)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData.write(FileMetaData.java:1118)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:373)\n\t... 23 more\nCaused by: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.checkOpen(S3ABlockOutputStream.java:243)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.write(S3ABlockOutputStream.java:294)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:197)\n\t... 32 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n\t... 43 more\nCaused by: java.io.UncheckedIOException: Failed to close current writer\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:124)\n\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:716)\n\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:698)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:453)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.io.IOException: can not write FileMetaData(version:1, schema:[SchemaElement(name:table, num_children:2), SchemaElement(type:INT32, repetition_type:REQUIRED, name:id, field_id:1), SchemaElement(type:BYTE_ARRAY, repetition_type:REQUIRED, name:data, converted_type:UTF8, field_id:2, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:INT32, encodings:[BIT_PACKED, PLAIN], path_in_schema:[id], codec:GZIP, num_values:1, total_uncompressed_size:27, total_compressed_size:47, data_page_offset:4, statistics:Statistics(max:00 00 00 00, min:00 00 00 00, null_count:0, max_value:00 00 00 00, min_value:00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:145, offset_index_length:10, column_index_offset:101, column_index_length:23), ColumnChunk(file_offset:51, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[BIT_PACKED, PLAIN], path_in_schema:[data], codec:GZIP, num_values:1, total_uncompressed_size:30, total_compressed_size:50, data_page_offset:51, statistics:Statistics(max:66 6F 6F, min:66 6F 6F, null_count:0, max_value:66 6F 6F, min_value:66 6F 6F), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:155, offset_index_length:10, column_index_offset:124, column_index_length:21)], total_byte_size:57, num_rows:1, file_offset:4, total_compressed_size:97, ordinal:0)], key_value_metadata:[KeyValue(key:iceberg.schema, value:{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"id\",\"required\":true,\"type\":\"int\"},{\"id\":2,\"name\":\"data\",\"required\":true,\"type\":\"string\"}]})], created_by:parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:376)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:143)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.writeFileMetaData(Util.java:138)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:1338)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:1203)\n\tat org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:255)\n\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n\t... 16 more\nCaused by: org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TTransportException: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:199)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:482)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:489)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:263)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:245)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:71)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1390)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData$FileMetaDataStandardScheme.write(FileMetaData.java:1240)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.FileMetaData.write(FileMetaData.java:1118)\n\tat org.apache.iceberg.shaded.org.apache.parquet.format.Util.write(Util.java:373)\n\t... 23 more\nCaused by: java.io.IOException: Filesystem WriteOperationHelper {bucket=lakefs-iceberg} closed\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.checkOpen(S3ABlockOutputStream.java:243)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.write(S3ABlockOutputStream.java:294)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.iceberg.shaded.org.apache.parquet.shaded.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:197)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"lakefs.main.rmoff.table4\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a4ed9-14ca-4dbc-9e3b-71be022551b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM lakefs.main.rmoff.table4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af4cdb-458c-4811-93b0-7ed040ffeece",
   "metadata": {},
   "source": [
    "## Commit the new table and its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec33a1-6332-4fc9-a003-a4b93914d218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repo.id, \"main\", CommitCreation(\n",
    "    message=\"Initial data load\",\n",
    "    metadata={'author': 'rmoff'}\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f55e0-5529-4c85-acb5-a5a822187cb2",
   "metadata": {},
   "source": [
    "## Create a new branch\n",
    "\n",
    "_This is copy-on-write; we're not duplicating the data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d1de1-eac2-4cf3-9fc9-6975f01201ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(repo.id, BranchCreation(\"dev\",\"main\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d38f9-65ef-4238-970c-26c46dad2e5d",
   "metadata": {},
   "source": [
    "## Observe that the new branch has the same data as `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6f1de-8c2d-4d55-960a-9926614b07e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM lakefs.dev.rmoff.table4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323deef4-37ad-4703-86f7-0c4a79b350a7",
   "metadata": {},
   "source": [
    "## Insert a row into the `dev` branch's version of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4ec35-aa4c-43ad-8100-4e7b285f748a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO lakefs.dev.rmoff.table4 VALUES(3,\"wibble\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2498b-85b7-4951-a261-38d469bb463a",
   "metadata": {},
   "source": [
    "## Inspect the `dev` version of the table with the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf0631-86b0-48ab-ba2b-db92c2a3bbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM lakefs.dev.rmoff.table4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a4ecd-ca58-4350-a4f7-6b4d0218b09e",
   "metadata": {},
   "source": [
    "## Observe that the `main` version of the table remain unaltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9ccb2-672c-451b-a902-5b53c09f64de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM lakefs.main.rmoff.table4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9585ff-fb10-4caf-af6e-0ea4d8c93160",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5982ee-c99d-4914-a836-e33cb70ca3b3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3e7fa-938c-45b5-9a67-b9eb24087b08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f201a0-5e4e-4135-9ab0-f750d98f271d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "if lakefsEndPoint=='http://lakefs:8000':\n",
    "    lakeFSWebUI='http://localhost:8000'\n",
    "else:\n",
    "    lakeFSWebUI=lakefsEndPoint\n",
    "\n",
    "md(f\"### üëâüèª View the objects in [lakeFS web UI]({lakeFSWebUI}/repositories/{repo.id}/objects)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
