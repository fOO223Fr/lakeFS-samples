{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40007e51",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Running data quality checks with lakeFS web hooks\n",
    "\n",
    "_This notebook shows how to use [lakeFS Hooks](https://docs.lakefs.io/hooks/overview.html)_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b0f59",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48a189",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beda647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3345e",
   "metadata": {},
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e31b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b75d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2206d9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fe6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"hooks-demo-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19301378",
   "metadata": {},
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd6c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed5b43",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21410385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version{v.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6261",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bab38",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd47e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550604ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ingest_branch = \"ingest-landing-area\"\n",
    "staging_branch = \"staging-area\"\n",
    "prod_branch = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76f5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465399a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3335f",
   "metadata": {},
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07acc4",
   "metadata": {},
   "source": [
    "## Creating Ingest and Staging branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c7e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2681e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=ingest_branch, \n",
    "                                                                    source=prod_branch)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee13a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=staging_branch, \n",
    "                                                                    source=prod_branch)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda20a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22215b",
   "metadata": {},
   "source": [
    "## Uploading movies data to ingest branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897649d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ingest_data = \"movies.csv\"\n",
    "\n",
    "ingest_path = f'dt={str(date.today())}/{ingest_data}'\n",
    "ingest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87b305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'/data/{ingest_data}', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=ingest_branch, \n",
    "                                 path=ingest_path, \n",
    "                                 content=f\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd829e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=ingest_branch).results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c0e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=ingest_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=\"netflix movie data arrived at landing area (today's partition)\")\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a6c11",
   "metadata": {},
   "source": [
    "## Uploading actions.yaml config file to staging branch\n",
    "\n",
    "* We want to run data quality tests on staging branch before merging the data into production. Hooks config file `actions.yaml` needs to be in the branch on which the tests are run.\n",
    "\n",
    "* So add `_lakefs_actions/actions.yaml` to staging branch\n",
    "* `actions.yaml` contains a pre-merge hook configured to check for file format validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d2b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hooks_config_yaml = \"actions.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f22bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./hooks/{hooks_config_yaml}', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=staging_branch, \n",
    "                                 path=f'{hooks_prefix}/{hooks_config_yaml}', \n",
    "                                 content=f\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b677a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=staging_branch).results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06aed40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message='Added hooks config file - actions.yaml to staging area')\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d4979",
   "metadata": {},
   "source": [
    "## Extracting data from ingest branch for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41981c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ingest_long_path = f\"s3a://{repo_name}/{ingest_branch}/{ingest_path}\"\n",
    "ingest_long_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad4a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df = spark.read.option(\"header\",\"true\").csv(ingest_long_path)\n",
    "print(movies_df.count())\n",
    "print(movies_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65193a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e1a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df = movies_df.sample(False,0.1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c8c5f",
   "metadata": {},
   "source": [
    "## Loading transformed data into Staging Area/Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee0af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "staging_long_path = f\"s3a://{repo_name}/{staging_branch}\"\n",
    "staging_long_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dde9d",
   "metadata": {},
   "source": [
    "## Scenario #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9676d4",
   "metadata": {},
   "source": [
    "### Writing parquet files to staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5577b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.write.option(\"header\",True)\\\n",
    "        .partitionBy(\"type\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .parquet(f\"{staging_long_path}/analytics/movies-by-type-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babad0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message='loaded paritioned movies parquet to staging area'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec9501",
   "metadata": {},
   "source": [
    "### Pushing parquet files to Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a051f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=staging_branch, \n",
    "                              destination_branch=prod_branch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7867f",
   "metadata": {},
   "source": [
    "## Scenario #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172a4f0",
   "metadata": {},
   "source": [
    "### Writing csv files to staging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8cab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.write.option(\"header\",True)\\\n",
    "        .partitionBy(\"type\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .csv(f\"{staging_long_path}/analytics/movies-by-type-csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc736c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=staging_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message='loaded paritioned movies csv to staging area'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e839f",
   "metadata": {},
   "source": [
    "### Pushing csv files to Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f5c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=staging_branch, \n",
    "                              destination_branch=prod_branch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831cb9b",
   "metadata": {},
   "source": [
    "### Why did the merge operation fail?\n",
    "If you look deeper into the error log, you'll see that the merge request failed with status code '412' (precondition failed). The actions file was executed and blocked a commit with a csv file to merge into main.\n",
    "\n",
    "## Action execution history\n",
    "\n",
    "üëâüèª You can see previous actions run [here](http://localhost:8000/repositories/hooks-demo-01/actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
