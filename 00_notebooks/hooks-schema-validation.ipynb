{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c070f9-46a7-4b1c-852c-e0fb1b1efa80",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Using [Lua Hooks](https://docs.lakefs.io/howto/hooks/lua.html) in lakeFS (similar to GitHub Actions)\n",
    "\n",
    "This notebook demonstrated how to create a pre-merge hook in lakeFS that validates the schema files before merging them into the production branch. \n",
    "\n",
    "1. Define a hook configuration file and a Lua script for schema validation. \n",
    "2. Perform an ETL process by creating an ingestion branch, defining the table schema, and creating a table and atomically promoted the data to the production branch through a merge.\n",
    "3. Attempt to change the schema of the table and promote it to production again. \n",
    "4. The pre-merge hook prevented the promotion due to schema changes, resulting in a Precondition Failed error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9466e4e-ce3e-42af-aa12-4b8fe3327911",
   "metadata": {},
   "source": [
    "![Actions UI](./images/LuaHooks/schemaValidationFlow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5260f",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44c241",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4117da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFOLKFSSAMPLES'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e6a6c",
   "metadata": {},
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f5826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08edd08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71d35a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2697e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"schema-validation-example-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577478a0-03be-421e-9c15-57987c2e5b5c",
   "metadata": {},
   "source": [
    "### Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c8110-a7f3-4222-880a-50a69129d7dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestionBranch = \"ingestion_branch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568f3af-deb4-4115-8314-9a9a919c6c68",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6e0a-446d-4b18-be51-ade404e49b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import lakefs\n",
    "from assets.lakefs_demo import print_commit, print_diff\n",
    "from pyspark.sql.types import ByteType, IntegerType, LongType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a74ab2-fb0b-4f62-b663-5987cc2120bc",
   "metadata": {},
   "source": [
    "### Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05df7e-31c5-4bf7-bc9b-4a19effc173f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"LAKECTL_SERVER_ENDPOINT_URL\"] = lakefsEndPoint\n",
    "os.environ[\"LAKECTL_CREDENTIALS_ACCESS_KEY_ID\"] = lakefsAccessKey\n",
    "os.environ[\"LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY\"] = lakefsSecretKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc3170",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130fd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.client.Client().version\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57327f0",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0f4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = lakefs.Repository(repo_name).create(storage_namespace=f\"{storageNamespace}/{repo_name}\", default_branch=mainBranch, exist_ok=True)\n",
    "branchMain = repo.branch(mainBranch)\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321137b",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad68bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aacc4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5c35b",
   "metadata": {},
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c517d4",
   "metadata": {},
   "source": [
    "## Setup and Configure Hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02c9a3-5d5d-4422-b12b-7275b03d8cc9",
   "metadata": {},
   "source": [
    "### Configure hooks in the repository\n",
    "\n",
    "* Upload [Hooks config YAML file](./hooks/pre-merge-schema-validation.yaml) for schema validation to check for any schema changes before data is merged to main branch\n",
    "* Hooks config file must be uploaded to \"_lakefs_actions\" prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc843202-39cd-4fe0-9f2a-a9e5807c15bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hooks_config_yaml = \"pre-merge-schema-validation.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n",
    "\n",
    "contentToUpload = open(f'./hooks/{hooks_config_yaml}', 'r').read()\n",
    "print(branchMain.object(f'{hooks_prefix}/{hooks_config_yaml}').upload(data=contentToUpload, mode='wb', pre_sign=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7b074-2d71-40a7-af84-a4fc96e5e30f",
   "metadata": {},
   "source": [
    "### Upload script\n",
    "\n",
    "##### The script [parquet_schema_change.lua](./hooks/parquet_schema_change.lua) checks for any schema changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffb6d4-8c0f-45e1-bac3-7d966d330c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lua_script_file_name = \"parquet_schema_change.lua\"\n",
    "lua_scripts_path = \"scripts\"\n",
    "\n",
    "contentToUpload = open(f'./hooks/{lua_script_file_name}', 'r').read()\n",
    "print(branchMain.object(f'{lua_scripts_path}/{lua_script_file_name}').upload(data=contentToUpload, mode='wb', pre_sign=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5605b-09d9-40f4-b6ff-eca0c0d8e313",
   "metadata": {},
   "source": [
    "### Commit changes to the lakeFS repo and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ce872-fd62-4fda-b061-2183f2e66f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = branchMain.commit(message='Added hooks config file and schema validation script')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bbe34-db01-4128-84d9-43d782aa3a77",
   "metadata": {},
   "source": [
    "# ETL Job Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a7ce6-c23e-4529-91d4-d50c7e273ca4",
   "metadata": {},
   "source": [
    "## Create a new branch which will be used to ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a5417-9a3f-4370-820c-469025be9e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branchIngestion = repo.branch(ingestionBranch).create(source_reference=mainBranch, exist_ok=True)\n",
    "print(f\"{ingestionBranch} ref:\", branchIngestion.get_commit().id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5dde0-d39b-4ecd-b3c4-0d16968f4c29",
   "metadata": {},
   "source": [
    "## For this demo - we'll be utilizing a dataset - [Orion Star - Sports and outdoors RDBMS dataset](https://www.kaggle.com/datasets/chethanp11/orion-star-sports-and-outdoors-rdbms-dataset) from [Kaggle](https://www.kaggle.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4da42-2d3c-4e1c-9b9b-91925d6d659c",
   "metadata": {},
   "source": [
    "## Define [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) data file schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad323351-29ab-4f97-83bc-dd4d81f663d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersSchema = StructType([\n",
    "  StructField(\"User_ID\", IntegerType(), False), \n",
    "  StructField(\"Country\", StringType(), False),\n",
    "  StructField(\"Gender\", StringType(), False),\n",
    "  StructField(\"Personal_ID\", IntegerType(), True),\n",
    "  StructField(\"Customer_Name\", StringType(), False),\n",
    "  StructField(\"Customer_FirstName\", StringType(), False),\n",
    "  StructField(\"Customer_LastName\", StringType(), False),\n",
    "  StructField(\"Birth_Date\", StringType(), False),\n",
    "  StructField(\"Customer_Address\", StringType(), False),\n",
    "  StructField(\"Street_ID\", LongType(), False),\n",
    "  StructField(\"Street_Number\", IntegerType(), False),\n",
    "  StructField(\"Customer_Type_ID\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8461a-6eb8-49d0-9652-229c7465c095",
   "metadata": {},
   "source": [
    "## Create Customers delta table in the new branch (using [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91e8de-7a05-468c-ad1a-05a2db3071d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersTablePath = f\"s3a://{repo.id}/{ingestionBranch}/tables/customers\"\n",
    "df = spark.read.csv('/data/OrionStar/CUSTOMER.csv',header=True,schema=customersSchema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(customersTablePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c751463-94ae-42e2-92e4-b8ff42b8d73d",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794fd73-cf59-4057-858e-8ae85aee546f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = branchIngestion.commit(message='Added customers Delta table', \n",
    "        metadata={'using': 'python_api'})\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3b56d-e4c5-4a9b-90ff-bac820520e92",
   "metadata": {},
   "source": [
    "## Promote the Data into production\n",
    "\n",
    "#### Merging the ingestion branch with the current schema to the production branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41d5e9-8bae-4a87-b487-77583242290f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = branchIngestion.merge_into(branchMain)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4227e9-7144-423f-becc-386661166133",
   "metadata": {},
   "source": [
    "# On the next ETL Cycle - Change the schema and try to promote new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e7a6e-d625-4dd5-8802-916346a5e80b",
   "metadata": {},
   "source": [
    "## Change \"Country\" column to \"Country_Name\" in the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bced5-a8c1-4955-a036-9f23c46df85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersSchema = StructType([\n",
    "  StructField(\"User_ID\", IntegerType(), False),\n",
    "  StructField(\"Country_Name\", StringType(), False), # Column name changes from Country to Country_name\n",
    "  StructField(\"Gender\", StringType(), False),\n",
    "  StructField(\"Personal_ID\", IntegerType(), True),\n",
    "  StructField(\"Customer_Name\", StringType(), False),\n",
    "  StructField(\"Customer_FirstName\", StringType(), False),\n",
    "  StructField(\"Customer_LastName\", StringType(), False),\n",
    "  StructField(\"Birth_Date\", StringType(), False),\n",
    "  StructField(\"Customer_Address\", StringType(), False),\n",
    "  StructField(\"Street_ID\", LongType(), False),\n",
    "  StructField(\"Street_Number\", IntegerType(), False),\n",
    "  StructField(\"Customer_Type_ID\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f38b6f-11db-4bfe-8f92-dc3a65152f96",
   "metadata": {},
   "source": [
    "## Create Customers delta table in the new branch (using [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c8352-ec8b-483a-be03-2162eb2a1a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersTablePath = f\"s3a://{repo.id}/{ingestionBranch}/tables/customers\"\n",
    "df = spark.read.csv('/data/OrionStar/CUSTOMER.csv',header=True,schema=customersSchema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(customersTablePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346df42-d5a5-4aa6-bc04-94febc80e4c3",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b13ee-6423-4b7a-b98f-b4aa3cf6b8ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = branchIngestion.commit(message='Added customers Delta tables with schema changes!', \n",
    "        metadata={'using': 'python_api'})\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7aa3d-215e-4a06-b69f-a5acc0f00cd3",
   "metadata": {},
   "source": [
    "## Merge new branch to the main branch\n",
    "\n",
    "Merge will fail because schema changed. \n",
    "\n",
    "Note the error message: `(412) Reason: Precondition Failed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091fa2e-647e-4a61-a73a-242399dc2a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = branchIngestion.merge_into(branchMain)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a0587-8c74-4cbe-a213-d766ea23cb65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## You can also review all Actions in lakeFS UI\n",
    "\n",
    "http://localhost:8000/repositories/schema-validation-example-repo/actions\n",
    "\n",
    "\n",
    "![Actions UI](./images/LuaHooks/SchemaValidation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36b6f-0cd2-4ffb-9b82-eecd06c8b5a8",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
