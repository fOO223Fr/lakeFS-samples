{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c070f9-46a7-4b1c-852c-e0fb1b1efa80",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Using [Lua Hooks]((https://docs.lakefs.io/hooks/)) in lakeFS (similar to GitHub Actions)\n",
    "\n",
    "This notebook demonstrated how to create a pre-merge hook in lakeFS that validates the schema files before merging them into the production branch. \n",
    "\n",
    "1. Define a hook configuration file and a Lua script for schema validation. \n",
    "2. Perform an ETL process by creating an ingestion branch, defining the table schema, and creating a table and atomically promoted the data to the production branch through a merge.\n",
    "3. Attempt to change the schema of the table and promote it to production again. \n",
    "4. The pre-merge hook prevented the promotion due to schema changes, resulting in a Precondition Failed error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9466e4e-ce3e-42af-aa12-4b8fe3327911",
   "metadata": {},
   "source": [
    "![Actions UI](./images/LuaHooks/schemaValidationFlow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5260f",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44c241",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4117da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e6a6c",
   "metadata": {},
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f5826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08edd08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71d35a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2697e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"schema-validation-example-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2656b06",
   "metadata": {},
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a0812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc3170",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130fd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version{v.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57327f0",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0f4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321137b",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad68bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a8965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestionBranch = \"ingestion_branch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aacc4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5c35b",
   "metadata": {},
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c517d4",
   "metadata": {},
   "source": [
    "## Setup and Configure Hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02c9a3-5d5d-4422-b12b-7275b03d8cc9",
   "metadata": {},
   "source": [
    "### Configure hooks in the repository\n",
    "\n",
    "* Upload [Hooks config YAML file](./LuaHooks/pre-merge-schema-validation.yaml) for schema validation to check for any schema changes before data is merged to main branch\n",
    "* Hooks config file must be uploaded to \"_lakefs_actions\" prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6f0b1-0e8f-4259-990b-a67c68c80564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hooks_config_yaml = \"pre-merge-schema-validation.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n",
    "with open(f'./hooks/{hooks_config_yaml}', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo.id, \n",
    "                                 branch=mainBranch, \n",
    "                                 path=f'{hooks_prefix}/{hooks_config_yaml}', \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7b074-2d71-40a7-af84-a4fc96e5e30f",
   "metadata": {},
   "source": [
    "### Upload script\n",
    "\n",
    "The script (`hooks/parquet_schema_change.lua`) checks for any schema changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffb6d4-8c0f-45e1-bac3-7d966d330c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lua_script_file_name = \"parquet_schema_change.lua\"\n",
    "lua_scripts_path = \"scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dbee3-951c-4174-b2f6-23c9823be6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./hooks/{lua_script_file_name}', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo.id, \n",
    "                                 branch=mainBranch, \n",
    "                                 path=f'{lua_scripts_path}/{lua_script_file_name}', \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5605b-09d9-40f4-b6ff-eca0c0d8e313",
   "metadata": {},
   "source": [
    "### Commit changes to the lakeFS repo and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f9856-8615-40e3-8608-dc34fd1ad872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=mainBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Added hooks config file and schema validation script'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bbe34-db01-4128-84d9-43d782aa3a77",
   "metadata": {},
   "source": [
    "# ETL Job Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a7ce6-c23e-4529-91d4-d50c7e273ca4",
   "metadata": {},
   "source": [
    "## Create a new branch which will be used to ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa8e43-f998-4e66-835f-6b3c38dca0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(\n",
    "    repository=repo.id, \n",
    "    branch_creation=BranchCreation(\n",
    "        name=ingestionBranch, source=mainBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5dde0-d39b-4ecd-b3c4-0d16968f4c29",
   "metadata": {},
   "source": [
    "## For this demo - we'll be utilizing a dataset - [Orion Star - Sports and outdoors RDBMS dataset](https://www.kaggle.com/datasets/chethanp11/orion-star-sports-and-outdoors-rdbms-dataset) from [Kaggle](https://www.kaggle.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4da42-2d3c-4e1c-9b9b-91925d6d659c",
   "metadata": {},
   "source": [
    "## Define [CUSTOMER.csv](../data/samples/OrionStar/CUSTOMER.csv) data file schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad323351-29ab-4f97-83bc-dd4d81f663d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ByteType, IntegerType, LongType, StringType, StructType, StructField\n",
    "\n",
    "customersSchema = StructType([\n",
    "  StructField(\"User_ID\", IntegerType(), False), \n",
    "  StructField(\"Country\", StringType(), False),\n",
    "  StructField(\"Gender\", StringType(), False),\n",
    "  StructField(\"Personal_ID\", IntegerType(), True),\n",
    "  StructField(\"Customer_Name\", StringType(), False),\n",
    "  StructField(\"Customer_FirstName\", StringType(), False),\n",
    "  StructField(\"Customer_LastName\", StringType(), False),\n",
    "  StructField(\"Birth_Date\", StringType(), False),\n",
    "  StructField(\"Customer_Address\", StringType(), False),\n",
    "  StructField(\"Street_ID\", LongType(), False),\n",
    "  StructField(\"Street_Number\", IntegerType(), False),\n",
    "  StructField(\"Customer_Type_ID\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8461a-6eb8-49d0-9652-229c7465c095",
   "metadata": {},
   "source": [
    "## Create Customers delta table in the new branch (using [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91e8de-7a05-468c-ad1a-05a2db3071d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersTablePath = f\"s3a://{repo.id}/{ingestionBranch}/tables/customers\"\n",
    "df = spark.read.csv('/data/OrionStar/CUSTOMER.csv',header=True,schema=customersSchema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(customersTablePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c751463-94ae-42e2-92e4-b8ff42b8d73d",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c59fde-bdd4-40ae-af01-05d4f85e06c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=ingestionBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Added customers Delta table', \n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3b56d-e4c5-4a9b-90ff-bac820520e92",
   "metadata": {},
   "source": [
    "## Promote the Data into production\n",
    "\n",
    "#### Merging the ingestion branch with the current schema to the production branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5219-f8e5-456c-ac34-906f2197d7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id,\n",
    "    source_ref=ingestionBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4227e9-7144-423f-becc-386661166133",
   "metadata": {},
   "source": [
    "# On the next ETL Cycle - Change the schema and try to promote new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e7a6e-d625-4dd5-8802-916346a5e80b",
   "metadata": {},
   "source": [
    "## Change \"Country\" column to \"Country_Name\" in the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bced5-a8c1-4955-a036-9f23c46df85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersSchema = StructType([\n",
    "  StructField(\"User_ID\", IntegerType(), False),\n",
    "  StructField(\"Country_Name\", StringType(), False), # Column name changes from Country to Country_name\n",
    "  StructField(\"Gender\", StringType(), False),\n",
    "  StructField(\"Personal_ID\", IntegerType(), True),\n",
    "  StructField(\"Customer_Name\", StringType(), False),\n",
    "  StructField(\"Customer_FirstName\", StringType(), False),\n",
    "  StructField(\"Customer_LastName\", StringType(), False),\n",
    "  StructField(\"Birth_Date\", StringType(), False),\n",
    "  StructField(\"Customer_Address\", StringType(), False),\n",
    "  StructField(\"Street_ID\", LongType(), False),\n",
    "  StructField(\"Street_Number\", IntegerType(), False),\n",
    "  StructField(\"Customer_Type_ID\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f38b6f-11db-4bfe-8f92-dc3a65152f96",
   "metadata": {},
   "source": [
    "## Create Customers delta table in the new branch (using [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c8352-ec8b-483a-be03-2162eb2a1a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customersTablePath = f\"s3a://{repo.id}/{ingestionBranch}/tables/customers\"\n",
    "df = spark.read.csv('/data/OrionStar/CUSTOMER.csv',header=True,schema=customersSchema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(customersTablePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346df42-d5a5-4aa6-bc04-94febc80e4c3",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9146afa-00fb-4fbe-9d82-5f020ecbced0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(\n",
    "    repository=repo.id,\n",
    "    branch=ingestionBranch,\n",
    "    commit_creation=CommitCreation(\n",
    "        message='Added customers Delta tables with schema changes!', \n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7aa3d-215e-4a06-b69f-a5acc0f00cd3",
   "metadata": {},
   "source": [
    "## Merge new branch to the main branch\n",
    "\n",
    "Merge will fail because schema changed. \n",
    "\n",
    "Note the error message: `(412) Reason: Precondition Failed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc738-f7fa-4170-abfe-95895f5d928f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id,\n",
    "    source_ref=ingestionBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a0587-8c74-4cbe-a213-d766ea23cb65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## You can also review all Actions in lakeFS UI\n",
    "\n",
    "http://localhost:8000/repositories/schema-validation-example-repo/actions\n",
    "\n",
    "\n",
    "![Actions UI](./Images/LuaHooks/SchemaValidation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36b6f-0cd2-4ffb-9b82-eecd06c8b5a8",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
