{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Reprocess and Backfill Data with new ETL logic\n",
    "\n",
    "_Note that whilst this works, it's a bit of a hack!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67662314-7dcd-4e6c-869f-24a1a76d1a0e",
   "metadata": {},
   "source": [
    "You will run following steps in this notebook (refer to the image below):\n",
    "\n",
    "1. Create repository with the Main branch\n",
    "2. Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "3. Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "4. Repetition of step # 2\n",
    "5. Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes.\n",
    "6. Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a0676-60ff-47e8-af5b-fa44206dcb3d",
   "metadata": {},
   "source": [
    "![Reprocess](./images/reprocess-data/Reprocess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6f67a-d18c-43f1-b4ee-8ce2e3038857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1744d-4095-4497-a443-c2cb76d0ecee",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4410b2f7-bbd1-48f2-88cd-43440e4cd8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6b274-8d00-4fbf-830b-339fd29e0ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb1c20a-e248-4596-a99b-a7a3784cf9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e6a97-c4c3-42b4-8881-568878f54d04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22a8f6-ae2d-449c-b5ae-6b0abc720210",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45fbab3-5f98-46ad-8e12-9021ed2bf81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"reprocess-backfill-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a658-9fa0-470c-b829-45d0e7dd1164",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4795b1e7-761e-4f7a-840b-982f99ff3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9217c-af15-48a1-ba77-c497d93b9cda",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bbe675-3c5e-41c7-8471-7a8a90253827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying lakeFS credentials‚Ä¶\n",
      "‚Ä¶‚úÖlakeFS credentials verified\n",
      "\n",
      "‚ÑπÔ∏èlakeFS version0.102.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version{v.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1441793-7174-4f6f-b400-7eca020ad561",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7196c9-370a-441a-8ea3-02ee03484991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://32673b7c4a71:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff781e4400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea20cc8-9a7b-4285-a675-7373a0fb65cc",
   "metadata": {},
   "source": [
    "### Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b839850-5954-4631-8e7e-d0dee6d17dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestBranch = \"ingest\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "processedFileName = \"lakefs_test_processed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db425f40-db8d-4298-8cdb-c532fc98fc97",
   "metadata": {},
   "source": [
    "### Define data file schema + python libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e42f399-19b7-4def-9988-916a7f8ba067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "dataFileSchema = StructType([\n",
    "  StructField(\"Apparel_Sales\", DoubleType(), False),\n",
    "  StructField(\"Books_Sales\", DoubleType(), False),\n",
    "  StructField(\"Electronics_Sales\", DoubleType(), False),\n",
    "  StructField(\"Furniture_Sales\", DoubleType(), False),\n",
    "  StructField(\"Toys_Sales\", DoubleType(), False)\n",
    "])\n",
    "processedDataFileSchema = StructType([\n",
    "  StructField(\"Apparel_Sales\", DoubleType(), False),\n",
    "  StructField(\"Books_Sales\", DoubleType(), False),\n",
    "  StructField(\"Electronics_Sales\", DoubleType(), False),\n",
    "  StructField(\"Furniture_Sales\", DoubleType(), False),\n",
    "  StructField(\"Toys_Sales\", DoubleType(), False),\n",
    "  StructField(\"Total_Sales\", DoubleType(), False),\n",
    "  StructField(\"Average_Sales_per_Product_Category\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf16b13-7300-4840-a3a9-91765e945b3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431be3f-d741-4cb6-98cf-8b21b8f2a489",
   "metadata": {},
   "source": [
    "## Step 1: Create repository with the Main branch\n",
    "\n",
    "### (if above mentioned repo already exists on your lakeFS server then you can skip this operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2218d-3af0-415d-a35a-ab4ba06ac040",
   "metadata": {},
   "source": [
    "![Step 1](./images/reprocess-data/Step1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384aa4d3-038a-4a56-b30e-c536e3911478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing repo reprocess-backfill-data using storage namespace s3://example/reprocess-backfill-data\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360b5f6-76db-438b-9aa0-e40070b86bb9",
   "metadata": {},
   "source": [
    "## Step 2: Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "\n",
    "### ([ETL](./ReprocessData/ETL.ipynb) job normally run as a batch job but run ETL job manually here for the demo. It will take around a minute to run this step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2b2de-617e-4485-8b41-e471feae8a93",
   "metadata": {},
   "source": [
    "![Step 2](./images/reprocess-data/Step2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48de4952-2697-4f50-a165-fc8bf0018115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "total 384\n",
      "-rw-r--r--  1 jovyan users  6703 Jun 19 10:01 00_index.ipynb\n",
      "-rw-r--r--  1 jovyan users   475 Jun 19 09:18 config.json\n",
      "drwxr-xr-x  2 jovyan users    64 Jun  2 14:48 data\n",
      "-rw-r--r--  1 jovyan users 23674 Jun 15 09:14 data-lineage.ipynb\n",
      "-rw-r--r--  1 jovyan users 16795 Jun 15 09:14 delta-diff.ipynb\n",
      "-rw-r--r--  1 jovyan users 31028 Jun 15 09:14 delta-lake.ipynb\n",
      "drwxr-xr-x  9 jovyan users   288 Jun 19 10:01 demos-and-talks\n",
      "-rw-r--r--  1 jovyan users 15555 Jun 19 10:36 dev-test.ipynb\n",
      "drwxr-xr-x  8 jovyan users   256 Jun 15 09:12 hooks\n",
      "-rw-r--r--  1 jovyan users 17302 Jun 15 09:14 hooks-demo.ipynb\n",
      "-rw-r--r--  1 jovyan users 31191 Jun 15 09:14 hooks-schema-and-pii-validation.ipynb\n",
      "-rw-r--r--  1 jovyan users 19844 Jun 15 09:14 hooks-schema-validation.ipynb\n",
      "drwxr-xr-x  8 jovyan users   256 Jun 15 09:12 images\n",
      "-rw-r--r--  1 jovyan users  8266 Jun 19 10:01 import-multiple-buckets.ipynb\n",
      "-rw-r--r--  1 jovyan users 51237 Jun 16 14:44 lakefs-iceberg.ipynb\n",
      "-rw-r--r--  1 jovyan users 40069 Jun 15 09:14 ml-experimentation-wine-quality-prediction.ipynb\n",
      "drwxr-xr-x  3 jovyan users    96 Jun 19 10:26 notebooks\n",
      "drwxr-xr-x 13 jovyan users   416 Jun 16 16:01 papermill\n",
      "drwxr-xr-x 15 jovyan users   480 Jun 19 11:29 papermill-out\n",
      "drwxr-xr-x  4 jovyan users   128 Jun  2 15:32 __pycache__\n",
      "-rw-r--r--  1 jovyan users 25809 Jun 15 09:14 rbac-demo.ipynb\n",
      "-rw-r--r--  1 jovyan users 18651 Jun 15 09:14 reprocess-backfill-data.ipynb\n",
      "drwxr-xr-x  4 jovyan users   128 Jun 15 09:12 reprocess-data\n",
      "-rw-r--r--  1 jovyan users 14571 Jun 15 09:14 spark-demo.ipynb\n",
      "drwxr-xr-x  5 jovyan users   160 Jun 16 14:14 spark-warehouse\n",
      "-rw-r--r--  1 jovyan users 13730 Jun 19 08:23 tmp.excalidraw\n",
      "drwxr-xr-x  4 jovyan users   128 Jun 19 10:01 utils_ml_reproducibility\n",
      "-rw-r--r--  1 jovyan users 20747 Jun 15 09:14 version-control-of-multi-buckets-pipelines.ipynb\n",
      "drwxr-xr-x 13 jovyan users   416 Jun 19 10:18 write-audit-publish\n",
      "\n",
      "./data:\n",
      "total 0\n",
      "\n",
      "./demos-and-talks:\n",
      "total 108\n",
      "-rw-r--r-- 1 jovyan users 23451 Jun 15 09:14 chaos-engineering_dais22.ipynb\n",
      "-rw-r--r-- 1 jovyan users   478 Jun 19 08:49 config.json\n",
      "-rw-r--r-- 1 jovyan users 35070 Jun 15 09:14 lakeFS-DeltaLake-multi-table-transaction-consistency.ipynb\n",
      "-rw-r--r-- 1 jovyan users  9074 Jun 15 09:14 lakeFSOnSynapse.ipynb\n",
      "drwxr-xr-x 4 jovyan users   128 Jun 19 10:01 ml_reproducibility\n",
      "-rw-r--r-- 1 jovyan users 31351 Jun 19 10:01 ml-reproducibility.ipynb\n",
      "\n",
      "./demos-and-talks/ml_reproducibility:\n",
      "total 20\n",
      "-rw-r--r-- 1 jovyan users  5927 Jun 19 10:01 file_utils.ipynb\n",
      "-rw-r--r-- 1 jovyan users 10626 Jun 19 10:01 ml_utils.ipynb\n",
      "\n",
      "./hooks:\n",
      "total 24\n",
      "-rw-r--r-- 1 jovyan users  420 Jun 15 09:12 actions.yaml\n",
      "-rw-r--r-- 1 jovyan users 6250 Jun 15 09:12 parquet_schema_change.lua\n",
      "-rw-r--r-- 1 jovyan users 2495 Jun 15 09:12 parquet_schema_validator.lua\n",
      "-rw-r--r-- 1 jovyan users 1030 Jun 15 09:12 pre-merge-schema-and-pii-validation.yaml\n",
      "-rw-r--r-- 1 jovyan users  625 Jun 15 09:12 pre-merge-schema-validation.yaml\n",
      "\n",
      "./images:\n",
      "total 220\n",
      "drwxr-xr-x 3 jovyan users     96 Jun 15 09:12 data-lineage\n",
      "-rw-r--r-- 1 jovyan users   5992 Jun 15 09:12 logo.svg\n",
      "drwxr-xr-x 6 jovyan users    192 Jun 15 09:12 LuaHooks\n",
      "-rw-r--r-- 1 jovyan users 216042 Jun 15 09:12 RBAC.png\n",
      "drwxr-xr-x 9 jovyan users    288 Jun 15 09:12 reprocess-data\n",
      "drwxr-xr-x 4 jovyan users    128 Jun 15 09:12 version-control-of-multi-buckets-pipelines\n",
      "\n",
      "./images/data-lineage:\n",
      "total 192\n",
      "-rw-r--r-- 1 jovyan users 194502 Jun 15 09:12 commitFlow.png\n",
      "\n",
      "./images/LuaHooks:\n",
      "total 760\n",
      "-rw-r--r-- 1 jovyan users 211777 Jun 15 09:12 ActionDetails.png\n",
      "-rw-r--r-- 1 jovyan users  84229 Jun 15 09:12 Actions.png\n",
      "-rw-r--r-- 1 jovyan users  84650 Jun 15 09:12 schemaValidationFlow.png\n",
      "-rw-r--r-- 1 jovyan users 390740 Jun 15 09:12 SchemaValidation.gif\n",
      "\n",
      "./images/reprocess-data:\n",
      "total 416\n",
      "-rw-r--r-- 1 jovyan users 93074 Jun 15 09:12 Reprocess.png\n",
      "-rw-r--r-- 1 jovyan users 32230 Jun 15 09:12 Step1.png\n",
      "-rw-r--r-- 1 jovyan users 36729 Jun 15 09:12 Step2.png\n",
      "-rw-r--r-- 1 jovyan users 49246 Jun 15 09:12 Step3.png\n",
      "-rw-r--r-- 1 jovyan users 53734 Jun 15 09:12 Step4.png\n",
      "-rw-r--r-- 1 jovyan users 71470 Jun 15 09:12 Step5.png\n",
      "-rw-r--r-- 1 jovyan users 74414 Jun 15 09:12 Step6.png\n",
      "\n",
      "./images/version-control-of-multi-buckets-pipelines:\n",
      "total 280\n",
      "drwxr-xr-x 2 jovyan users     64 Jun  2 10:44 MultiBucketsPipelines\n",
      "-rw-r--r-- 1 jovyan users 285953 Jun 15 09:12 MultiBucketsPipelines.png\n",
      "\n",
      "./images/version-control-of-multi-buckets-pipelines/MultiBucketsPipelines:\n",
      "total 0\n",
      "\n",
      "./notebooks:\n",
      "total 0\n",
      "\n",
      "./papermill:\n",
      "total 3700\n",
      "-rw-r--r-- 1 jovyan users    9698 Jun 16 16:03 00_index.ipynb\n",
      "-rw-r--r-- 1 jovyan users   56303 Jun 16 16:03 data-lineage.ipynb\n",
      "-rw-r--r-- 1 jovyan users   44765 Jun 16 16:03 delta-diff.ipynb\n",
      "-rw-r--r-- 1 jovyan users   98458 Jun 16 16:02 delta-lake.ipynb\n",
      "-rw-r--r-- 1 jovyan users   44976 Jun 16 16:04 dev-test.ipynb\n",
      "-rw-r--r-- 1 jovyan users   17180 Jun 16 16:03 import-multiple-buckets.ipynb\n",
      "-rw-r--r-- 1 jovyan users   70897 Jun 16 16:02 lakefs-iceberg.ipynb\n",
      "-rw-r--r-- 1 jovyan users 3263512 Jun 16 16:05 ml-experimentation-wine-quality-prediction.ipynb\n",
      "-rw-r--r-- 1 jovyan users   39472 Jun 16 16:02 reprocess-backfill-data.ipynb\n",
      "-rw-r--r-- 1 jovyan users   75710 Jun 16 16:02 spark-demo.ipynb\n",
      "-rw-r--r-- 1 jovyan users   47056 Jun 16 16:03 version-control-of-multi-buckets-pipelines.ipynb\n",
      "\n",
      "./papermill-out:\n",
      "total 1268\n",
      "-rw-r--r-- 1 jovyan users   9702 Jun 19 11:29 00_index.ipynb\n",
      "-rw-r--r-- 1 jovyan users  62914 Jun 19 11:29 data-lineage.ipynb\n",
      "-rw-r--r-- 1 jovyan users  58846 Jun 19 11:29 delta-diff.ipynb\n",
      "-rw-r--r-- 1 jovyan users  81753 Jun 19 11:28 delta-lake.ipynb\n",
      "-rw-r--r-- 1 jovyan users  26652 Jun 19 11:29 dev-test.ipynb\n",
      "drwxr-xr-x 5 jovyan users    160 Jun 19 10:54 failed\n",
      "-rw-r--r-- 1 jovyan users  17093 Jun 19 11:29 import-multiple-buckets.ipynb\n",
      "-rw-r--r-- 1 jovyan users  70812 Jun 19 11:29 lakefs-iceberg.ipynb\n",
      "-rw-r--r-- 1 jovyan users  92450 Jun 19 11:29 ml-experimentation-wine-quality-prediction.ipynb\n",
      "-rw-r--r-- 1 jovyan users  39381 Jun 19 11:29 reprocess-backfill-data.ipynb\n",
      "-rw-r--r-- 1 jovyan users  51091 Jun 19 11:28 spark-demo.ipynb\n",
      "-rw-r--r-- 1 jovyan users 696832 Jun 19 11:29 stderr\n",
      "-rw-r--r-- 1 jovyan users  61548 Jun 19 11:29 version-control-of-multi-buckets-pipelines.ipynb\n",
      "\n",
      "./papermill-out/failed:\n",
      "total 140\n",
      "-rw-r--r-- 1 jovyan users 26653 Jun 19 10:44 dev-test.ipynb\n",
      "-rw-r--r-- 1 jovyan users 70901 Jun 19 10:43 lakefs-iceberg.ipynb\n",
      "-rw-r--r-- 1 jovyan users 39477 Jun 19 10:43 reprocess-backfill-data.ipynb\n",
      "\n",
      "./__pycache__:\n",
      "total 8\n",
      "-rw-r--r-- 1 jovyan users 1147 Jun  2 15:30 hello-dagster.cpython-310.pyc\n",
      "-rw-r--r-- 1 jovyan users 1818 Jun  2 15:32 hello-dagster.cpython-311.pyc\n",
      "\n",
      "./reprocess-data:\n",
      "total 16\n",
      "-rw-r--r-- 1 jovyan users 5934 Jun 15 09:14 etl.ipynb\n",
      "-rw-r--r-- 1 jovyan users 5430 Jun 15 09:14 reprocessing.ipynb\n",
      "\n",
      "./spark-warehouse:\n",
      "total 0\n",
      "drwxr-xr-x 16 jovyan users 512 May 31 13:25 books\n",
      "drwxr-xr-x 16 jovyan users 512 May 31 13:26 genre\n",
      "drwxr-xr-x  2 jovyan users  64 Jun 16 14:14 nyc.db\n",
      "\n",
      "./spark-warehouse/books:\n",
      "total 24\n",
      "-rw-r--r-- 1 jovyan users 1295 May 31 13:25 part-00000-08719555-2c67-42cd-a7cc-7f97c5c1e627-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1253 May 31 13:25 part-00001-08719555-2c67-42cd-a7cc-7f97c5c1e627-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1250 May 31 13:25 part-00002-08719555-2c67-42cd-a7cc-7f97c5c1e627-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1239 May 31 13:25 part-00003-08719555-2c67-42cd-a7cc-7f97c5c1e627-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1135 May 31 13:25 part-00004-08719555-2c67-42cd-a7cc-7f97c5c1e627-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1117 May 31 13:25 part-00005-08719555-2c67-42cd-a7cc-7f97c5c1e627-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users    0 May 31 13:25 _SUCCESS\n",
      "\n",
      "./spark-warehouse/genre:\n",
      "total 24\n",
      "-rw-r--r-- 1 jovyan users 1241 May 31 13:26 part-00000-d8f69477-b87a-4267-a9fd-580a2c283e85-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1210 May 31 13:26 part-00001-d8f69477-b87a-4267-a9fd-580a2c283e85-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1188 May 31 13:26 part-00002-d8f69477-b87a-4267-a9fd-580a2c283e85-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1135 May 31 13:26 part-00003-d8f69477-b87a-4267-a9fd-580a2c283e85-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1104 May 31 13:26 part-00004-d8f69477-b87a-4267-a9fd-580a2c283e85-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users 1060 May 31 13:26 part-00005-d8f69477-b87a-4267-a9fd-580a2c283e85-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users    0 May 31 13:26 _SUCCESS\n",
      "\n",
      "./spark-warehouse/nyc.db:\n",
      "total 0\n",
      "\n",
      "./utils_ml_reproducibility:\n",
      "total 8\n",
      "-rw-r--r-- 1 jovyan users 5927 Jun 15 09:14 file_utils.ipynb\n",
      "\n",
      "./write-audit-publish:\n",
      "total 136\n",
      "drwxr-xr-x 3 jovyan users    96 Jun 16 14:17 '$PWD'\n",
      "-rw-r--r-- 1 jovyan users   308 Jun 15 09:12  docker-compose-nessie.yml\n",
      "-rw-r--r-- 1 jovyan users  1201 Jun 15 09:12  README.md\n",
      "drwxr-xr-x 2 jovyan users    64 Jun  2 14:50  spark-warehouse\n",
      "drwxr-xr-x 3 jovyan users    96 Jun 15 09:12  spark_warehouse\n",
      "-rw-r--r-- 1 jovyan users 15969 Jun 19 10:18  wap-delta.ipynb\n",
      "-rw-r--r-- 1 jovyan users 21756 Jun 19 10:18  wap-hudi.ipynb\n",
      "-rw-r--r-- 1 jovyan users 19582 Jun 19 10:18  wap-iceberg.ipynb\n",
      "-rw-r--r-- 1 jovyan users 50292 Jun 15 09:14  wap-lakefs.ipynb\n",
      "-rw-r--r-- 1 jovyan users 13245 Jun 15 09:14  wap-nessie.ipynb\n",
      "\n",
      "'./write-audit-publish/$PWD':\n",
      "total 0\n",
      "drwxr-xr-x 3 jovyan users 96 Jun 16 14:17 warehouse\n",
      "\n",
      "'./write-audit-publish/$PWD/warehouse':\n",
      "total 0\n",
      "drwxr-xr-x 2 jovyan users 64 Jun 16 14:17 nyc\n",
      "\n",
      "'./write-audit-publish/$PWD/warehouse/nyc':\n",
      "total 0\n",
      "\n",
      "./write-audit-publish/spark-warehouse:\n",
      "total 0\n",
      "\n",
      "./write-audit-publish/spark_warehouse:\n",
      "total 0\n",
      "drwxr-xr-x 3 jovyan users 96 Jun 15 09:12 iceberg\n",
      "\n",
      "./write-audit-publish/spark_warehouse/iceberg:\n",
      "total 0\n",
      "drwxr-xr-x 4 jovyan users 128 Jun 15 09:12 permits_1fe3136b-05e0-4264-af6c-7433a0289440\n",
      "\n",
      "./write-audit-publish/spark_warehouse/iceberg/permits_1fe3136b-05e0-4264-af6c-7433a0289440:\n",
      "total 0\n",
      "drwxr-xr-x  6 jovyan users 192 Jun 15 09:12 data\n",
      "drwxr-xr-x 16 jovyan users 512 Jun 15 09:12 metadata\n",
      "\n",
      "./write-audit-publish/spark_warehouse/iceberg/permits_1fe3136b-05e0-4264-af6c-7433a0289440/data:\n",
      "total 84\n",
      "-rw-r--r-- 1 jovyan users 51115 Jun 15 09:12 00000-3-87318a49-aa77-4580-9c87-f007650a5749-00001.parquet\n",
      "-rw-r--r-- 1 jovyan users 28906 Jun 15 09:12 00122-10-04c1ad16-bac8-403b-83fa-a54b38b309ff-00001.parquet\n",
      "\n",
      "./write-audit-publish/spark_warehouse/iceberg/permits_1fe3136b-05e0-4264-af6c-7433a0289440/metadata:\n",
      "total 48\n",
      "-rw-r--r-- 1 jovyan users 4803 Jun 15 09:12 00000-16fefe49-786e-4924-a5fa-2e5e2ec284d7.metadata.json\n",
      "-rw-r--r-- 1 jovyan users 6169 Jun 15 09:12 00001-366d44c1-0c70-475a-b1bb-cf3cf5373e5e.metadata.json\n",
      "-rw-r--r-- 1 jovyan users 6972 Jun 15 09:12 00d6c9d7-1f7a-4834-92f8-bf5e455da6a1-m0.avro\n",
      "-rw-r--r-- 1 jovyan users 6971 Jun 15 09:12 00d6c9d7-1f7a-4834-92f8-bf5e455da6a1-m1.avro\n",
      "-rw-r--r-- 1 jovyan users 6977 Jun 15 09:12 0ef1854d-d1ea-43b3-8ce5-5c78f2e07c82-m0.avro\n",
      "-rw-r--r-- 1 jovyan users 3857 Jun 15 09:12 snap-171805288804968515-1-00d6c9d7-1f7a-4834-92f8-bf5e455da6a1.avro\n",
      "-rw-r--r-- 1 jovyan users 3820 Jun 15 09:12 snap-5323679328118569268-1-0ef1854d-d1ea-43b3-8ce5-5c78f2e07c82.avro\n"
     ]
    }
   ],
   "source": [
    "!ls -lR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7db274-07b5-49f9-b7de-5b912818317c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü© Created ingestion branch: ingest_2023-06-19_13-41-55\n",
      "üü© Ingested data file: lakefs_test.csv\n",
      "\n",
      "üü© Reading data from ingestion branch\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "\n",
      "üü© Processed data with wrong value for Average field. Average value is Total divided 4 instead of dividing by 5\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                              3.75|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n",
      "üü© Appended processed data to ingestion branch\n",
      "\n",
      "üü© Committed ingestion branch and added Git URL for the ETL logic as a metadata\n",
      "Example Git URL: https://github.com/treeverse/lakeFS-samples/blob/main/03-apache-spark-python-demo/Airflow/etl_task1.py\n",
      "\n",
      "üü© Merged ingestion branch to main branch\n",
      "\n",
      "üü© üü© ETL job finished üü© üü© \n"
     ]
    }
   ],
   "source": [
    "%run reprocess-data/etl.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca865d-0aa0-46bb-b6f2-54d7205dda64",
   "metadata": {},
   "source": [
    "# Reprocessing Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace14847-e55c-4b53-8648-0ae88eaebbad",
   "metadata": {},
   "source": [
    "## Step 3: Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "### (you can change the name for reprocessing branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8029d9f-2f74-4cb1-950f-616ef69c063d",
   "metadata": {},
   "source": [
    "![Step 3](./images/reprocess-data/Step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732966a0-f4e1-4a81-98ae-89b9c0799b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reprocessBranch = \"new-logic\"\n",
    "%run reprocess-data/reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc3b3d-3e5e-4be7-82b4-10124d85613e",
   "metadata": {},
   "source": [
    "## While ETL logic is getting fixed, old ETL job is still running in parallel.\n",
    "\n",
    "## Received new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25504879-37e8-46b2-8c3f-88596fe0240b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_new.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbbaaa-51cf-4986-be84-d6a22997e706",
   "metadata": {},
   "source": [
    "## Step 4: Repetition of step # 2\n",
    "\n",
    "### (run [ETL](./ReprocessData/ETL.ipynb) job again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d26f30-afc7-4a10-a3b9-6486a48dff7a",
   "metadata": {},
   "source": [
    "![Step 4](./images/reprocess-data/Step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25463f0e-e09e-4c46-9baf-97c2a2e4a4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./reprocess-data/etl.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fcd72-417c-4079-a051-6ee8fe6662ac",
   "metadata": {},
   "source": [
    "## Now Reprocessing branch is behind Main branch in terms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e289db8-a33d-4c70-933f-e24282fd0788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + reprocessBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{reprocessBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0462b6-c8a1-4212-bb1b-54ffbfda4c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d894e-d6bb-4a2d-8132-c405b8defabb",
   "metadata": {},
   "source": [
    "## Once ETL logic is fixed, pause the old ETL job to deploy new ETL logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e5cbd-0a0a-48ff-827e-68dc0dd4b712",
   "metadata": {},
   "source": [
    "## Step 5: Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes\n",
    "### (you can change the name for the \"Backfill and Deploy\" branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job again on \"Backfill and Deploy\" branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74206d1e-9ec8-4209-a940-49f7b2d333f6",
   "metadata": {},
   "source": [
    "![Step 5](./images/reprocess-data/Step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216328cf-29b7-4aa1-bfce-0bddbed58590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backfillAndDeployBranch = \"backfill-and-deploy\"\n",
    "reprocessBranch = backfillAndDeployBranch\n",
    "%run ./reprocess-data/reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19852e24-d198-4d0f-98fe-d756baf586e2",
   "metadata": {},
   "source": [
    "## Now \"Backfill and Deploy\" branch has same data as Main branch and correct ETL logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573cfb2-049c-4617-ba99-b169c84cfb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + backfillAndDeployBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{backfillAndDeployBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2fc43-9056-414c-9516-d1eb6da77631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113388d-94b3-4bf9-807f-0ea08360089e",
   "metadata": {},
   "source": [
    "## Step 6: Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df9f1d-c21a-4ab3-b7f0-f2aa5bb7fd4b",
   "metadata": {},
   "source": [
    "![Step 6](./images/reprocess-data/Step6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40762c9-0344-4cbc-b28a-12673f30a670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id, source_ref=backfillAndDeployBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762d1a0-276f-4a20-8ca0-18f4a463a622",
   "metadata": {},
   "source": [
    "# Reprocessing and Backfill completes\n",
    "\n",
    "## Verify data on Main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f67d-98ad-45f8-b52d-a4a8a9860443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583e2de-b113-4e31-9bab-1de5feb16f9d",
   "metadata": {},
   "source": [
    "## Now you can schedule the new ETL job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b24a6f-56e6-4bd3-b898-0ff0c188e223",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
