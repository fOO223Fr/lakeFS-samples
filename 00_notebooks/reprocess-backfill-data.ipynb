{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# Reprocess and Backfill Data with new ETL logic\n",
    "\n",
    "_Note that whilst this works, it's a bit of a hack!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67662314-7dcd-4e6c-869f-24a1a76d1a0e",
   "metadata": {},
   "source": [
    "You will run following steps in this notebook (refer to the image below):\n",
    "\n",
    "1. Create repository with the Main branch\n",
    "2. Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "3. Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "4. Repetition of step # 2\n",
    "5. Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes.\n",
    "6. Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a0676-60ff-47e8-af5b-fa44206dcb3d",
   "metadata": {},
   "source": [
    "![Reprocess](./images/reprocess-data/Reprocess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6f67a-d18c-43f1-b4ee-8ce2e3038857",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1744d-4095-4497-a443-c2cb76d0ecee",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4410b2f7-bbd1-48f2-88cd-43440e4cd8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFOLKFSSAMPLES'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6b274-8d00-4fbf-830b-339fd29e0ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb1c20a-e248-4596-a99b-a7a3784cf9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e6a97-c4c3-42b4-8881-568878f54d04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22a8f6-ae2d-449c-b5ae-6b0abc720210",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45fbab3-5f98-46ad-8e12-9021ed2bf81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"reprocess-backfill-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a658-9fa0-470c-b829-45d0e7dd1164",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4795b1e7-761e-4f7a-840b-982f99ff3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9217c-af15-48a1-ba77-c497d93b9cda",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bbe675-3c5e-41c7-8471-7a8a90253827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying lakeFS credentials‚Ä¶\n",
      "‚Ä¶‚úÖlakeFS credentials verified\n",
      "\n",
      "‚ÑπÔ∏èlakeFS version0.104.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_config()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version {v['version_config']['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1441793-7174-4f6f-b400-7eca020ad561",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7196c9-370a-441a-8ea3-02ee03484991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eba7a9ebda2f:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f72004efeb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea20cc8-9a7b-4285-a675-7373a0fb65cc",
   "metadata": {},
   "source": [
    "### Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b839850-5954-4631-8e7e-d0dee6d17dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestBranch = \"ingest\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "processedFileName = \"lakefs_test_processed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db425f40-db8d-4298-8cdb-c532fc98fc97",
   "metadata": {},
   "source": [
    "### Define data file schema + python libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e42f399-19b7-4def-9988-916a7f8ba067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "dataFileSchema = StructType([\n",
    "  StructField(\"Apparel_Sales\", DoubleType(), False),\n",
    "  StructField(\"Books_Sales\", DoubleType(), False),\n",
    "  StructField(\"Electronics_Sales\", DoubleType(), False),\n",
    "  StructField(\"Furniture_Sales\", DoubleType(), False),\n",
    "  StructField(\"Toys_Sales\", DoubleType(), False)\n",
    "])\n",
    "processedDataFileSchema = StructType([\n",
    "  StructField(\"Apparel_Sales\", DoubleType(), False),\n",
    "  StructField(\"Books_Sales\", DoubleType(), False),\n",
    "  StructField(\"Electronics_Sales\", DoubleType(), False),\n",
    "  StructField(\"Furniture_Sales\", DoubleType(), False),\n",
    "  StructField(\"Toys_Sales\", DoubleType(), False),\n",
    "  StructField(\"Total_Sales\", DoubleType(), False),\n",
    "  StructField(\"Average_Sales_per_Product_Category\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf16b13-7300-4840-a3a9-91765e945b3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431be3f-d741-4cb6-98cf-8b21b8f2a489",
   "metadata": {},
   "source": [
    "## Step 1: Create repository with the Main branch\n",
    "\n",
    "### (if above mentioned repo already exists on your lakeFS server then you can skip this operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2218d-3af0-415d-a35a-ab4ba06ac040",
   "metadata": {},
   "source": [
    "![Step 1](./images/reprocess-data/Step1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384aa4d3-038a-4a56-b30e-c536e3911478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository reprocess-backfill-data does not exist, so going to try and create it now.\n",
      "Created new repo reprocess-backfill-data using storage namespace s3://example/reprocess-backfill-data\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360b5f6-76db-438b-9aa0-e40070b86bb9",
   "metadata": {},
   "source": [
    "## Step 2: Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "\n",
    "### ([ETL](./ReprocessData/ETL.ipynb) job normally run as a batch job but run ETL job manually here for the demo. It will take around a minute to run this step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2b2de-617e-4485-8b41-e471feae8a93",
   "metadata": {},
   "source": [
    "![Step 2](./images/reprocess-data/Step2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c7db274-07b5-49f9-b7de-5b912818317c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü© Created ingestion branch: ingest_2023-07-17_07-55-23\n",
      "üü© Ingested data file: lakefs_test.csv\n",
      "\n",
      "üü© Reading data from ingestion branch\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "\n",
      "üü© Processed data with wrong value for Average field. Average value is Total divided 4 instead of dividing by 5\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                              3.75|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n",
      "üü© Appended processed data to ingestion branch\n",
      "\n",
      "üü© Committed ingestion branch and added Git URL for the ETL logic as a metadata\n",
      "Example Git URL: https://github.com/treeverse/lakeFS-samples/blob/main/03-apache-spark-python-demo/Airflow/etl_task1.py\n",
      "\n",
      "üü© Merged ingestion branch to main branch\n",
      "\n",
      "üü© üü© ETL job finished üü© üü© \n"
     ]
    }
   ],
   "source": [
    "%run ./reprocess-data/etl.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca865d-0aa0-46bb-b6f2-54d7205dda64",
   "metadata": {},
   "source": [
    "# Reprocessing Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace14847-e55c-4b53-8648-0ae88eaebbad",
   "metadata": {},
   "source": [
    "## Step 3: Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "### (you can change the name for reprocessing branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8029d9f-2f74-4cb1-950f-616ef69c063d",
   "metadata": {},
   "source": [
    "![Step 3](./images/reprocess-data/Step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732966a0-f4e1-4a81-98ae-89b9c0799b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü© Created new-logic branch from main branch\n",
      "\n",
      "üü© Reading data from new-logic branch\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "\n",
      "üü© Processed data with correct value for Average field\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                               3.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n",
      "üü© Overwrote processed data to new-logic branch\n",
      "\n",
      "üü© Committed new-logic branch and added Git URL for the new ETL logic as a metadata\n",
      "Example Git URL: https://github.com/treeverse/lakeFS-samples/blob/main/03-apache-spark-python-demo/Airflow/etl_task2_1.py\n",
      "\n",
      "üü© Diff between new-logic branch and the main branch\n",
      "Path                                                                                Path Type      Size(Bytes)  Type\n",
      "----------------------------------------------------------------------------------  -----------  -------------  -------\n",
      "lakefs_test_processed.csv/part-00000-5cc4af36-c076-44be-99d4-548518f98215-c000.csv  object                  29  removed\n",
      "lakefs_test_processed.csv/part-00000-8282e3e5-c781-4402-ba67-adb94732c31f-c000.csv  object                  29  added\n",
      "\n",
      "üü© üü© Process finished üü© üü©\n"
     ]
    }
   ],
   "source": [
    "reprocessBranch = \"new-logic\"\n",
    "%run ./reprocess-data/reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc3b3d-3e5e-4be7-82b4-10124d85613e",
   "metadata": {},
   "source": [
    "## While ETL logic is getting fixed, old ETL job is still running in parallel.\n",
    "\n",
    "## Received new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25504879-37e8-46b2-8c3f-88596fe0240b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_new.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbbaaa-51cf-4986-be84-d6a22997e706",
   "metadata": {},
   "source": [
    "## Step 4: Repetition of step # 2\n",
    "\n",
    "### (run [ETL](./ReprocessData/ETL.ipynb) job again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d26f30-afc7-4a10-a3b9-6486a48dff7a",
   "metadata": {},
   "source": [
    "![Step 4](./images/reprocess-data/Step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25463f0e-e09e-4c46-9baf-97c2a2e4a4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü© Created ingestion branch: ingest_2023-07-17_07-55-41\n",
      "üü© Ingested data file: lakefs_test_new.csv\n",
      "\n",
      "üü© Reading data from ingestion branch\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "\n",
      "üü© Processed data with wrong value for Average field. Average value is Total divided 4 instead of dividing by 5\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|      150.0|                              37.5|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n",
      "üü© Appended processed data to ingestion branch\n",
      "\n",
      "üü© Committed ingestion branch and added Git URL for the ETL logic as a metadata\n",
      "Example Git URL: https://github.com/treeverse/lakeFS-samples/blob/main/03-apache-spark-python-demo/Airflow/etl_task1.py\n",
      "\n",
      "üü© Merged ingestion branch to main branch\n",
      "\n",
      "üü© üü© ETL job finished üü© üü© \n"
     ]
    }
   ],
   "source": [
    "%run ./reprocess-data/etl.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fcd72-417c-4079-a051-6ee8fe6662ac",
   "metadata": {},
   "source": [
    "## Now Reprocessing branch is behind Main branch in terms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e289db8-a33d-4c70-933f-e24282fd0788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data on new-logic branch\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                               3.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed data on \" + reprocessBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{reprocessBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0462b6-c8a1-4212-bb1b-54ffbfda4c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data on main branch\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|      150.0|                              37.5|\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                              3.75|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d894e-d6bb-4a2d-8132-c405b8defabb",
   "metadata": {},
   "source": [
    "## Once ETL logic is fixed, pause the old ETL job to deploy new ETL logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e5cbd-0a0a-48ff-827e-68dc0dd4b712",
   "metadata": {},
   "source": [
    "## Step 5: Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes\n",
    "### (you can change the name for the \"Backfill and Deploy\" branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job again on \"Backfill and Deploy\" branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74206d1e-9ec8-4209-a940-49f7b2d333f6",
   "metadata": {},
   "source": [
    "![Step 5](./images/reprocess-data/Step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216328cf-29b7-4aa1-bfce-0bddbed58590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü© Created backfill-and-deploy branch from main branch\n",
      "\n",
      "üü© Reading data from backfill-and-deploy branch\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+\n",
      "\n",
      "üü© Processed data with correct value for Average field\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|      150.0|                              30.0|\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                               3.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n",
      "üü© Overwrote processed data to backfill-and-deploy branch\n",
      "\n",
      "üü© Committed backfill-and-deploy branch and added Git URL for the new ETL logic as a metadata\n",
      "Example Git URL: https://github.com/treeverse/lakeFS-samples/blob/main/03-apache-spark-python-demo/Airflow/etl_task2_1.py\n",
      "\n",
      "üü© Diff between backfill-and-deploy branch and the main branch\n",
      "Path                                                                                Path Type      Size(Bytes)  Type\n",
      "----------------------------------------------------------------------------------  -----------  -------------  -------\n",
      "lakefs_test_processed.csv/part-00000-0970b498-cfaa-4219-a337-c2036a0f8900-c000.csv  object                  29  removed\n",
      "lakefs_test_processed.csv/part-00000-5cc4af36-c076-44be-99d4-548518f98215-c000.csv  object                  29  removed\n",
      "lakefs_test_processed.csv/part-00000-ee339c99-b3d1-4db9-bec9-4c1f2e90b24f-c000.csv  object                  29  added\n",
      "lakefs_test_processed.csv/part-00001-ee339c99-b3d1-4db9-bec9-4c1f2e90b24f-c000.csv  object                  29  added\n",
      "\n",
      "üü© üü© Process finished üü© üü©\n"
     ]
    }
   ],
   "source": [
    "backfillAndDeployBranch = \"backfill-and-deploy\"\n",
    "reprocessBranch = backfillAndDeployBranch\n",
    "%run ./reprocess-data/reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19852e24-d198-4d0f-98fe-d756baf586e2",
   "metadata": {},
   "source": [
    "## Now \"Backfill and Deploy\" branch has same data as Main branch and correct ETL logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9573cfb2-049c-4617-ba99-b169c84cfb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data on backfill-and-deploy branch\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|      150.0|                              30.0|\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                               3.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed data on \" + backfillAndDeployBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{backfillAndDeployBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d2fc43-9056-414c-9516-d1eb6da77631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data on main branch\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|      150.0|                              37.5|\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                              3.75|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113388d-94b3-4bf9-807f-0ea08360089e",
   "metadata": {},
   "source": [
    "## Step 6: Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df9f1d-c21a-4ab3-b7f0-f2aa5bb7fd4b",
   "metadata": {},
   "source": [
    "![Step 6](./images/reprocess-data/Step6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a40762c9-0344-4cbc-b28a-12673f30a670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '1681878bdebe8e4b665ec5e7ae837ebac1e441336b7410eabf3732cc0b0b2a82'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.refs.merge_into_branch(\n",
    "    repository=repo.id, source_ref=backfillAndDeployBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762d1a0-276f-4a20-8ca0-18f4a463a622",
   "metadata": {},
   "source": [
    "# Reprocessing and Backfill completes\n",
    "\n",
    "## Verify data on Main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aa3f67d-98ad-45f8-b52d-a4a8a9860443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data on main branch\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|Apparel_Sales|Books_Sales|Electronics_Sales|Furniture_Sales|Toys_Sales|Total_Sales|Average_Sales_per_Product_Category|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "|         10.0|       20.0|             30.0|           40.0|      50.0|      150.0|                              30.0|\n",
      "|          1.0|        2.0|              3.0|            4.0|       5.0|       15.0|                               3.0|\n",
      "+-------------+-----------+-----------------+---------------+----------+-----------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo.id}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583e2de-b113-4e31-9bab-1de5feb16f9d",
   "metadata": {},
   "source": [
    "## Now you can schedule the new ETL job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b24a6f-56e6-4bd3-b898-0ff0c188e223",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
