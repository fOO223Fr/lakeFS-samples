{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0d51de",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f557468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lakefs_client in /opt/conda/lib/python3.9/site-packages (0.89.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /opt/conda/lib/python3.9/site-packages (from lakefs_client) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil->lakefs_client) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lakefs_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dc5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.26.48)\r\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.48 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.29.48)\r\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.6.0)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.48->boto3) (1.26.8)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.48->boto3) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.48->boto3) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7267f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.9/site-packages (4.7.0.68)\r\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from opencv-python) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19aadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.19.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.0.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (60.5.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (23.1.4)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df39dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbimporter in /opt/conda/lib/python3.9/site-packages (0.3.4)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e14965",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import joblib\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import nbimporter\n",
    "import pprint\n",
    "\n",
    "from datetime import date, time, datetime\n",
    "from utils.ml_utils import *\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "print(\"Loaded all libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d40ca",
   "metadata": {},
   "source": [
    "### Configuring Boto3, lakeFSClient and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring boto3 client\n",
    "s3_client = boto3.client('s3',\n",
    "    endpoint_url='http://host.docker.internal:8000',\n",
    "    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n",
    "    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')\n",
    "\n",
    "s3_resource = boto3.resource('s3',\n",
    "    endpoint_url='http://host.docker.internal:8000',\n",
    "    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n",
    "    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AccessKey and SecretKey are present in the docker-compose.yaml file we used to spin up the everything bagel\n",
    "lakefsAccessKey = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "lakefsSecretKey = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "lakefsEndPoint = \"http://lakefs:8000\"\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc22c0",
   "metadata": {},
   "source": [
    "### Configuring S3A Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8125e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d25570",
   "metadata": {},
   "source": [
    "## Demo Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce745d9e",
   "metadata": {},
   "source": [
    "1. Show the dataset in MinIO\n",
    "2. Create a new MinIO bucket for lakeFS repository\n",
    "3. Create a lakeFS repository (ml-demo)\n",
    "4. Import dataset into the lakeFS repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb42ed",
   "metadata": {},
   "source": [
    "## Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997200c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"ml-demo\"\n",
    "\n",
    "ingest_branch = \"_main_imported\"\n",
    "exp1_branch = \"experiment-1\"\n",
    "exp2_branch = \"experiment-2\"\n",
    "\n",
    "prod_branch = \"main\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{repo_name}\"\n",
    "\n",
    "images_path = \"dogs_dataset_/images/Images\"\n",
    "annotations = \"dogs_dataset_/annotations/Annotations\"\n",
    "\n",
    "raw_path = \"raw\"\n",
    "processed_path = \"processed\"\n",
    "artifact_path = \"artifacts\"\n",
    "metrics_path = \"metrics\"\n",
    "training_code_path = \"src\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d4979",
   "metadata": {},
   "source": [
    "## File utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_s3(bucket, key):\n",
    "    \n",
    "    bucket = s3_resource.Bucket(bucket)\n",
    "    file_stream = BytesIO()\n",
    "    bucket.Object(key).download_fileobj(file_stream)\n",
    "    np_1d_array = np.frombuffer(file_stream.getbuffer(), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(np_1d_array, cv2.IMREAD_COLOR).copy()\n",
    "    \n",
    "    return resize_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_list_from_s3(bucket, key, delimiter, n_cats):\n",
    "    \n",
    "    list_resp = s3_client.list_objects_v2(Bucket=bucket, \n",
    "                                          Prefix=key+\"/\",\n",
    "                                         Delimiter=delimiter)\n",
    "    print(\"List_resp\", list_resp)\n",
    "    \n",
    "    category_list = [ x['Prefix'] for x in list_resp['CommonPrefixes'][:n_cats]]\n",
    "    print(category_list)\n",
    "    \n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e41657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_labels(bucket, category_list, n_images):\n",
    "\n",
    "    img_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for index, category in enumerate(category_list):\n",
    "        # breed = category.split(\"/\")[-2]\n",
    "        list_resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=category)\n",
    "\n",
    "        for c in list_resp['Contents'][:n_images]:\n",
    "            key = c['Key']\n",
    "            img = get_img_from_s3(bucket, key)\n",
    "            label = index\n",
    "            \n",
    "            img_list.append(img)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    images = np.array(img_list)\n",
    "    labels = np.array(labels_list)\n",
    "\n",
    "    # print(\"Images shape = \",images.shape,\"\\nLabels shape = \",labels.shape)\n",
    "    # print(type(images),type(labels))\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51beb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(params):\n",
    "    \n",
    "    category_list = get_category_list_from_s3(bucket=params['repo_name'],\n",
    "                                         key=params['image_path'],\n",
    "                                         delimiter=params['delimiter'],\n",
    "                                         n_cats=params['n_cats']\n",
    "                                         )\n",
    "    \n",
    "\n",
    "    images, labels = get_images_and_labels(bucket=repo_name,\n",
    "                                             category_list=category_list,\n",
    "                                             n_images=params['n_images'])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model_name, bucket_name, key):\n",
    "    \n",
    "    key = f\"{key}/{model_name}\"\n",
    "    \n",
    "    # READ\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "        fp.seek(0)\n",
    "        model = joblib.load(fp)\n",
    "\n",
    "    # DELETE\n",
    "    # s3_client.delete_object(Bucket=bucket_name, Key=key)\n",
    "    \n",
    "    print(type(model))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8878687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, model_name, bucket_name, key):\n",
    "    \n",
    "    joblib.dump(model, model_name)\n",
    "    \n",
    "    key = f\"{key}/{model_name}\"\n",
    "    print(model_name, bucket_name, key)\n",
    "\n",
    "    # WRITE\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(model, fp)\n",
    "        fp.seek(0)\n",
    "        s3_client.put_object(Body=fp.read(), Bucket=bucket_name, Key=key)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics, bucket_name, key):\n",
    "    \n",
    "    data = [(str(metrics['loss']), str(metrics['accuracy']))]\n",
    "\n",
    "    schema = StructType([ \\\n",
    "        StructField(\"loss\",StringType(),True), \\\n",
    "        StructField(\"accuracy\",StringType(),True) \\\n",
    "      ])\n",
    " \n",
    "    df = spark.createDataFrame(data=data,schema=schema)\n",
    "    df.printSchema()\n",
    "    df.show(truncate=False)\n",
    "    \n",
    "    path = f\"s3a://{bucket_name}/{key}\"\n",
    "    df.write.json(path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(bucket_name, key):\n",
    "    \n",
    "    path = f\"s3a://{bucket_name}/{key}\"\n",
    "    \n",
    "    df = spark.read.json(path)\n",
    "    metrics = df.collect()[0]\n",
    "    loss = metrics['loss']\n",
    "    accuracy = metrics['accuracy']\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c7a99",
   "metadata": {},
   "source": [
    "# Experimentation Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70646f0a",
   "metadata": {},
   "source": [
    "## Experiment #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933890",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp1 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp1_branch,\n",
    "    'image_path': f\"{exp1_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp1_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp1_branch}/{metrics_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 100,\n",
    "    'is_shuffle':True,\n",
    "    'is_normalize': False,\n",
    "    'epochs': 200,\n",
    "    'train_test_split_ratio': 0.2,\n",
    "    'optimizer': \"adam\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d34b62",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #1\n",
    "\n",
    "#### Create a new branch: `experiment-1` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=exp1_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "client.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a1af5",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)\n",
    "print(\"Loading training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Commit the training data after preprocessing under /processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad3fb7",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, metrics1 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276823",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(metrics1, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af70a8",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model1, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9faa3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "print(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a2c2f",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4543ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model1_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ccd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3aed7",
   "metadata": {},
   "source": [
    "## Experiment #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6211d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp2 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp2_branch,\n",
    "    'image_path': f\"{exp2_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp2_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp2_branch}/{metrics_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf501c1e",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #2\n",
    "\n",
    "1. Create a new branch: `experiment-2` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ccb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.list_branches(repo_name)\n",
    "\n",
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=exp2_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "\n",
    "client.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f218ff1f",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31106d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Commit training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92c7ef",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e18b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2, metrics2 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194bd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_metrics(metrics2, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d5d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31f738",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model2, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b498572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81964b",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18faed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model2_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e786c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b3bb2",
   "metadata": {},
   "source": [
    "### Compare models in both branches. Merge the winning model to Prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ce3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch = exp2_branch\n",
    "if metrics1['accuracy']> metrics2['accuracy']:\n",
    "    win_branch = exp1_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e980de",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=win_branch, \n",
    "                              destination_branch=prod_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f93192",
   "metadata": {},
   "source": [
    "## Reproducing ML experiments with lakeFS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_branch = exp1_branch\n",
    "tag = f'{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}_{tag_branch}'\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9da60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.tags.create_tag(\n",
    "    repository=repo_name,\n",
    "    tag_creation=models.TagCreation(\n",
    "        id=tag, \n",
    "        ref=tag_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b58d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tag ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': tag,\n",
    "    'image_path': f\"{tag}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{tag}/{artifact_path}\",\n",
    "    'metrics_path': f\"{tag}/{metrics_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "pprint.pprint(params_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806dfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20135ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = preprocess(images, labels, params['is_shuffle'], params['is_normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785c193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_model_reloaded = model_load(params_tag['model_name'], \n",
    "           params_tag['repo_name'], \n",
    "           params_tag['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tag_model_reloaded.predict(images)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4921d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822be2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215ef31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c764caef",
   "metadata": {},
   "source": [
    "## DONE!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
