{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0d51de",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f557468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install lakefs_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7267f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42110941",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf181a5f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import joblib\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import nbimporter\n",
    "import pprint\n",
    "\n",
    "from datetime import date, time\n",
    "from utils.ml_utils import *\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "print(\"Loaded all libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d40ca",
   "metadata": {},
   "source": [
    "### Configuring Boto3, lakeFSClient and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring boto3 client\n",
    "s3_client = boto3.client('s3',\n",
    "    endpoint_url='http://host.docker.internal:8000',\n",
    "    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n",
    "    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')\n",
    "\n",
    "s3_resource = boto3.resource('s3',\n",
    "    endpoint_url='http://host.docker.internal:8000',\n",
    "    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',\n",
    "    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AccessKey and SecretKey are present in the docker-compose.yaml file we used to spin up the everything bagel\n",
    "lakefsAccessKey = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "lakefsSecretKey = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "lakefsEndPoint = \"http://lakefs:8000\"\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc22c0",
   "metadata": {},
   "source": [
    "### Configuring S3A Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8125e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bccc3",
   "metadata": {},
   "source": [
    "## Demo Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b02c4",
   "metadata": {},
   "source": [
    "1. Show the dataset in MinIO\n",
    "2. Create a new MinIO bucket for lakeFS repository\n",
    "3. Create a lakeFS repository (ml-demo)\n",
    "4. Import dataset into the lakeFS repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ef4f6",
   "metadata": {},
   "source": [
    "## Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997200c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"ml-demo\"\n",
    "\n",
    "ingest_branch = \"_main_imported\"\n",
    "exp1_branch = \"experiment-1\"\n",
    "exp2_branch = \"experiment-2\"\n",
    "\n",
    "prod_branch = \"main\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{repo_name}\"\n",
    "\n",
    "images_path = \"dogs_dataset_/images/Images\"\n",
    "annotations = \"dogs_dataset_/annotations/Annotations\"\n",
    "\n",
    "raw_path = \"raw\"\n",
    "processed_path = \"processed\"\n",
    "artifact_path = \"artifacts\"\n",
    "metrics_path = \"metrics\"\n",
    "training_code_path = \"src\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d4979",
   "metadata": {},
   "source": [
    "## File utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_s3(bucket, key):\n",
    "    \n",
    "    bucket = s3_resource.Bucket(bucket)\n",
    "    file_stream = BytesIO()\n",
    "    bucket.Object(key).download_fileobj(file_stream)\n",
    "    np_1d_array = np.frombuffer(file_stream.getbuffer(), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(np_1d_array, cv2.IMREAD_COLOR).copy()\n",
    "    \n",
    "    return resize_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_list_from_s3(bucket, branch,  key, delimiter, n_cats):\n",
    "    \n",
    "    list_resp = s3_client.list_objects_v2(Bucket=bucket, \n",
    "                                          Prefix=key, \n",
    "                                          Delimiter=delimiter)\n",
    "    #print(\"List_resp\", list_resp)\n",
    "    \n",
    "    category_list = [ x['Prefix'] for x in list_resp['CommonPrefixes'][:n_cats]]\n",
    "    #print(len(category_list))\n",
    "    \n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e41657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_labels(bucket, category_list, n_images):\n",
    "\n",
    "    img_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for index, category in enumerate(category_list):\n",
    "        # breed = category.split(\"/\")[-2]\n",
    "        list_resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=category)\n",
    "\n",
    "        for c in list_resp['Contents'][:n_images]:\n",
    "            key = c['Key']\n",
    "            img = get_img_from_s3(bucket, key)\n",
    "            label = index\n",
    "            \n",
    "            img_list.append(img)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    images = np.array(img_list)\n",
    "    labels = np.array(labels_list)\n",
    "\n",
    "    # print(\"Images shape = \",images.shape,\"\\nLabels shape = \",labels.shape)\n",
    "    # print(type(images),type(labels))\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51beb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(params):\n",
    "    \n",
    "    category_list = get_category_list_from_s3(bucket=params['repo_name'],\n",
    "                                         branch=params['branch'],\n",
    "                                         key=params['image_path'],\n",
    "                                         delimiter=params['delimiter'],\n",
    "                                         n_cats=params['n_cats']\n",
    "                                         )\n",
    "    \n",
    "    images, labels = get_images_and_labels(bucket=repo_name,\n",
    "                                             category_list=category_list,\n",
    "                                             n_images=params['n_images'])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model_name, bucket_name, key):\n",
    "    \n",
    "    key = f\"{key}/{model_name}\"\n",
    "    \n",
    "    # READ\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "        fp.seek(0)\n",
    "        model = joblib.load(fp)\n",
    "\n",
    "    # DELETE\n",
    "    # s3_client.delete_object(Bucket=bucket_name, Key=key)\n",
    "    \n",
    "    print(type(model))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8878687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, model_name, bucket_name, key):\n",
    "    \n",
    "    joblib.dump(model, model_name)\n",
    "    \n",
    "    key = f\"{key}/{model_name}\"\n",
    "    print(model_name, bucket_name, key)\n",
    "\n",
    "    # WRITE\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(model, fp)\n",
    "        fp.seek(0)\n",
    "        s3_client.put_object(Body=fp.read(), Bucket=bucket_name, Key=key)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e50c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics, bucket_name, key):\n",
    "    \n",
    "    data = [(str(metrics['loss']), str(metrics['accuracy']))]\n",
    "\n",
    "    schema = StructType([ \\\n",
    "        StructField(\"loss\",StringType(),True), \\\n",
    "        StructField(\"accuracy\",StringType(),True) \\\n",
    "      ])\n",
    " \n",
    "    df = spark.createDataFrame(data=data,schema=schema)\n",
    "    df.printSchema()\n",
    "    df.show(truncate=False)\n",
    "    \n",
    "    path = f\"s3a://{bucket_name}/{key}\"\n",
    "    df.write.json(path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ef23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(bucket_name, key):\n",
    "    \n",
    "    path = f\"s3a://{bucket_name}/{key}\"\n",
    "    \n",
    "    df = spark.read.json(path)\n",
    "    metrics = df.collect()[0]\n",
    "    loss = metrics['loss']\n",
    "    accuracy = metrics['accuracy']\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc003b3e",
   "metadata": {},
   "source": [
    "# Experimentation Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d9c57",
   "metadata": {},
   "source": [
    "## Experiment #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933890",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp1 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp1_branch,\n",
    "    'image_path': f\"{exp1_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp1_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp1_branch}/{metrics_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 100,\n",
    "    'is_shuffle':True,\n",
    "    'is_normalize': False,\n",
    "    'epochs': 200,\n",
    "    'train_test_split_ratio': 0.2,\n",
    "    'optimizer': \"adam\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87090771",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #1\n",
    "\n",
    "#### Create a new branch: `experiment-1` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8180c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=exp1_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "client.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60426aa0",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)\n",
    "print(\"Loading training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f540109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Commit the training data after preprocessing under /processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adf045",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, metrics1 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(metrics1, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "print(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7040e6",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model1, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "print(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc0293",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model1_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574e9ac",
   "metadata": {},
   "source": [
    "## Experiment #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6211d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp2 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp2_branch,\n",
    "    'image_path': f\"{exp2_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp2_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp2_branch}/{metrics_path}\",\n",
    "    'model_name': \"model-exp2.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49ae6f",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #2\n",
    "\n",
    "1. Create a new branch: `experiment-2` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.list_branches(repo_name)\n",
    "\n",
    "client.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=models.BranchCreation(name=exp2_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "\n",
    "client.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f3fe2",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Commit training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cee9b",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e18b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2, metrics2 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9703877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_metrics(metrics2, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1163d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d7254",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model2, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d8461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "client.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=models.CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ff1e4",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b114f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model2_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77b34f",
   "metadata": {},
   "source": [
    "### Compare models in both branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch = exp2_branch\n",
    "if metrics1['accuracy']> metrics2['accuracy']:\n",
    "    win_branch = exp1_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=win_branch, \n",
    "                              destination_branch=prod_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034e68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708f0e86",
   "metadata": {},
   "source": [
    "## DONE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33e8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
