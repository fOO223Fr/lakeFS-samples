{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d773767c-565a-415a-9cee-7feecc2d5e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# [Integration of lakeFS with Unity Catalog](https://docs.lakefs.io/integrations/unity-catalog.html)\n",
    "\n",
    "## Use Case: Isolated Development and Testing Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a903fa-02e3-4dac-9407-749e9c6bf2de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5064bb5-fcc3-44af-895e-9c96cd338720",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create lakeFS Python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b243b52-2cc0-4820-849c-c7d1f7f60e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lakefs\n",
    "from lakefs.client import Client\n",
    "\n",
    "lakefsEndPoint = '<lakeFS Endpoint URL>'\n",
    "lakefsAccessKey = '<lakeFS Access Key>'\n",
    "lakefsSecretKey = '<lakeFS Secret Key>'\n",
    "\n",
    "clt = Client(\n",
    "   host=lakefsEndPoint,\n",
    "   username=lakefsAccessKey,\n",
    "   password=lakefsSecretKey,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "206b55a3-3f12-4922-a041-033d65d8b46f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548f9231-f204-443d-83aa-5d4d4d4783c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "repositoryName = \"unity-catalog-demo\"\n",
    "storageNamespace = 's3://<bucket-name>/' + repositoryName\n",
    "sourceBranch = \"main\"\n",
    "\n",
    "repo = lakefs.Repository(\n",
    " repositoryName,\n",
    " client=clt).create(\n",
    "   storage_namespace=storageNamespace,\n",
    "   default_branch=sourceBranch,\n",
    "   exist_ok=True)\n",
    "branchMain = repo.branch(sourceBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b98bce2-8c09-4b50-a6b6-dac0b6bd4dde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Letâ€™s define the table descriptor and upload it to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dec63d4-5403-439f-9959-de9754399c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "table_name = \"famous_people\"\n",
    "unity_catalog_name = 'lakefs_unity_catalog_demo'\n",
    "\n",
    "table_descriptor = {\n",
    "   'name': table_name,\n",
    "   'type': 'delta',\n",
    "   'path': f'tables/{table_name}',\n",
    "   'catalog': unity_catalog_name,\n",
    "}\n",
    "\n",
    "# Write table descriptor to lakeFS\n",
    "with branchMain.object(path=f'_lakefs_tables/{table_name}.yaml').writer() as out:\n",
    "   yaml.safe_dump(table_descriptor, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d142bcbd-e16f-4783-9459-c200867e74f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Upload the Unity Catalog exporter script to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac5d0cf-b882-44f5-a531-4f654f73dcea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "luaScriptName = \"scripts/unity_export.lua\"\n",
    "\n",
    "lua_script = \"\"\"\n",
    "\n",
    "local aws = require(\"aws\")\n",
    "local formats = require(\"formats\")\n",
    "local databricks = require(\"databricks\")\n",
    "local delta_export = require(\"lakefs/catalogexport/delta_exporter\")\n",
    "local unity_export = require(\"lakefs/catalogexport/unity_exporter\")\n",
    "\n",
    "\n",
    "local sc = aws.s3_client(args.aws.access_key_id, args.aws.secret_access_key, args.aws.region)\n",
    "\n",
    "\n",
    "-- Export Delta Lake tables export:\n",
    "local delta_client = formats.delta_client(args.lakefs.access_key_id, args.lakefs.secret_access_key, args.aws.region)\n",
    "local delta_table_locations = delta_export.export_delta_log(action, args.table_defs, sc.put_object, delta_client, \"_lakefs_tables\")\n",
    "\n",
    "\n",
    "-- Register the exported table in Unity Catalog:\n",
    "local databricks_client = databricks.client(args.databricks_host, args.databricks_token)\n",
    "local registration_statuses = unity_export.register_tables(action, \"_lakefs_tables\", delta_table_locations, databricks_client, args.warehouse_id)\n",
    "for t, status in pairs(registration_statuses) do\n",
    "   print(\"Unity catalog registration for table \\\\\"\" .. t .. \"\\\\\" completed with commit schema status : \" .. status .. \"\\\\n\")\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "branchMain.object(path=luaScriptName).upload(data=lua_script, mode='wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a93834b0-b4e6-46ca-a912-02fd7cd36857",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define an action configuration that will run the above script after a commit or merge is completed over the main branch and upload it to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad25bb2-1d41-4b6a-ac59-5b21b0b51f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "newBranch = \"dev\"\n",
    "\n",
    "databricks_host = 'https://<instance-name>.cloud.databricks.com'\n",
    "databricks_token = '<Databricks personal access token>'\n",
    "warehouse_id = '<Databricks SQL Warehouse ID>'\n",
    "\n",
    "aws_region = '<AWS Region>'\n",
    "aws_access_key_id = '<AWS Access Key>'\n",
    "aws_secret_access_key = '<AWS Secret Key>'\n",
    "\n",
    "hook_definition = {\n",
    "   'name': 'unity_exporter',\n",
    "   'on': {\n",
    "       'post-commit': {\n",
    "           'branches': [sourceBranch, newBranch+'*']\n",
    "       },\n",
    "       'post-create-branch': {\n",
    "           'branches': [newBranch+'*']\n",
    "       }\n",
    "   },\n",
    "   'hooks': [\n",
    "       {\n",
    "           'id': 'Unity-Registration',\n",
    "           'type': 'lua',\n",
    "           'properties': {\n",
    "               'script_path': luaScriptName,\n",
    "               'args': {\n",
    "                   'aws': {\n",
    "                     'access_key_id': aws_access_key_id,\n",
    "                     'secret_access_key': aws_secret_access_key,\n",
    "                     'region': aws_region\n",
    "                   },\n",
    "                   'lakefs': {\n",
    "                       'access_key_id': lakefsAccessKey,\n",
    "                       'secret_access_key': lakefsSecretKey \n",
    "                   },\n",
    "                   'table_defs': [table_name],\n",
    "                   'databricks_host': databricks_host,\n",
    "                   'databricks_token': databricks_token,\n",
    "                   'warehouse_id': warehouse_id\n",
    "               }\n",
    "           }\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "\n",
    "with branchMain.object(path='_lakefs_actions/unity_exporter_action.yaml').writer() as out:\n",
    "   yaml.safe_dump(hook_definition, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91827cc7-c318-4e3a-8692-0390cf626138",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create the Delta Table in source branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899c415e-80b8-468a-a1c3-27d342f6c7dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "   ('James','Bond','England','intelligence'),\n",
    "   ('Robbie','Williams','England','music'),\n",
    "   ('Hulk','Hogan','USA','entertainment'),\n",
    "   ('Mister','T','USA','entertainment'),\n",
    "   ('Rafael','Nadal','Spain','professional athlete'),\n",
    "   ('Paul','Haver','Belgium','music'),\n",
    "]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"category\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"category\", \"country\").save(f\"lakefs://{repositoryName}/{sourceBranch}/tables/{table_name}\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb1724f-b393-4cca-995f-871ff0b7cf55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6577b798-3d39-41c3-9a3c-291aa4c0d980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "branchMain.commit(message='Added configuration files and Delta table!', \n",
    "        metadata={'using': 'python_api'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d89e2b9-3b3c-43c7-8acb-f0c7a59c0334",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demo Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c375b5-6098-4238-9db5-5d305d41dbb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Go to SQL Editor, click on \"All\" tab and refresh the schema\n",
    "##### You will notice \"main\" schema under \"lakefs_unity_catalog_demo\" catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8283edec-1eb2-4580-8385-bf7273d500aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run SQL to read the data from the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ecd68e4-33f7-4ce7-ab57-d7861c3ba203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM `{unity_catalog_name}`.`{sourceBranch}`.`{table_name}`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa53fd4-9096-43dd-bf44-d29612438ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54e8846-3193-4904-b376-6e947f305479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "newBranch = \"dev1\"\n",
    "branchDev = repo.branch(newBranch).create(source_reference=sourceBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e2456b-1018-4fc9-9226-1df0108d2558",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Go back to SQL Editor and refresh the schema\n",
    "##### You will notice new schema for the new branch created in previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91621781-c4b8-4f99-98ac-06f93426e6cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run SQL to read the data from the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ed4c5c-e3a7-4090-ba28-6aa57bfe8211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM `{unity_catalog_name}`.`{newBranch}`.`{table_name}`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1fe1ce-afd5-41f7-9930-178a620a2625",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Update Delta Table in the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88754846-65b7-4871-b79c-d211ecfd07ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_us = df.filter(col(\"country\") == \"USA\")\n",
    "df_us.write.format(\"delta\").mode(\"overwrite\").save(f\"lakefs://{repositoryName}/{newBranch}/tables/{table_name}\")\n",
    "df_us.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d96ac1d7-7c26-4bbd-a17a-eeeeb9463de1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Commit changes in the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da564102-8412-4507-8105-151b8b39d6b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "branchDev.commit(message='Updated delta table!', \n",
    "        metadata={'using': 'python_api'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2e7a1a-8576-46ce-95d9-77cd4cfa2572",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run SQL to read the data from the new branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76dafba2-d9fc-4ebc-9101-026b59602373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM `{unity_catalog_name}`.`{newBranch}`.`{table_name}`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ffa8ac-55d5-4fa3-8941-0d470558b521",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run SQL to read the data from the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec333e13-c657-40d3-b5eb-1257a7d6a83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM `{unity_catalog_name}`.`{sourceBranch}`.`{table_name}`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57cee0e3-b0c6-4da5-a98d-60624af424ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demo Completes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48762546-4449-4999-88ea-21b5b17530de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the [lakeFS Slack group](https://lakefs.io/slack)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4061283325167605,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Unity Catalog Export Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
