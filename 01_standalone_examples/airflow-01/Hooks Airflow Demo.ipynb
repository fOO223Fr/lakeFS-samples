{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "# Integration of lakeFS with Airflow via Hooks\n",
    "\n",
    "## Use Case: Isolated Ingestion & ETL Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ab23c-8bb8-4a2f-b979-350e7f9b42e0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "###### This Notebook requires connecting to a lakeFS Server. \n",
    "###### Run lakeFS locally with Docker (https://docs.lakefs.io/quickstart/run.html).\n",
    "\n",
    "##### Also, make sure that lakeFS server can connect to Airflow server either directly or using Virtual Private Network(VPN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18518157-aa20-4dea-9527-8539ef70d664",
   "metadata": {},
   "source": [
    "## Setup Task: Change your lakeFS credentials (Access Key and Secret Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be107f-1e9d-40dc-93cf-4063eac6f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://host.docker.internal:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io'\n",
    "lakefsAccessKey = 'AKIAIOSFOLKFSSAMPLES'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a278bb5-71f4-4ce7-a61f-c9df9ff80fec",
   "metadata": {},
   "source": [
    "## Setup Task: You can change lakeFS repo name (it can be an existing repo or provide another repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d6bdb-17c4-4bc8-815d-1720f5e23aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"airflow-hooks-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95914e1e-4f3c-4ae6-b0ec-15bc6199ebc0",
   "metadata": {},
   "source": [
    "## Setup Task: Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b70e52-0c5c-4b1c-a4f4-5854e3135fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceBranch = \"main\"\n",
    "newBranch = \"ingest\"\n",
    "airflowBranch = \"etl_airflow\"\n",
    "newPath = \"partitioned_data\"\n",
    "successFileName = \"success.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49611b2f-467b-442d-86d3-9ae73f97094c",
   "metadata": {},
   "source": [
    "## Setup Task: Storage Information\n",
    "#### Change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43069c8-bd2d-4875-b857-cb4ddf154400",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example/' + repo # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633e291-d28e-4638-9fc6-6d955a1cf440",
   "metadata": {},
   "source": [
    "## Setup Task: Run additional [Setup](./airflow/Hooks/HooksSetup.ipynb) tasks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a117dcb-4535-4770-8ea4-97907502c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./airflow/Hooks/HooksSetup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf01192-1bd4-4886-b034-00c646aac4c0",
   "metadata": {},
   "source": [
    "### You will run following steps in this notebook (refer to the image below):\n",
    "\n",
    "##### - Create repository with the Main branch\n",
    "##### - Create Ingest branch from the Main branch, add data file to ingest branch and commit the changes\n",
    "##### - Post-Commit hook will trigger Airflow Transformation DAG\n",
    "##### - Airflow Transformation DAG will create ETL branch from the Ingest branch\n",
    "##### - Airflow Transformation DAG will run transformation task and will create Success file if transformation succeeds\n",
    "##### - Airflow Transformation DAG will commit the changes and will merge ETL branch into Ingest branch\n",
    "##### - Merge Ingest branch into the Main branch\n",
    "##### - Pre-Merge hook will trigger another Airflow DAG which will look for the Success file in the Ingest branch, will confirm successful completion of the ETL job and merge will succeed\n",
    "##### - If Pre-Merge hook DAG fails then merge will also fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b45531-dd00-4de9-8ba6-52cfa80c50eb",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/1-AllSteps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422d8af-c714-4d32-aa7b-eddfacbbddb4",
   "metadata": {},
   "source": [
    "## If repo already exists on your lakeFS server then you can skip following step otherwise create a new repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306b4e7-916b-4564-b001-2d872db80223",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a20fd-8db5-4598-a434-99adf29a68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = lakefs.Repository(repo).create(storage_namespace=storageNamespace, default_branch=sourceBranch, exist_ok=True)\n",
    "main = repository.branch(sourceBranch)\n",
    "print(repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca865d-0aa0-46bb-b6f2-54d7205dda64",
   "metadata": {},
   "source": [
    "# Ingest and ETL Process Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7cd60-8a5a-4f90-9f2b-527093844411",
   "metadata": {},
   "source": [
    "## Create ingest branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dd0b8-5303-474e-865c-f65c0913dc37",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86783c-01cc-4d64-900e-430c22209ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = repository.branch(newBranch).create(source_reference=sourceBranch)\n",
    "print(branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3018451-500a-4ef2-b5a0-70b5cda7f64a",
   "metadata": {},
   "source": [
    "## Upload bad data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87e2b7-81c5-4cbd-bf5c-b68bf163ad33",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67accbb2-7e38-4985-94f7-2740880c0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "contentToUpload = open(os.path.expanduser('~')+'/airflow/Hooks/data/bad_data_file/'+fileName, 'r').read()\n",
    "branch.object(fileName).upload(data=contentToUpload, mode='wb', pre_sign=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cd957-beb8-4d40-9bef-56b0ae9be47c",
   "metadata": {},
   "source": [
    "## Upload [Post-Commit Actions](./airflow/Hooks/actions_post_commit.yaml) file. This will invoke Post-Commit DAG.\n",
    "\n",
    "#### You can review [Post-Commit DAG](./airflow/Hooks/lakefs_hooks_post_commit_dag.py) program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cdca0e-e3ea-45f5-b1df-f468111f83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contentToUpload = open(os.path.expanduser('~')+'/airflow/Hooks/actions_post_commit.yaml', 'r').read() # Only a single file per upload which must be named \\\\\\\"content\\\\\\\"\n",
    "branch.object('_lakefs_actions/actions_post_commit.yaml').upload(data=contentToUpload, mode='wb', pre_sign=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39753a2-6908-47b1-9614-37214d5a9612",
   "metadata": {},
   "source": [
    "## Upload [Pre-Merge Actions](./airflow/Hooks/actions_pre_merge.yaml) file. This will invoke Pre-Merge DAG to verify if Post-Commit DAG was successful or not.\n",
    "\n",
    "#### You can review [Pre-Merge DAG](./airflow/Hooks/lakefs_hooks_pre_merge_dag.py) program. DAG verifies success.txt file which is created by Post-Commit DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa6df3-ff96-4e3f-a360-970ffefd87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "contentToUpload = open(os.path.expanduser('~')+'/airflow/Hooks/actions_pre_merge.yaml', 'r').read() # Only a single file per upload which must be named \\\\\\\"content\\\\\\\"\n",
    "branch.object('_lakefs_actions/actions_pre_merge.yaml').upload(data=contentToUpload, mode='wb', pre_sign=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0e30a-9638-465d-9ee5-9dd41ba99fd8",
   "metadata": {},
   "source": [
    "## Compare ingest to main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed47112-b997-4b4b-9d85-8525ac4addda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map(\n",
    "    lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "    branch.uncommitted())\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(\n",
    "    results,\n",
    "    headers=['Path','Path Type','Size(Bytes)','Type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2faa4ae-b2e4-47ab-b51f-b61d9ed4d846",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa483f4d-7ffe-4081-8f90-af19e987c8f5",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc404452-97cd-40f5-8919-7d1bfa8f7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = branch.commit(message='Uploaded bad data file!',\n",
    "        metadata={'airflow dag url': 'http://127.0.0.1:8080/dags/lakefs_hooks_post_commit_dag/grid',\n",
    "                  'ml model version': 'v1.0'})\n",
    "print(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b123a-4077-488d-8b35-9d0ae52fffcb",
   "metadata": {},
   "source": [
    "## Post-Commit DAG will get triggered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3e139-8f12-4d2e-b089-3a1001526de3",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7680654-f7a3-4ea2-932c-2844ab12a950",
   "metadata": {},
   "source": [
    "### Visualize [Post-Commit DAG Graph](http://127.0.0.1:8080/dags/lakefs_hooks_post_commit_dag/graph) in Airflow UI. Login by using username \"airflow\" and password \"airflow\".\n",
    "\n",
    "##### Toggle Auto Refresh switch in DAG Graph to see the continuous progress of the workflow.\n",
    "##### Click on any lakeFS related task box, then click on \"lakeFS UI\" button (this URL will take you to applicable branch/commit/data file in lakeFS). You will also find this URL in the Airflow log if you click on Log button and search for \"lakeFS URL\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1381f-e625-45f6-b572-342225596f17",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111dea51-cd7b-4536-b082-e25183e8e5f2",
   "metadata": {},
   "source": [
    "## DAG will create ETL branch (with timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a6305-92b0-45e3-a245-629d07fb2125",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ad959-98ae-499a-b7d6-e5ed420f93aa",
   "metadata": {},
   "source": [
    "## Transformation job fails due to bad data\n",
    "\n",
    "### Task \"transformation\" will fail in this case. Click on \"transformation\" task box, then click on Log button and search for \"Exception\". You will notice following exception:\n",
    "### \"Partition column _c4 not found in schema struct<_c0:string,_c1:string,_c2:string,_c3:string>\"\n",
    "\n",
    "### This exception happens because column \"_c4\" (or 5th column) is missing in the latest file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523341a0-2b00-48e6-aeb1-43fc2646c6d7",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89745d5b-fa08-493b-91b3-9ee8936feff3",
   "metadata": {},
   "source": [
    "## Try to merge ingest branch into the main branch. Merge will fail if either Post-Commit DAG fails or DAG is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb14f1-b0c5-47f5-9a4b-70bf88f32289",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = branch.merge_into(main)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52788d-ab8c-41b2-b013-6c694f6c2fb4",
   "metadata": {},
   "source": [
    "### Visualize [Pre-Merge DAG Graph](http://127.0.0.1:8080/dags/lakefs_hooks_pre_merge_dag/graph) in Airflow UI\n",
    "\n",
    "### Task \"sense_success_file\" will fail in this case. Click on \"sense_success_file\" task box, then click on Log button. You will notice following message in the log:\n",
    "### File 'success.txt' not found on branch 'ingest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9cf54-4472-4543-892c-344df3981bdc",
   "metadata": {},
   "source": [
    "## Upload correct data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbf120-5c9c-41a6-9ddf-834090a1c323",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1616bd3-dbb6-4aaa-a4ba-80df274bb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contentToUpload = open(os.path.expanduser('~')+'/airflow/Hooks/data/correct_data_file/'+fileName, 'r').read()\n",
    "branch.object(fileName).upload(contentToUpload, mode='wb', pre_sign=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bc8b6-eac7-44a2-9348-9b4d5683235a",
   "metadata": {},
   "source": [
    "## Add Release Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d9580-e254-493d-ab34-3434d322905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contentToUpload = open(os.path.expanduser('~')+'/airflow/Hooks/data/ReleaseNotes.txt', 'r').read()\n",
    "branch.object('ReleaseNotes.txt').upload(contentToUpload, mode='wb', pre_sign=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349dffa-f7df-4671-b531-b368bb756f15",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata. Post-Commit DAG will get triggered again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9af2a-f9b5-4513-a556-5944145914c5",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf009633-0dce-4f17-b5f8-5d0e1ba2787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = branch.commit(message='Uploaded correct data file!',\n",
    "        metadata={'airflow dag url': 'http://127.0.0.1:8080/dags/lakefs_hooks_post_commit_dag/grid',\n",
    "                  'ml model version': 'v1.0'})\n",
    "print(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26899264-f4c9-4021-b7c9-c454bb9b5d0f",
   "metadata": {},
   "source": [
    "### Visualize [Post-Commit DAG Graph](http://127.0.0.1:8080/dags/lakefs_hooks_post_commit_dag/graph) in Airflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18f32-e39b-49e2-b5f8-1c16f865d6bb",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa029fe-6b43-4e0f-a905-c58bfb2c514e",
   "metadata": {},
   "source": [
    "## DAG will create ETL branch (with timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984b0ea-b299-47f5-8926-bd685a9e0efb",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77694f-70df-4de8-9daa-75d621109aea",
   "metadata": {},
   "source": [
    "## Transformation job will succeed and will create Success file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2672c5c-cefe-4e16-8472-00744baebf18",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99984ce-0df0-46bf-ae80-856772c813dd",
   "metadata": {},
   "source": [
    "## Merge ETL branch into Ingest branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166bf52-e499-49da-90a2-28795c144d20",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab052d4-c8f3-48dd-b591-1d32662c8b71",
   "metadata": {},
   "source": [
    "## Add tag for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226746f-11a8-47ee-b72b-35369dd3fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'v1.0'\n",
    "lakefs.Tag(repo, tag).create(newBranch, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38aa9b-ecbd-4521-a977-b2fbad23e377",
   "metadata": {},
   "source": [
    "## Merge Ingest branch into the Main branch. Merge will succeed this time because Post-Commit DAG succeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714953a-1045-499a-9832-d9763aef274e",
   "metadata": {},
   "source": [
    "![Step 1](./Images/AirflowHooks/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4261fc3-efc4-401c-9248-fdce6966640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = branch.merge_into(main)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60dd74-ea60-4c87-83ee-c7841621a2d4",
   "metadata": {},
   "source": [
    "### Visualize [Pre-Merge DAG Graph](http://127.0.0.1:8080/dags/lakefs_hooks_pre_merge_dag/graph) in Airflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bc8ab-e654-42b3-9ae2-c2d06ec64e9f",
   "metadata": {},
   "source": [
    "## Read data by using tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6927d47-3c08-4494-aecb-dbd4d601ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'v1.0'\n",
    "dataPath = f\"s3a://{repo}/{tag}/{fileName}\"\n",
    "\n",
    "df = spark.read.csv(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304f0dd-ce0c-4b8a-a856-806be4d87673",
   "metadata": {},
   "source": [
    "## If you want you can atomically rollback all changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e16c7-0e29-4083-b266-b4a8ecc449e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.revert(parent_number=1, reference_id=sourceBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fc9c6-c8f2-4b75-8e5e-f7af1dcd6e36",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58ce89-ac44-450d-89b7-f9f43cf8f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
