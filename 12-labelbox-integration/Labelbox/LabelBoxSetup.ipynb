{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba3e2fa-47db-4668-8426-c9fb612b4136",
   "metadata": {},
   "source": [
    "# Integration of lakeFS with Labelbox\n",
    "\n",
    "## Use Case: ML Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f8456-55df-4049-9a8f-72e536ad2ec4",
   "metadata": {},
   "source": [
    "## Setup Task: Import required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc28cf0-9a6d-4bf0-8dd8-0fd49f996216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import labelbox\n",
    "import datetime\n",
    "from tabulate import tabulate\n",
    "from uuid import uuid4 ## to generate unique IDs\n",
    "import json\n",
    "from labelbox.schema.ontology import OntologyBuilder, Tool, Classification,Option\n",
    "import random\n",
    "from labelbox.data.annotation_types import (\n",
    "    Label,\n",
    "    Point,\n",
    "    LabelList,\n",
    "    ImageData,\n",
    "    Rectangle,\n",
    "    ObjectAnnotation,\n",
    ")\n",
    "from labelbox.data.serialization import NDJsonConverter\n",
    "import time\n",
    "from labelbox.schema.annotation_import import LabelImport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1259ef-d6d6-48d5-80ab-9c23ce15de8c",
   "metadata": {},
   "source": [
    "## Setup Task: lakeFS Upload Objects Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0dbf1-6115-43ec-80ce-36ee3547e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files(repo, branch, path, files):\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        contentToUpload = open(file, 'rb') # Only a single file per upload which must be named \\\\\\\"content\\\\\\\"\n",
    "        client.objects.upload_object(\n",
    "            repository=repo,\n",
    "            branch=branch,\n",
    "            path=path+'/'+os.path.basename(file), content=contentToUpload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee258c-e939-41c6-bf5f-f456333ffac0",
   "metadata": {},
   "source": [
    "## Setup Task: lakeFS Stage Object Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0094af-c6e0-465f-8be5-bc3cac41575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.model.object_stage_creation import ObjectStageCreation\n",
    "from lakefs_client.model.object_user_metadata import ObjectUserMetadata\n",
    "\n",
    "def object_stage(source_uri, size_bytes, content_type):\n",
    "    object_stage_creation = ObjectStageCreation(\n",
    "        physical_address=source_uri,\n",
    "        checksum=\"\",\n",
    "        size_bytes=size_bytes,\n",
    "        mtime=1,\n",
    "        metadata=ObjectUserMetadata( # optional\n",
    "            key=\"version: v1\",\n",
    "        ),\n",
    "        content_type=content_type,\n",
    "    ) # ObjectStageCreation | \n",
    "    return object_stage_creation\n",
    " \n",
    "def stage_objects(repo_name, importBranch, source_uri, path, size_bytes, content_type):   \n",
    "    object_stage_creation = object_stage(source_uri, size_bytes, content_type)\n",
    "    try:       \n",
    "        api_response_1 = client.objects.stage_object(repo_name, importBranch, path, object_stage_creation)\n",
    "       \n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(\"Exception when calling objects->stage_object: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be33a9-e80f-43db-a7a0-9ffe22649d49",
   "metadata": {},
   "source": [
    "## Setup Task: Create S3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903271f5-ff5c-4b25-9d64-517a75b95ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3',\n",
    "    endpoint_url='https://s3.' + awsRegion + '.amazonaws.com',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b28561-2878-4299-b566-3039b6e605ef",
   "metadata": {},
   "source": [
    "## Setup Task: Create lakeFS Python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69327827-9bd2-44b3-b1e2-e2ae47b7a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "import datetime\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8f1c2-65d7-4d03-b68b-064d3194672c",
   "metadata": {},
   "source": [
    "## Setup Task: S3A Gateway configuration\n",
    "\n",
    "##### Note: lakeFS can be configured to work with Spark in two ways:\n",
    "###### * Access lakeFS using the S3A gateway https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-s3a-gateway.\n",
    "###### * Access lakeFS using the lakeFS-specific Hadoop FileSystem https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c95ae-6447-4f61-b4ed-6f2ad00f2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16dcdd-12cb-445c-89c2-b34640ac562c",
   "metadata": {},
   "source": [
    "## Setup Task: Create Labelbox Python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122a925-d080-401f-b666-b556323cb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_client = labelbox.Client(LB_API_KEY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
