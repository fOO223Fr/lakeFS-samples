{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "# Use Case: Reprocess and Backfill Data with new ETL logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f4469-9708-435d-bec9-c943fd87a0bf",
   "metadata": {},
   "source": [
    "## Setup Task: Change your lakeFS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa46799-5f86-4104-9447-d91328edbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsAccessKey = '<lakeFS Access Key>'\n",
    "lakefsSecretKey = '<lakeFS Secret Key>'\n",
    "lakefsEndPoint = '<lakeFS Endpoint URL>' # e.g. 'https://username.aws_region_name.lakefscloud.io'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1c562-9899-4343-b887-c5c018ea79b0",
   "metadata": {},
   "source": [
    "## Setup Task: Storage Information\n",
    "#### Change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b151c2-743d-43e1-9f2f-25482967c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://<S3 Bucket Name>/' # e.g. \"s3://username-lakefs-cloud/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093db8e-68d9-409f-bde7-73ee4ceca5a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Task: Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b839850-5954-4631-8e7e-d0dee6d17dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestBranch = \"ingest\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "processedFileName = \"lakefs_test_processed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdc705-14b7-43be-b5d0-69a7f685e6a9",
   "metadata": {},
   "source": [
    "## Run additional [Setup](./ReprocessData/Setup.ipynb) tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc6cee-c566-4b04-909b-077ef4e98646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./ReprocessData/Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724cfd8-bda5-4c16-bff6-e05d0ad7f74a",
   "metadata": {},
   "source": [
    "## You can change lakeFS repo name (it can be an existing repo or provide another repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da678dc-5a25-4b90-9dea-30052f45e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"my-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431be3f-d741-4cb6-98cf-8b21b8f2a489",
   "metadata": {},
   "source": [
    "## If above mentioned repo already exists on your lakeFS server then you can skip following step otherwise create a new repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7152ea0-0d5b-4035-ad32-d43ac6453c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repositories.create_repository(\n",
    "    repository_creation=models.RepositoryCreation(\n",
    "        name=repo,\n",
    "        storage_namespace=storageNamespace,\n",
    "        default_branch=mainBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360b5f6-76db-438b-9aa0-e40070b86bb9",
   "metadata": {},
   "source": [
    "## [ETL](./ReprocessData/ETL.ipynb) job normally run as a batch job but run ETL job manually here for the demo\n",
    "\n",
    "### It will take around a minute to run this ETL job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7db274-07b5-49f9-b7de-5b912818317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./ReprocessData/ETL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca865d-0aa0-46bb-b6f2-54d7205dda64",
   "metadata": {},
   "source": [
    "# Reprocessing Starts\n",
    "\n",
    "## Set the name for reprocessing branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732966a0-f4e1-4a81-98ae-89b9c0799b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocessBranch = \"new-logic\"\n",
    "%run ./ReprocessData/Reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc3b3d-3e5e-4be7-82b4-10124d85613e",
   "metadata": {},
   "source": [
    "## While ETL logic is getting fixed, old ETL job is still running in parallel.\n",
    "\n",
    "## Received new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25504879-37e8-46b2-8c3f-88596fe0240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_new.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbbaaa-51cf-4986-be84-d6a22997e706",
   "metadata": {},
   "source": [
    "## Run [ETL](./ReprocessData/ETL.ipynb) job again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02480abb-d470-4742-8bae-86a71c676f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./ReprocessData/ETL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fcd72-417c-4079-a051-6ee8fe6662ac",
   "metadata": {},
   "source": [
    "## Now Reprocessing branch is behind Main branch in terms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e289db8-a33d-4c70-933f-e24282fd0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + reprocessBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo}/{reprocessBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0462b6-c8a1-4212-bb1b-54ffbfda4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d894e-d6bb-4a2d-8132-c405b8defabb",
   "metadata": {},
   "source": [
    "## Once ETL logic is fixed, pause the old ETL job to deploy new ETL logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e5cbd-0a0a-48ff-827e-68dc0dd4b712",
   "metadata": {},
   "source": [
    "## Set the name for \"Backfill and Deploy\" branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baeef2-6c8e-4537-81a4-c04204dc6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "backfillAndDeployBranch = \"backfill-and-deploy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a59637-db0c-43b2-8572-e91795ea1fdd",
   "metadata": {},
   "source": [
    "## Run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job again on \"Backfill and Deploy\" branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216328cf-29b7-4aa1-bfce-0bddbed58590",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocessBranch = backfillAndDeployBranch\n",
    "%run ./ReprocessData/Reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19852e24-d198-4d0f-98fe-d756baf586e2",
   "metadata": {},
   "source": [
    "## Now \"Backfill and Deploy\" branch has same data as Main branch and correct ETL logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573cfb2-049c-4617-ba99-b169c84cfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + backfillAndDeployBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo}/{backfillAndDeployBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2fc43-9056-414c-9516-d1eb6da77631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113388d-94b3-4bf9-807f-0ea08360089e",
   "metadata": {},
   "source": [
    "## Merge \"Backfill and Deploy\" branch to Main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40762c9-0344-4cbc-b28a-12673f30a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(\n",
    "    repository=repo, source_ref=backfillAndDeployBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762d1a0-276f-4a20-8ca0-18f4a463a622",
   "metadata": {},
   "source": [
    "# Reprocessing and Backfill completes\n",
    "\n",
    "## Verify data on Main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f67d-98ad-45f8-b52d-a4a8a9860443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583e2de-b113-4e31-9bab-1de5feb16f9d",
   "metadata": {},
   "source": [
    "## Now you can schedule the new ETL job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
