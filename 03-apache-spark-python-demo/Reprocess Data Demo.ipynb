{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "# Use Case: Reprocess and Backfill Data with new ETL logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67662314-7dcd-4e6c-869f-24a1a76d1a0e",
   "metadata": {},
   "source": [
    "### You will run following steps in this notebook (refer to the image below):\n",
    "\n",
    "#### Step 1: Create repository with the Main branch\n",
    "#### Step 2: Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "#### Step 3: Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "#### Step 4: Repetition of step # 2\n",
    "#### Step 5: Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes.\n",
    "#### Step 6: Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a0676-60ff-47e8-af5b-fa44206dcb3d",
   "metadata": {},
   "source": [
    "![Reprocess](./Images/ReprocessData/Reprocess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a7450-eb31-413f-b952-a823d9176a30",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "###### This Notebook requires connecting to a lakeFS Server. \n",
    "###### To spin up lakeFS quickly - use the Playground (https://demo.lakefs.io) which provides lakeFS server on-demand with a single click; \n",
    "###### Or, alternatively, refer to lakeFS Quickstart doc (https://docs.lakefs.io/quickstart/installing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f4469-9708-435d-bec9-c943fd87a0bf",
   "metadata": {},
   "source": [
    "## Setup Task: Change your lakeFS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa46799-5f86-4104-9447-d91328edbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = '<lakeFS Endpoint URL>' # e.g. 'https://username.aws_region_name.lakefscloud.io'\n",
    "lakefsAccessKey = '<lakeFS Access Key>'\n",
    "lakefsSecretKey = '<lakeFS Secret Key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1c562-9899-4343-b887-c5c018ea79b0",
   "metadata": {},
   "source": [
    "## Setup Task: Storage Information\n",
    "#### Change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b151c2-743d-43e1-9f2f-25482967c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://<S3 Bucket Name>/' # e.g. \"s3://username-lakefs-cloud/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093db8e-68d9-409f-bde7-73ee4ceca5a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Task: Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b839850-5954-4631-8e7e-d0dee6d17dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestBranch = \"ingest\"\n",
    "fileName = \"lakefs_test.csv\"\n",
    "processedFileName = \"lakefs_test_processed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfdc705-14b7-43be-b5d0-69a7f685e6a9",
   "metadata": {},
   "source": [
    "## Run additional [Setup](./ReprocessData/Setup.ipynb) tasks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc6cee-c566-4b04-909b-077ef4e98646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./ReprocessData/Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8724cfd8-bda5-4c16-bff6-e05d0ad7f74a",
   "metadata": {},
   "source": [
    "## You can change lakeFS repo name (it can be an existing repo or provide another repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da678dc-5a25-4b90-9dea-30052f45e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"my-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431be3f-d741-4cb6-98cf-8b21b8f2a489",
   "metadata": {},
   "source": [
    "## Step 1: Create repository with the Main branch\n",
    "\n",
    "### (if above mentioned repo already exists on your lakeFS server then you can skip this operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2218d-3af0-415d-a35a-ab4ba06ac040",
   "metadata": {},
   "source": [
    "![Step 1](./Images/ReprocessData/Step1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7152ea0-0d5b-4035-ad32-d43ac6453c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repositories.create_repository(\n",
    "    repository_creation=models.RepositoryCreation(\n",
    "        name=repo,\n",
    "        storage_namespace=storageNamespace,\n",
    "        default_branch=mainBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360b5f6-76db-438b-9aa0-e40070b86bb9",
   "metadata": {},
   "source": [
    "## Step 2: Create ingestion branch from the Main branch, ingest data file, run the ETL job, commit the changes and merge ingestion branch to the Main branch\n",
    "\n",
    "### ([ETL](./ReprocessData/ETL.ipynb) job normally run as a batch job but run ETL job manually here for the demo. It will take around a minute to run this step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2b2de-617e-4485-8b41-e471feae8a93",
   "metadata": {},
   "source": [
    "![Step 2](./Images/ReprocessData/Step2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7db274-07b5-49f9-b7de-5b912818317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./ReprocessData/ETL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca865d-0aa0-46bb-b6f2-54d7205dda64",
   "metadata": {},
   "source": [
    "# Reprocessing Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace14847-e55c-4b53-8648-0ae88eaebbad",
   "metadata": {},
   "source": [
    "## Step 3: Create new-logic branch from the Main branch, fix ETL logic and commit the changes\n",
    "### (you can change the name for reprocessing branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8029d9f-2f74-4cb1-950f-616ef69c063d",
   "metadata": {},
   "source": [
    "![Step 3](./Images/ReprocessData/Step3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732966a0-f4e1-4a81-98ae-89b9c0799b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocessBranch = \"new-logic\"\n",
    "%run ./ReprocessData/Reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc3b3d-3e5e-4be7-82b4-10124d85613e",
   "metadata": {},
   "source": [
    "## While ETL logic is getting fixed, old ETL job is still running in parallel.\n",
    "\n",
    "## Received new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25504879-37e8-46b2-8c3f-88596fe0240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_new.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbbaaa-51cf-4986-be84-d6a22997e706",
   "metadata": {},
   "source": [
    "## Step 4: Repetition of step # 2\n",
    "\n",
    "### (run [ETL](./ReprocessData/ETL.ipynb) job again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d26f30-afc7-4a10-a3b9-6486a48dff7a",
   "metadata": {},
   "source": [
    "![Step 4](./Images/ReprocessData/Step4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02480abb-d470-4742-8bae-86a71c676f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./ReprocessData/ETL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fcd72-417c-4079-a051-6ee8fe6662ac",
   "metadata": {},
   "source": [
    "## Now Reprocessing branch is behind Main branch in terms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e289db8-a33d-4c70-933f-e24282fd0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + reprocessBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo}/{reprocessBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0462b6-c8a1-4212-bb1b-54ffbfda4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d894e-d6bb-4a2d-8132-c405b8defabb",
   "metadata": {},
   "source": [
    "## Once ETL logic is fixed, pause the old ETL job to deploy new ETL logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e5cbd-0a0a-48ff-827e-68dc0dd4b712",
   "metadata": {},
   "source": [
    "## Step 5: Create backfill-and-deploy branch from the Main branch, run new ETL logic, overwrite processed data and commit the changes\n",
    "### (you can change the name for the \"Backfill and Deploy\" branch and run [Reprocessing](./ReprocessData/Reprocessing.ipynb) job again on \"Backfill and Deploy\" branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74206d1e-9ec8-4209-a940-49f7b2d333f6",
   "metadata": {},
   "source": [
    "![Step 5](./Images/ReprocessData/Step5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216328cf-29b7-4aa1-bfce-0bddbed58590",
   "metadata": {},
   "outputs": [],
   "source": [
    "backfillAndDeployBranch = \"backfill-and-deploy\"\n",
    "reprocessBranch = backfillAndDeployBranch\n",
    "%run ./ReprocessData/Reprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19852e24-d198-4d0f-98fe-d756baf586e2",
   "metadata": {},
   "source": [
    "## Now \"Backfill and Deploy\" branch has same data as Main branch and correct ETL logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573cfb2-049c-4617-ba99-b169c84cfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on \" + backfillAndDeployBranch + \" branch\")\n",
    "dataPath = f\"s3a://{repo}/{backfillAndDeployBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2fc43-9056-414c-9516-d1eb6da77631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113388d-94b3-4bf9-807f-0ea08360089e",
   "metadata": {},
   "source": [
    "## Step 6: Merge backfill-and-deploy branch to the Main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df9f1d-c21a-4ab3-b7f0-f2aa5bb7fd4b",
   "metadata": {},
   "source": [
    "![Step 6](./Images/ReprocessData/Step6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40762c9-0344-4cbc-b28a-12673f30a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(\n",
    "    repository=repo, source_ref=backfillAndDeployBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762d1a0-276f-4a20-8ca0-18f4a463a622",
   "metadata": {},
   "source": [
    "# Reprocessing and Backfill completes\n",
    "\n",
    "## Verify data on Main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f67d-98ad-45f8-b52d-a4a8a9860443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed data on main branch\")\n",
    "dataPath = f\"s3a://{repo}/{mainBranch}/{processedFileName}\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(processedDataFileSchema).load(dataPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583e2de-b113-4e31-9bab-1de5feb16f9d",
   "metadata": {},
   "source": [
    "## Now you can schedule the new ETL job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b24a6f-56e6-4bd3-b898-0ff0c188e223",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
