{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1674ec1-9227-4159-a9bc-5000b31f6e12",
   "metadata": {},
   "source": [
    "# [Integration of lakeFS with Airflow](https://docs.lakefs.io/integrations/airflow.html)\n",
    "\n",
    "## Use Case: Troubleshooting production issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddc884-bdf5-4fc5-97b1-38662358268c",
   "metadata": {},
   "source": [
    "## Change your lakeFS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83f622-2e07-4f20-aa70-8958124c7d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsAccessKey = '<lakeFS Access Key>'\n",
    "lakefsSecretKey = '<lakeFS Secret Key>'\n",
    "lakefsEndPoint = '<lakeFS Endpoint URL>' # e.g. 'https://username.aws_region_name.lakefscloud.io'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5ae63-dc9b-43b3-a883-0a3865ad5dc6",
   "metadata": {},
   "source": [
    "## You can change lakeFS repo and branch name but it should be an existing repo and branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1f393-3346-440f-8083-99aeb6013443",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"my-repo\"\n",
    "sourceBranch = \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f793f5-22f2-43f7-8e5f-f39149703314",
   "metadata": {},
   "source": [
    "## Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3e3d3-df16-4899-a52e-4cbb2892e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "newBranch = \"airflow_demo\"\n",
    "newPath = \"partitioned_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba46998-3a3d-4b52-ba59-6ee5c1893634",
   "metadata": {},
   "source": [
    "## Review and copy [demo Workflow](./Airflow/lakefs-dag.py) program to Airflow DAGs directory. If you make any changes in the program, then run this command again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a10ee4-95e7-4284-8bcb-6ad2304c76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo cp ./Airflow/lakefs-dag.py /root/airflow/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1768d3-d5e2-4b75-a674-934fda8efb4a",
   "metadata": {},
   "source": [
    "## Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba09e1a-7858-4e53-a88d-de68ecddd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "sudo pkill airflow\n",
    "sudo airflow standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2c0d0-6190-476d-aa3d-8f927728009c",
   "metadata": {},
   "source": [
    "## Create Airflow connections for lakeFS and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57d6e8-8ce2-4895-a215-1b7045abb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Airflow to start\n",
    "! sudo sleep 10\n",
    "\n",
    "! sudo airflow connections delete conn_lakefs\n",
    "lakeFSConnectionCommand = 'airflow connections add conn_lakefs --conn-type=http --conn-host=' + lakefsEndPoint + ' --conn-extra=\\'{\"access_key_id\":\"' + lakefsAccessKey + '\",\"secret_access_key\":\"' + lakefsSecretKey + '\"}\\''\n",
    "! sudo $lakeFSConnectionCommand\n",
    "\n",
    "! sudo airflow connections delete conn_spark\n",
    "sparkConnectionCommand = 'airflow connections add conn_spark --conn-type=spark --conn-host=local[*]'\n",
    "! sudo $sparkConnectionCommand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8481df7-94f9-4bcf-a736-b2f58074aac1",
   "metadata": {},
   "source": [
    "## Set Airflow variables which are used by the demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4304d-8a10-4858-992f-a44d07f671ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo airflow variables set lakefsAccessKey $lakefsAccessKey\n",
    "! sudo airflow variables set lakefsSecretKey $lakefsSecretKey\n",
    "! sudo airflow variables set lakefsEndPoint $lakefsEndPoint\n",
    "! sudo airflow variables set repo $repo\n",
    "! sudo airflow variables set sourceBranch $sourceBranch\n",
    "! sudo airflow variables set newBranch $newBranch\n",
    "! sudo airflow variables set newPath $newPath\n",
    "! sudo airflow variables set conn_lakefs 'conn_lakefs'\n",
    "\n",
    "import os\n",
    "spark_home = os.getenv('SPARK_HOME')\n",
    "! sudo airflow variables set spark_home $spark_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc058c-494e-4bbe-811d-9b488abdf5fb",
   "metadata": {},
   "source": [
    "## Set the fileName Airflow variable. This file is used by the demo workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9e0f4-f3b4-4e4f-9a0e-44f1f9f30bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test.csv\"\n",
    "! sudo airflow variables set fileName $fileName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a6b5c-c3d4-4bbf-bfb3-89309f813b4f",
   "metadata": {},
   "source": [
    "## Find Airflow admin password and copy the password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107bdca-5e26-41f3-b223-c9c0b7bb843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo cat /root/airflow/standalone_admin_password.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf42ac-9e4a-46a6-a615-16825da83598",
   "metadata": {},
   "source": [
    "## Visualize [demo workflow DAG Graph](http://127.0.0.1:8080/dags/lakeFS_workflow/graph) in Airflow UI. Login by using username admin and password received in the previous command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230ad3d-420b-491a-9129-51f99c5e95b2",
   "metadata": {},
   "source": [
    "## Trigger demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c89a0-d8e7-4689-80a4-9dee7aa0fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo airflow dags unpause lakeFS_workflow\n",
    "! sudo airflow dags trigger lakeFS_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d0e77-8cb9-47fb-81b2-74c96fae10b9",
   "metadata": {},
   "source": [
    "## Visualize [demo workflow DAG Graph](http://127.0.0.1:8080/dags/lakeFS_workflow/graph). Toggle Auto Refresh switch in DAG Graph to see the continuous progress of the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440b100-98a9-4ef2-b622-4e7bbd9d2862",
   "metadata": {},
   "source": [
    "## Once the demo workflow finishes in around 5 minutes, you can use the latest or new file. This file has bad data, and it will cause workflow to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d825b-f4e6-4527-b7ca-48079a0a4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_latest_file.csv\"\n",
    "! sudo airflow variables set fileName $fileName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bae53d-16d2-4a6e-b05f-66e0854db56e",
   "metadata": {},
   "source": [
    "## Trigger demo workflow again by using the latest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecafe94-4c8c-483e-95ec-3e5d81cae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo airflow dags trigger lakeFS_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101b97f-0fc9-4831-9fe3-834fca1f7c2a",
   "metadata": {},
   "source": [
    "## Visualize [demo workflow DAG Graph](http://127.0.0.1:8080/dags/lakeFS_workflow/graph) for the new run with the latest file.\n",
    "\n",
    "### Task \"etl_task3\" will fail in this case. Click on \"etl_task3\" task box, then click on Log button and search for Exception. You will notice following exception:\n",
    "### \"Partition column _c4 not found in schema struct<_c0:string,_c1:string,_c2:string,_c3:string>\"\n",
    "\n",
    "### This exception happens because column \"_c4\" (or 5th column) is missing in the latest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bde21f-0e43-4edf-8de7-73cc4392553b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
