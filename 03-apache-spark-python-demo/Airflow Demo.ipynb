{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1674ec1-9227-4159-a9bc-5000b31f6e12",
   "metadata": {},
   "source": [
    "# [Integration of lakeFS with Airflow](https://docs.lakefs.io/integrations/airflow.html)\n",
    "\n",
    "## Use Case: Troubleshooting production issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddc884-bdf5-4fc5-97b1-38662358268c",
   "metadata": {},
   "source": [
    "## Change your lakeFS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a905b0-ab02-426d-8049-7138de6efc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsAccessKey = '<lakeFS Access Key>'\n",
    "lakefsSecretKey = '<lakeFS Secret Key>'\n",
    "lakefsEndPoint = '<lakeFS Endpoint URL>' # e.g. 'https://username.aws_region_name.lakefscloud.io'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5ae63-dc9b-43b3-a883-0a3865ad5dc6",
   "metadata": {},
   "source": [
    "## You can change lakeFS repo name (it can be an existing repo or provide another repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1f393-3346-440f-8083-99aeb6013443",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"my-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f793f5-22f2-43f7-8e5f-f39149703314",
   "metadata": {},
   "source": [
    "## Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3e3d3-df16-4899-a52e-4cbb2892e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceBranch = \"main\"\n",
    "newBranch = \"airflow_demo\"\n",
    "newPath = \"partitioned_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47d797-9ca6-42ca-ba34-e1b5a217ee4a",
   "metadata": {},
   "source": [
    "## Working with the lakeFS Python client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9675a-a0d6-49c6-8b87-81c341f167b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e125990-9f42-4eff-b980-02d9d9adced7",
   "metadata": {},
   "source": [
    "## Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf3530-e8db-43a4-b424-bccf4a7fe914",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.config.get_lake_fs_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdeffb-d15f-4d84-87ae-bd2af291758a",
   "metadata": {},
   "source": [
    "## Storage Information - Optional on Playground\n",
    "#### Change the Storage Namespace to a location in the bucket youâ€™ve configured. The storage namespace is a location in the underlying storage where data for this repository will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2bea94-b287-4515-afba-51c2de0df3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://<S3 Bucket Name>/' # e.g. \"s3://username-lakefs-cloud/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ac852-ac82-45bf-b49a-1323aa673f2d",
   "metadata": {},
   "source": [
    "## Create Repository - Optional on Playground or if repository exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44f5c3-6648-4f6b-b639-7ac871734698",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repositories.create_repository(\n",
    "    repository_creation=models.RepositoryCreation(\n",
    "        name=repo,\n",
    "        storage_namespace=storageNamespace,\n",
    "        default_branch=sourceBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba46998-3a3d-4b52-ba59-6ee5c1893634",
   "metadata": {},
   "source": [
    "## Review and copy [demo Workflow](./Airflow/lakefs-dag.py) program to Airflow DAGs directory. If you make any changes in the program, then run this command again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a10ee4-95e7-4284-8bcb-6ad2304c76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo cp ./Airflow/lakefs-dag.py /root/airflow/dags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1768d3-d5e2-4b75-a674-934fda8efb4a",
   "metadata": {},
   "source": [
    "## Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1db02-1c0a-4bfa-a1fb-b51caddcf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "sudo pkill airflow\n",
    "sudo airflow standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2c0d0-6190-476d-aa3d-8f927728009c",
   "metadata": {},
   "source": [
    "## Create Airflow connections for lakeFS and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962eb91d-f103-4ce9-beb7-b717e6a7098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Airflow to start\n",
    "! sleep 10\n",
    "\n",
    "! sudo airflow connections delete conn_lakefs\n",
    "lakeFSConnectionCommand = 'airflow connections add conn_lakefs --conn-type=http --conn-host=' + lakefsEndPoint + ' --conn-extra=\\'{\"access_key_id\":\"' + lakefsAccessKey + '\",\"secret_access_key\":\"' + lakefsSecretKey + '\"}\\''\n",
    "! sudo $lakeFSConnectionCommand\n",
    "\n",
    "! sudo airflow connections delete conn_spark\n",
    "sparkConnectionCommand = 'airflow connections add conn_spark --conn-type=spark --conn-host=local[*]'\n",
    "! sudo $sparkConnectionCommand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8481df7-94f9-4bcf-a736-b2f58074aac1",
   "metadata": {},
   "source": [
    "## Set Airflow variables which are used by the demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4304d-8a10-4858-992f-a44d07f671ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo airflow variables set lakefsAccessKey $lakefsAccessKey\n",
    "! sudo airflow variables set lakefsSecretKey $lakefsSecretKey\n",
    "! sudo airflow variables set lakefsEndPoint $lakefsEndPoint\n",
    "! sudo airflow variables set repo $repo\n",
    "! sudo airflow variables set sourceBranch $sourceBranch\n",
    "! sudo airflow variables set newBranch $newBranch\n",
    "! sudo airflow variables set newPath $newPath\n",
    "! sudo airflow variables set conn_lakefs 'conn_lakefs'\n",
    "\n",
    "import os\n",
    "spark_home = os.getenv('SPARK_HOME')\n",
    "! sudo airflow variables set spark_home $spark_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc058c-494e-4bbe-811d-9b488abdf5fb",
   "metadata": {},
   "source": [
    "## Set the fileName Airflow variable. This file is used by the demo workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9e0f4-f3b4-4e4f-9a0e-44f1f9f30bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test.csv\"\n",
    "! sudo airflow variables set fileName $fileName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a6b5c-c3d4-4bbf-bfb3-89309f813b4f",
   "metadata": {},
   "source": [
    "## Find Airflow admin password and copy the password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107bdca-5e26-41f3-b223-c9c0b7bb843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo cat /root/airflow/standalone_admin_password.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf42ac-9e4a-46a6-a615-16825da83598",
   "metadata": {},
   "source": [
    "## Visualize [demo workflow DAG Graph](http://127.0.0.1:8080/dags/lakeFS_workflow/graph) in Airflow UI. Login by using username admin and password received in the previous command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230ad3d-420b-491a-9129-51f99c5e95b2",
   "metadata": {},
   "source": [
    "## Trigger demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c89a0-d8e7-4689-80a4-9dee7aa0fac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! sudo airflow dags unpause lakeFS_workflow\n",
    "! sudo airflow dags trigger lakeFS_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d0e77-8cb9-47fb-81b2-74c96fae10b9",
   "metadata": {},
   "source": [
    "## Visualize [demo workflow DAG Graph](http://127.0.0.1:8080/dags/lakeFS_workflow/graph).\n",
    "### Toggle Auto Refresh switch in DAG Graph to see the continuous progress of the workflow.\n",
    "### Click on any task box, then click on Log button and search for \"lakeFS URL\" (this URL will take you to applicable branch/commit/data file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440b100-98a9-4ef2-b622-4e7bbd9d2862",
   "metadata": {},
   "source": [
    "## Once the demo workflow finishes in around 5 minutes, you can use the latest or new file. This file has bad data, and it will cause workflow to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d825b-f4e6-4527-b7ca-48079a0a4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"lakefs_test_latest_file.csv\"\n",
    "! sudo airflow variables set fileName $fileName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bae53d-16d2-4a6e-b05f-66e0854db56e",
   "metadata": {},
   "source": [
    "## Trigger demo workflow again by using the latest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecafe94-4c8c-483e-95ec-3e5d81cae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo airflow dags trigger lakeFS_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101b97f-0fc9-4831-9fe3-834fca1f7c2a",
   "metadata": {},
   "source": [
    "## Visualize [demo workflow DAG Graph](http://127.0.0.1:8080/dags/lakeFS_workflow/graph) for the new run with the latest file.\n",
    "\n",
    "### Task \"etl_task3\" will fail in this case. Click on \"etl_task3\" task box, then click on Log button and search for \"Exception\". You will notice following exception:\n",
    "### \"Partition column _c4 not found in schema struct<_c0:string,_c1:string,_c2:string,_c3:string>\"\n",
    "\n",
    "### This exception happens because column \"_c4\" (or 5th column) is missing in the latest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bde21f-0e43-4edf-8de7-73cc4392553b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
