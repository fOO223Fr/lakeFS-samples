{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c070f9-46a7-4b1c-852c-e0fb1b1efa80",
   "metadata": {},
   "source": [
    "# [Using Hooks or Git like actions](https://docs.lakefs.io/hooks/)\n",
    "\n",
    "## This notebook demonstrated how to create a pre-merge hook in lakeFS that validates the schema files before merging them into the production branch. \n",
    "\n",
    "### 1. Define a hook configuration file and a Lua script for schema validation. \n",
    "### 2. Perform an ETL process by creating an ingestion branch, defining the table schema, and creating a table and atomically promoted the data to the production branch through a merge.\n",
    "### 3. Attempt to change the schema of the table and promote it to production again. \n",
    "### 4. The pre-merge hook prevented the promotion due to schema changes, resulting in a Precondition Failed error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9466e4e-ce3e-42af-aa12-4b8fe3327911",
   "metadata": {},
   "source": [
    "![Actions UI](./Images/LuaHooks/schemaValidationFlow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94502b61-903c-4833-8176-322bcf41a4ab",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "###### This Notebook requires connecting to a lakeFS Server. \n",
    "###### To spin up lakeFS quickly - spin up a playground environment on lakeFS Cloud (https://lakefs.cloud) which provides lakeFS server on-demand with a single click; \n",
    "###### Or, alternatively, refer to lakeFS Quickstart doc (https://docs.lakefs.io/quickstart/installing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183dd7fb-140a-40d7-b28f-3c8d39f1885d",
   "metadata": {},
   "source": [
    "## Setup Task: Change your lakeFS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4afb2-892d-4448-8fb1-4c982602c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'https://' # e.g. 'https://YOURLAKEFS.lakefscloud.io' or 'http://host.docker.internal:8000' (if lakeFS is running in local Docker container)\n",
    "lakefsAccessKey = ''\n",
    "lakefsSecretKey = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a5a58-62dd-4d23-ac3f-8fb980d090b8",
   "metadata": {},
   "source": [
    "## Setup Task: You can change lakeFS repo name (it can be an existing repo or provide a new repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda6fbf-8f10-4ab8-8008-f260836e8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"schema-validation-example-repo\"\n",
    "mainBranch = \"main\"\n",
    "ingestionBranch = \"ingestion_branch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5335c-5116-4410-9ee7-8dc01572b655",
   "metadata": {},
   "source": [
    "## Setup Task: Storage Information \n",
    "Note: If you spun up a lakeFS Cloud instance, and have not yet connected your storage, copy the storage namespace of the sample repository and append a string to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd26d1-34c1-4f76-9cf6-1d66366feda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://iddos3/blogpostschemavalidation/random/kjsdte3d7/' # e.g. \"s3://username-lakefs-cloud/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acdc6f-a95e-4597-8cf0-440440718d38",
   "metadata": {},
   "source": [
    "## Setup Task: Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d520d44-e006-4668-8874-15dfbbce3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "import os\n",
    "from pyspark.sql.types import ByteType, IntegerType, LongType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad4ca6-e452-4410-8958-381d9122fca1",
   "metadata": {},
   "source": [
    "## Setup Task: Working with the lakeFS Python client API\n",
    "\n",
    "###### Note: To learn more about lakeFS Python integration visit https://docs.lakefs.io/integrations/python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab83e4-eec6-42b4-a10c-ec2a3fa7f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "if not 'client' in locals():\n",
    "    # lakeFS credentials and endpoint\n",
    "    configuration = lakefs_client.Configuration()\n",
    "    configuration.username = lakefsAccessKey\n",
    "    configuration.password = lakefsSecretKey\n",
    "    configuration.host = lakefsEndPoint\n",
    "\n",
    "    client = LakeFSClient(configuration)\n",
    "    print(\"Created lakeFS client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a53c0-f41a-42d3-8bbb-a27d35e54502",
   "metadata": {},
   "source": [
    "## Setup Task: Run PySpark with the Delta Lake package and additional configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf5883-49be-45c3-8ea0-ba7bb3a8cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:2.0.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d8886-ec47-4ce9-ab98-2d33e146a20e",
   "metadata": {},
   "source": [
    "## Setup Task: S3A Gateway configuration\n",
    "\n",
    "##### Note: lakeFS can be configured to work with Spark in two ways:\n",
    "###### * Access lakeFS using the S3A gateway https://docs.lakefs.io/integrations/spark.html#use-the-s3-gateway.\n",
    "###### * Access lakeFS using the lakeFS-specific Hadoop FileSystem https://docs.lakefs.io/integrations/spark.html#use-the-lakefs-hadoop-filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0597007-0c2b-437e-9106-de4f933d962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9d37d-4a4e-4e04-8ba2-1138e0f858dc",
   "metadata": {},
   "source": [
    "## Setup Task: Create lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5211c7f-d2b0-439e-8adf-22a5233ada29",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repositories.create_repository(\n",
    "    repository_creation=models.RepositoryCreation(\n",
    "        name=repo,\n",
    "        storage_namespace=storageNamespace,\n",
    "        default_branch=mainBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02c9a3-5d5d-4422-b12b-7275b03d8cc9",
   "metadata": {},
   "source": [
    "## Setup Task: Upload [Hooks config YAML file](./LuaHooks/pre-merge-schema-validation.yaml) for schema validation to check for any schema changes before data is merged to main branch\n",
    "\n",
    "### Hooks config file must be uploaded to \"_lakefs_actions\" prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6f0b1-0e8f-4259-990b-a67c68c80564",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks_config_yaml = \"pre-merge-schema-validation.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n",
    "with open(f'./LuaHooks/{hooks_config_yaml}', 'rb') as f:\n",
    "    client.objects.upload_object(repository=repo, \n",
    "                                 branch=mainBranch, \n",
    "                                 path=f'{hooks_prefix}/{hooks_config_yaml}', \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7b074-2d71-40a7-af84-a4fc96e5e30f",
   "metadata": {},
   "source": [
    "## Setup Task: Upload [Schema Change script](./LuaHooks/parquet_schema_change.lua) to check for any schema changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffb6d4-8c0f-45e1-bac3-7d966d330c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lua_script_file_name = \"parquet_schema_change.lua\"\n",
    "lua_scripts_path = \"scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dbee3-951c-4174-b2f6-23c9823be6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./LuaHooks/{lua_script_file_name}', 'rb') as f:\n",
    "    client.objects.upload_object(repository=repo, \n",
    "                                 branch=mainBranch, \n",
    "                                 path=f'{lua_scripts_path}/{lua_script_file_name}', \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5605b-09d9-40f4-b6ff-eca0c0d8e313",
   "metadata": {},
   "source": [
    "## Setup Task: Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f9856-8615-40e3-8608-dc34fd1ad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.commits.commit(\n",
    "    repository=repo,\n",
    "    branch=mainBranch,\n",
    "    commit_creation=models.CommitCreation(\n",
    "        message='Added hooks config file and schema validation script'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bbe34-db01-4128-84d9-43d782aa3a77",
   "metadata": {},
   "source": [
    "# ETL Job Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a7ce6-c23e-4529-91d4-d50c7e273ca4",
   "metadata": {},
   "source": [
    "## Create a new branch which will be used to ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa8e43-f998-4e66-835f-6b3c38dca0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.branches.create_branch(\n",
    "    repository=repo, \n",
    "    branch_creation=models.BranchCreation(\n",
    "        name=ingestionBranch, source=mainBranch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5dde0-d39b-4ecd-b3c4-0d16968f4c29",
   "metadata": {},
   "source": [
    "## For this demo - we'll be utilizing a dataset - [Orion Star - Sports and outdoors RDBMS dataset](https://www.kaggle.com/datasets/chethanp11/orion-star-sports-and-outdoors-rdbms-dataset) from [Kaggle](https://www.kaggle.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4da42-2d3c-4e1c-9b9b-91925d6d659c",
   "metadata": {},
   "source": [
    "## Define [CUSTOMER.csv](../data/samples/OrionStar/CUSTOMER.csv) data file schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad323351-29ab-4f97-83bc-dd4d81f663d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customersSchema = StructType([\n",
    "  StructField(\"User_ID\", IntegerType(), False), \n",
    "  StructField(\"Country\", StringType(), False),\n",
    "  StructField(\"Gender\", StringType(), False),\n",
    "  StructField(\"Personal_ID\", IntegerType(), True),\n",
    "  StructField(\"Customer_Name\", StringType(), False),\n",
    "  StructField(\"Customer_FirstName\", StringType(), False),\n",
    "  StructField(\"Customer_LastName\", StringType(), False),\n",
    "  StructField(\"Birth_Date\", StringType(), False),\n",
    "  StructField(\"Customer_Address\", StringType(), False),\n",
    "  StructField(\"Street_ID\", LongType(), False),\n",
    "  StructField(\"Street_Number\", IntegerType(), False),\n",
    "  StructField(\"Customer_Type_ID\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8461a-6eb8-49d0-9652-229c7465c095",
   "metadata": {},
   "source": [
    "## Create Customers delta table in the new branch (using [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91e8de-7a05-468c-ad1a-05a2db3071d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customersTablePath = f\"s3a://{repo}/{ingestionBranch}/tables/customers\"\n",
    "df = spark.read.csv('./data/samples/OrionStar/CUSTOMER.csv',header=True,schema=customersSchema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(customersTablePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c751463-94ae-42e2-92e4-b8ff42b8d73d",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c59fde-bdd4-40ae-af01-05d4f85e06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.commits.commit(\n",
    "    repository=repo,\n",
    "    branch=ingestionBranch,\n",
    "    commit_creation=models.CommitCreation(\n",
    "        message='Added customers Delta table', \n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3b56d-e4c5-4a9b-90ff-bac820520e92",
   "metadata": {},
   "source": [
    "## Promote the Data into production\n",
    "\n",
    "#### Merging the ingestion branch with the current schema to the production branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5219-f8e5-456c-ac34-906f2197d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(\n",
    "    repository=repo,\n",
    "    source_ref=ingestionBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4227e9-7144-423f-becc-386661166133",
   "metadata": {},
   "source": [
    "# On the next ETL Cycle - Change the schema and try to promote new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e7a6e-d625-4dd5-8802-916346a5e80b",
   "metadata": {},
   "source": [
    "## Change \"Country\" column to \"Country_Name\" in the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bced5-a8c1-4955-a036-9f23c46df85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customersSchema = StructType([\n",
    "  StructField(\"User_ID\", IntegerType(), False),\n",
    "  StructField(\"Country_Name\", StringType(), False), # Column name changes from Country to Country_name\n",
    "  StructField(\"Gender\", StringType(), False),\n",
    "  StructField(\"Personal_ID\", IntegerType(), True),\n",
    "  StructField(\"Customer_Name\", StringType(), False),\n",
    "  StructField(\"Customer_FirstName\", StringType(), False),\n",
    "  StructField(\"Customer_LastName\", StringType(), False),\n",
    "  StructField(\"Birth_Date\", StringType(), False),\n",
    "  StructField(\"Customer_Address\", StringType(), False),\n",
    "  StructField(\"Street_ID\", LongType(), False),\n",
    "  StructField(\"Street_Number\", IntegerType(), False),\n",
    "  StructField(\"Customer_Type_ID\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f38b6f-11db-4bfe-8f92-dc3a65152f96",
   "metadata": {},
   "source": [
    "## Create Customers delta table in the new branch (using [CUSTOMER.csv](./data/samples/OrionStar/CUSTOMER.csv) file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c8352-ec8b-483a-be03-2162eb2a1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customersTablePath = f\"s3a://{repo}/{ingestionBranch}/tables/customers\"\n",
    "df = spark.read.csv('./data/samples/OrionStar/CUSTOMER.csv',header=True,schema=customersSchema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(customersTablePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346df42-d5a5-4aa6-bc04-94febc80e4c3",
   "metadata": {},
   "source": [
    "## Commit changes and attach some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9146afa-00fb-4fbe-9d82-5f020ecbced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.commits.commit(\n",
    "    repository=repo,\n",
    "    branch=ingestionBranch,\n",
    "    commit_creation=models.CommitCreation(\n",
    "        message='Added customers Delta tables with schema changes!', \n",
    "        metadata={'using': 'python_api'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7aa3d-215e-4a06-b69f-a5acc0f00cd3",
   "metadata": {},
   "source": [
    "## Merge new branch to the main branch\n",
    "\n",
    "#### Merge will fail because schema changed. Review the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc738-f7fa-4170-abfe-95895f5d928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refs.merge_into_branch(\n",
    "    repository=repo,\n",
    "    source_ref=ingestionBranch, \n",
    "    destination_branch=mainBranch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a0587-8c74-4cbe-a213-d766ea23cb65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## You can also review all Actions in lakeFS UI\n",
    "\n",
    "![Actions UI](./Images/LuaHooks/SchemaValidation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1431583-0484-4df6-93f8-43bfa37a8cae",
   "metadata": {},
   "source": [
    "# Want to learn about more advanced use cases with hooks?\n",
    "\n",
    "## Try out the broader \"Hooks Schema Validation.ipynb\" notebook in https://github.com/treeverse/lakeFS-samples/tree/main/03-multiple-samples that incorporates also preventing promotion of PII data using hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36b6f-0cd2-4ffb-9b82-eecd06c8b5a8",
   "metadata": {},
   "source": [
    "## More Questions?\n",
    "\n",
    "###### Join the lakeFS Slack group - https://lakefs.io/slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c86e6-3abc-487a-be04-6df89f75b90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
